<!-- livebook:{"persist_outputs":true} -->

# Chapter 5: Pretraining on unlabeled data

```elixir
Mix.install([
  {:nx, "~> 0.5"},
  {:exla, "~> 0.5"},
  {:axon, "~> 0.5"},
  {:tiktoken, "~> 0.3.2"},
  {:table_rex, "~> 3.1.1"},
  {:kino_vega_lite, "~> 0.1.11"}
])
```

## Introduction

This section covers:

* Pretraining the LLM.
* Implementing the training code.
* Evaluating the performance process.
* Saving and loading model weights.

In the context of LLMs and other deep learning models, weights refer to the trainable
parameters that the learning process adjusts. These weights are also known as
weight parameters or simply parameters.

```elixir
tokenizer = "gpt-3.5-turbo"
gpt_config_124m = [
  vocab_size: 50257,
  context_length: 1024,
  emb_dim: 768,
  n_heads: 12,
  n_layers: 12,
  drop_rate: 0.1,
  qkv_bias: false
]
```

<!-- livebook:{"output":true} -->

```
[
  vocab_size: 50257,
  context_length: 1024,
  emb_dim: 768,
  n_heads: 12,
  n_layers: 12,
  drop_rate: 0.1,
  qkv_bias: false
]
```

```elixir
defmodule Transformer.Layers do
  import Nx.Defn
  
  def attention(%Axon{} = input, opts \\ []) do
    #opts = Keyword.validate!(opts, [:name, :d_in, :d_out, :num_heads])
    head_dim = div(opts[:d_out], opts[:num_heads])
    w_query = Axon.param("w_query", fn _ -> {opts[:d_in], opts[:d_out]} end)
    w_key = Axon.param("w_key", fn _ -> {opts[:d_in], opts[:d_out]} end)
    w_value = Axon.param("w_value", fn _ -> {opts[:d_in], opts[:d_out]} end)
    out_proj = Axon.param("out_proj", fn _ -> {opts[:d_out], opts[:d_out]} end)

    Axon.layer(
      &attention_impl/6,
      [input, w_query, w_key, w_value, out_proj],
      name: opts[:name],
      op_name: :causal_attention,
      head_dim: head_dim,
      num_heads: opts[:num_heads]
    )
  end

  #defnp attention_impl(input, w_query, w_key, w_value, head_dim, num_heads, _opts \\ []) do
  defnp attention_impl(input, w_query, w_key, w_value, out_proj, opts \\ []) do
    {b, num_tokens, _d_in} = Nx.shape(input)
    keys = Nx.dot(input, w_key)
    queries = Nx.dot(input, w_query)
    values = Nx.dot(input, w_value)
    d_k = Nx.axis_size(keys, -1)

    keys_reshaped = 
      keys
      |> Nx.reshape({b, num_tokens, opts[:num_heads], opts[:head_dim]}) 
      |> Nx.transpose(axes: [0, 2, 1, 3])
    
    queries_reshaped = 
      queries
      |> Nx.reshape({b, num_tokens, opts[:num_heads], opts[:head_dim]}) 
      |> Nx.transpose(axes: [0, 2, 1, 3])
    
    values_reshaped = 
      values
      |> Nx.reshape({b, num_tokens, opts[:num_heads], opts[:head_dim]}) 
      |> Nx.transpose(axes: [0, 2, 1, 3])

    attn_score =
      keys_reshaped
      |> Nx.transpose(axes: [0, 1, 3, 2])
      |> then(&Nx.dot(queries_reshaped, [3], [0, 1], &1, [2], [0, 1]))

    simple_mask =
      attn_score
      |> then(&Nx.broadcast(Nx.Constants.infinity(), &1))
      |> Nx.triu(k: 1)

    masked = Nx.multiply(simple_mask, -1) |> Nx.add(attn_score)

    attn_weights =
      masked
      |> Nx.divide(Nx.pow(d_k, 0.5))
      |> Axon.Activations.softmax(axis: -1)
    
    context_vec =
      attn_weights
      |> Nx.dot([3], [0, 1], values_reshaped, [2], [0, 1])
      |> Nx.transpose(axes: [0, 2, 1, 3])

    context_vec
    |> Nx.reshape({b, num_tokens, opts[:num_heads] * opts[:head_dim]})
    |> Nx.dot(out_proj)
  end

  def shortcut(x, layer_impl, opts \\ []) when is_function(layer_impl) do
    with {:arity, arity} <- Function.info(layer_impl, :arity),
          layer_output <- execute_layer(x, layer_impl, opts, arity),
          output <- shortcut_impl(x, layer_output, opts) do
      output
    end    
  end

  defp execute_layer(x, layer_impl, _opts, 1), do: layer_impl.(x)
  defp execute_layer(x, layer_impl, opts, _arity), do: layer_impl.(x, opts)
  defp shortcut_impl(x, layer_output, opts) do
    use_shortcut? = Keyword.get(opts, :use_shortcut, false)
    if use_shortcut?, 
      do: Axon.add(x, layer_output),
      else: layer_output
  end

  def normalization(%Axon{} = input, opts \\ []) do
    #opts = Keyword.validate!(opts, [:name, :eps, :emb_dim])
    eps = Keyword.get(opts, :eps, 1.00e-5)
    scale = Axon.param("scale", {opts[:emb_dim]}, initializer: &ones(&1, type: &2))
    shift = Axon.param("shift", {opts[:emb_dim]}, initializer: &zeros(&1, type: &2))

    Axon.layer(
      &normalization_impl/4,
      [input, scale, shift],
      name: opts[:name],
      op_name: :normalization,
      eps: eps
    )
  end

  defp ones(shape, opts) do
    opts = Keyword.validate!(opts, [:type])
    Nx.iota(shape, type: opts[:type]) |> Nx.fill(1)
  end

  defp zeros(shape, opts) do
    opts = Keyword.validate!(opts, [:type])
    Nx.iota(shape, type: opts[:type]) |> Nx.fill(0)
  end

  defnp normalization_impl(input, scale, shift, opts \\ []) do
    mean = Nx.mean(input, axes: [-1], keep_axes: true)
    variance = Nx.variance(input, axes: [-1], keep_axes: true)
    denominator = variance |> Nx.add(opts[:eps]) |> Nx.sqrt()

    input
    |> Nx.subtract(mean)
    |> Nx.divide(denominator)
    |> Nx.multiply(scale)
    |> Nx.add(shift)
  end

  def pos_embedding(%Axon{} = x, vocab_size, embedding_size, opts \\ []) do
    opts = Keyword.validate!(opts, [:name, kernel_initializer: :uniform])

    kernel_shape = &Axon.Shape.embedding_kernel(&1, vocab_size, embedding_size)

    kernel = Axon.param("kernel", kernel_shape, initializer: opts[:kernel_initializer])

    Axon.layer(&pos_embedding_impl/3, [x, kernel], name: opts[:name], op_name: :pos_embedding)
  end

  defnp pos_embedding_impl(x, kernel, _opts \\ []) do
    {_batch_size, sequence_size} = Nx.shape(x)
    input = Nx.iota({1, sequence_size})
    Nx.take(kernel, input, axis: 0)
  end

  def feedforward(input, emb_dim) do
    input
    |> Axon.dense(4*emb_dim)
    |> Axon.activation(:gelu)
    |> Axon.dense(emb_dim)
  end

  def feedforward_block(input, opts) do
    input
    |> normalization(opts)
    |> feedforward(opts[:emb_dim])
    |> Axon.dropout(rate: opts[:drop_rate])
  end

  def attention_block(input, opts) do
    input
    |> normalization(opts)
    |> attention(d_in: opts[:emb_dim], d_out: opts[:emb_dim], num_heads: opts[:n_heads] )
    |> Axon.dropout(rate: opts[:drop_rate])
  end

  def block(input, opts \\ []) do
    input
    |> shortcut(&attention_block(&1, opts), use_shortcut: true)
    |> shortcut(&feedforward_block(&1, opts), use_shortcut: true)
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Transformer.Layers, <<70, 79, 82, 49, 0, 0, 51, ...>>, {:block, 2}}
```

```elixir
defmodule MyGPT do
  @gpt_config_124m gpt_config_124m
  def model(input_shape \\ {2, 4, 768}, opts \\ @gpt_config_124m) do
    Axon.input("sequence", shape: input_shape)
    |> embedding_block(opts)
    |> Axon.dropout(rate: opts[:drop_rate])
    |> transformer_blocks(12, opts)
    |> Transformer.Layers.normalization(opts)
    |> Axon.dense(opts[:vocab_size], use_bias: false)
  end

  def embedding_block(input, opts) do
    token_emb = Axon.embedding(input, opts[:vocab_size], opts[:emb_dim])
    pos_emb = Transformer.Layers.pos_embedding(input, opts[:context_length], opts[:emb_dim])

    Axon.add(token_emb, pos_emb)
  end

  def transformer_blocks(input, n_blocks, transformer_opts) do
    for _n_block <- 1..n_blocks, reduce: input do
      model_acc ->
        Transformer.Layers.block(model_acc, transformer_opts)
    end
  end

  def text_to_token_ids(tokenizer, texts) when is_list(texts) do
    token_ids_list = 
      for text <- texts do
        {:ok, token_ids} = text_to_token_ids(tokenizer, text)
        token_ids
      end
    Nx.stack(token_ids_list, axis: 1) |> Nx.squeeze()
  end

  def text_to_token_ids(tokenizer, text) do
    {:ok, tokens} = Tiktoken.encode(tokenizer, text)
    {:ok, Nx.tensor(tokens, type: :s64) |> Nx.new_axis(0)}
  end

  def token_ids_to_text(tokenizer, token_ids) do
    tokens_ids = Nx.to_flat_list(token_ids)
    Tiktoken.decode(tokenizer, tokens_ids)
  end

  def generate_tokens(predict_fn, model_params, input, max_new_token) when is_function(predict_fn) do
    generate_tokens_impl(predict_fn, model_params, input, max_new_token)
  end

  def generate_tokens_with_model(model, model_params, input, max_new_token) when model_params == %{} do
    {init_fn, predict_fn} = Axon.build(model, compiler: EXLA)
    template = Nx.template(Nx.shape(input), :s64)
    init_model_params = init_fn.(template, model_params)
    generate_tokens_impl(predict_fn, init_model_params, input, max_new_token)
  end

  def generate_tokens_with_model(model, model_params, input, max_new_token) do
    {_init_fn, predict_fn} = Axon.build(model, compiler: EXLA)
    generate_tokens_impl(predict_fn, model_params, input, max_new_token)
  end

  defp generate_tokens_impl(predict_fn, model_params, input, max_new_token) do
    for _new_token_index <- 1..max_new_token, reduce: input do
      input_acc ->
        logit = predict_fn.(model_params, input_acc)

        # Get last element of the vector.
        predicted_new_token =
          logit[[.., -1]]
          |> Axon.Layers.softmax(axis: -1)
          |> Nx.argmax(axis: -1)
          |> Nx.new_axis(0)

        Nx.concatenate([input_acc, predicted_new_token], axis: 1)
    end
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, MyGPT, <<70, 79, 82, 49, 0, 0, 25, ...>>, {:generate_tokens_impl, 4}}
```

## 5.1 Evaluating generative text models

```elixir
gpt_config_124m = [
  vocab_size: 50257,
  context_length: 256,
  emb_dim: 768,
  n_heads: 12,
  n_layers: 12,
  drop_rate: 0,
  qkv_bias: false
]
```

<!-- livebook:{"output":true} -->

```
[
  vocab_size: 50257,
  context_length: 256,
  emb_dim: 768,
  n_heads: 12,
  n_layers: 12,
  drop_rate: 0,
  qkv_bias: false
]
```

```elixir
model = MyGPT.model()
{init_fn, predict_fn} = Axon.build(model, compiler: EXLA)
template = Nx.template({1, 4}, :s64)
params = init_fn.(template, %{})
```

<!-- livebook:{"output":true} -->

```
%{
  "pos_embedding_0" => %{
    "kernel" => #Nx.Tensor<
      f32[1024][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224041>
      [
        [-0.007060885429382324, 0.007274620234966278, 0.0041606067679822445, 0.005386946257203817, 0.00733589893206954, -0.007371966727077961, 0.001868531689979136, 0.005129444412887096, 0.009811586700379848, 8.908510208129883e-4, -0.009878544136881828, -0.002141666365787387, 0.002241623355075717, -0.009590706788003445, 0.0030502795707434416, -0.008582658134400845, 0.00640228483825922, -0.003917272202670574, 0.0073986840434372425, 3.515219723340124e-4, -6.22868537902832e-4, 0.008721787482500076, -0.004013793542981148, 0.008192270062863827, 0.003637268440797925, -0.006199588533490896, -0.005837616976350546, 0.0019061875063925982, 0.009164171293377876, -0.0012545108329504728, 0.007469587028026581, 0.0021345734130591154, -0.006843104027211666, -0.009557719342410564, -0.006849651224911213, 0.004082290921360254, -0.006217131391167641, -0.00927067268639803, -0.00440683588385582, 0.003057455876842141, -0.004662494640797377, -0.005233602598309517, -0.003934979438781738, 0.005652947351336479, -0.00962330587208271, -0.009265951812267303, 0.0034995507448911667, -0.005006186664104462, ...],
        ...
      ]
    >
  },
  "normalization_6" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224033>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224034>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "dense_4" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223978>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223979>
      [
        [-0.010270211845636368, 0.006820410490036011, 0.0019992003217339516, 0.02657410129904747, -0.03538765385746956, 0.029271116480231285, -0.012326033785939217, -0.002058743266388774, -0.01075215358287096, 0.03410271182656288, 0.03817693144083023, 0.025283364579081535, 0.039454199373722076, 0.002474817680194974, 0.03245581313967705, 0.00926691759377718, 0.014417932368814945, -0.011562041938304901, 0.02228909730911255, -0.025484904646873474, -6.200073403306305e-4, 0.036014776676893234, -0.02309742197394371, 0.016414569690823555, -0.021966824308037758, 0.01388510875403881, 0.017405197024345398, 0.01072456780821085, -0.038961075246334076, 0.01383789349347353, -0.015734264627099037, 0.028081117197871208, 0.0073573705740273, -0.020549198612570763, -0.013459506444633007, -0.021737435832619667, -0.010065346024930477, -0.021324951201677322, -0.03263194486498833, 0.016179686412215233, -0.03413737565279007, -0.013564559631049633, -0.025418292731046677, -0.03161655738949776, -0.00948886014521122, ...],
        ...
      ]
    >
  },
  "normalization_5" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224031>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224032>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "dense_20" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223967>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223968>
      [
        [0.004870744422078133, -0.03184974193572998, -0.030071798712015152, -0.01683010533452034, 0.006775578949600458, 0.03029133751988411, -0.019637612625956535, 0.0033226204104721546, 0.027346057817339897, -0.038697995245456696, 0.023994145914912224, 0.003443147987127304, -0.013484896160662174, 0.027026478201150894, -0.005860373843461275, 0.020501313731074333, 0.0039795804768800735, 0.007202595006674528, -0.02333843894302845, -0.012154784984886646, -0.028300553560256958, -0.02931131236255169, -0.026982033625245094, -0.032288867980241776, -0.024990495294332504, 0.024031447246670723, -0.027153801172971725, 0.025181874632835388, -0.02820858173072338, 0.008711240254342556, -0.020283469930291176, -0.034664466977119446, -0.00473489286378026, 0.02474534884095192, -0.009637407027184963, 0.012906383723020554, 0.01971171610057354, -0.013825575821101665, -0.028503092005848885, -0.03887939453125, -0.0029850599821656942, -0.01856226846575737, -0.030813219025731087, ...],
        ...
      ]
    >
  },
  "normalization_1" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223993>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223994>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "normalization_9" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224039>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224040>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "dense_22" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223971>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223972>
      [
        [0.0034290587063878775, 0.02325303666293621, 0.017983973026275635, 0.0036507092881947756, -0.029444195330142975, -0.013926001265645027, 0.025092994794249535, 0.03882244974374771, 1.5551074466202408e-4, 0.016167066991329193, 0.022907333448529243, -0.010581770911812782, -0.03860994055867195, -0.017351524904370308, -0.038868892937898636, 6.90463581122458e-4, 0.004097997210919857, -0.0015407165046781301, -1.6944932212936692e-5, 0.0232403427362442, 0.023378022015094757, 0.0228725578635931, 0.0017855322221294045, -0.014771504327654839, 0.015504028648138046, -0.005855906754732132, -0.008247874677181244, -0.03497270122170448, -0.023066801950335503, 0.004176237620413303, 0.020056117326021194, 0.027729853987693787, -0.025042762979865074, -0.002837088657543063, -0.02771267294883728, 0.03927137702703476, -0.006017100065946579, 0.028540335595607758, 0.01726962812244892, 0.010915749706327915, ...],
        ...
      ]
    >
  },
  "causal_attention_0" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223893>
      [
        [-0.019175291061401367, -0.05620482563972473, -0.008011311292648315, 0.042143821716308594, -0.03634865581989288, 0.02358192205429077, -0.03994932770729065, -0.04177790880203247, 0.005804151296615601, -5.772113800048828e-4, 0.020412936806678772, 0.006356790661811829, -0.03173127770423889, 0.011857032775878906, -0.050463274121284485, -0.05759413540363312, 0.049270883202552795, -0.0042218416929244995, -0.021425455808639526, -0.05656208097934723, 0.006219640374183655, 0.03625169396400452, 0.008986473083496094, 0.02922476828098297, -0.004139408469200134, -0.007297545671463013, 0.015016719698905945, 0.04972977936267853, 0.060343384742736816, -0.0387236624956131, -0.025317281484603882, 0.019617080688476562, -0.011727318167686462, 0.023797377943992615, 0.012865126132965088, 0.02385176718235016, -0.0065391212701797485, 0.0010329484939575195, 0.010577231645584106, 0.0357387512922287, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223894>
      [
        [-0.047006577253341675, -0.03904096782207489, 0.04191003739833832, 0.03801827132701874, 0.03630341589450836, 0.034335553646087646, -0.061589211225509644, 0.010644853115081787, -0.0392012894153595, -0.008061304688453674, 0.0555187463760376, 0.005824029445648193, 0.026513442397117615, 0.0029243379831314087, -0.002939090132713318, -0.06126375496387482, -0.0190047025680542, -0.054323554039001465, -0.019014328718185425, -0.016748547554016113, 0.029145389795303345, -0.02775178849697113, -0.01543658971786499, -0.050953954458236694, 0.048380330204963684, -0.05886845290660858, 0.035739168524742126, 0.03226272761821747, -7.264316082000732e-4, -0.0329291969537735, -0.019827470183372498, -0.002714112401008606, 0.022738739848136902, -0.001860782504081726, 8.760690689086914e-4, -0.001912921667098999, 0.020788580179214478, 0.011096283793449402, 0.05218653380870819, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223895>
      [
        [-0.04020151495933533, 0.020423635840415955, -0.01556532084941864, -0.031026005744934082, 0.03168374300003052, -0.016254544258117676, 0.005474403500556946, 0.02722051739692688, -0.02372296154499054, 0.06200495362281799, -0.023918360471725464, -0.03313007950782776, -0.05771678686141968, -0.030772686004638672, 0.031689032912254333, -0.058578312397003174, -0.05967891216278076, -0.007798686623573303, -0.02480502426624298, 0.008558526635169983, 0.038065314292907715, 0.0221279114484787, 0.03430965542793274, -0.04827965795993805, 0.04904693365097046, -0.05542020499706268, -0.05248251557350159, -0.060298800468444824, 0.005015358328819275, 0.025826215744018555, -0.009997174143791199, 0.04585385322570801, -0.037919312715530396, 0.04098424315452576, 0.02791959047317505, -0.05764932930469513, -0.004208371043205261, 0.05632558465003967, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223896>
      [
        [-0.02613326907157898, -0.001987934112548828, -0.03845413029193878, 0.05832624435424805, -0.004508420825004578, -0.04551650583744049, 0.03648141026496887, -3.2742321491241455e-4, -0.009660303592681885, -0.034169793128967285, -0.004486531019210815, 0.05299989879131317, -0.014728963375091553, 0.021208718419075012, 0.054772719740867615, 0.030904456973075867, 0.020968332886695862, -0.023156732320785522, 0.0322081595659256, -0.025118574500083923, 0.05915658175945282, -0.018342137336730957, 0.03504827618598938, -0.042255252599716187, -0.039723023772239685, -0.00955246388912201, 0.010663434863090515, 0.02119426429271698, -0.03531740605831146, -0.019000038504600525, -0.044335365295410156, 0.02854520082473755, -0.06198248267173767, -0.03163684904575348, -0.04262682795524597, 0.041292980313301086, 0.03768084943294525, ...],
        ...
      ]
    >
  },
  "normalization_14" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224003>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224004>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "normalization_7" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224035>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224036>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "normalization_11" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223997>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223998>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "normalization_2" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224015>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224016>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "dense_12" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223949>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223950>
      [
        [-0.015925871208310127, 0.009328939020633698, -0.008754884824156761, -0.009141705930233002, -0.026865540072321892, -0.0280645489692688, -0.022113334387540817, -0.0012031467631459236, 0.038243167102336884, 0.02969193086028099, 0.025515863671898842, 0.011882364749908447, 0.037830889225006104, 0.0035562869161367416, -9.53015754930675e-4, -0.00739323953166604, 0.015531896613538265, 0.023421552032232285, 0.021488728001713753, -0.03845229372382164, 0.0077180396765470505, -0.011452597565948963, 0.026390958577394485, -0.013281773775815964, -0.03902168199419975, -0.029475823044776917, -0.030041422694921494, -0.010721524246037006, -0.003969317302107811, -0.001028532860800624, 0.03337198868393898, 0.013844112865626812, 0.014459747821092606, -0.028008701279759407, ...],
        ...
      ]
    >
  },
  "dense_23" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223973>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223974>
      [
        [0.01820378564298153, 0.02513214200735092, 0.02019086666405201, 6.149370892671868e-5, 0.03533006086945534, 0.0031746961176395416, 0.024222176522016525, 0.003586595645174384, -0.010967593640089035, 0.021342311054468155, 0.015422055497765541, -0.031186940148472786, -0.002499236026778817, 0.019134804606437683, 0.019215261563658714, 0.014050327241420746, -0.023976871743798256, -0.013647578656673431, -0.0064771766774356365, -0.01782964915037155, -0.03583754971623421, 0.001932485611177981, 0.016471341252326965, 0.03907326981425285, 0.03910063952207565, -0.01616004668176174, -0.03441900387406349, -0.027950363233685493, 0.034320130944252014, -0.012021326459944248, 0.01260341051965952, -0.03163352981209755, -0.005752474535256624, ...],
        ...
      ]
    >
  },
  "embedding_0" => %{
    "kernel" => #Nx.Tensor<
      f32[50257][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223990>
      [
        [-0.008046426810324192, -0.005801000632345676, 0.0019265675218775868, 0.0027360653039067984, -0.00512055866420269, 0.007794530130922794, -0.007652327883988619, 0.007234899792820215, -0.0017656468553468585, -0.0020308159291744232, 0.009583994746208191, -0.002934760879725218, 0.004365928005427122, -0.004899859428405762, 0.004798395559191704, -0.005690045189112425, -0.009535593912005424, 0.007920925505459309, -0.005476539023220539, 0.00788002461194992, -0.003048570128157735, -3.4298180253244936e-4, -0.006712541449815035, 0.005692949052900076, 0.007098774891346693, -0.006864671595394611, -0.008847828023135662, -0.005944471340626478, 0.005876514595001936, -0.008840975351631641, 0.005137097556143999, -0.0071597956120967865, 0.009742521680891514, ...],
        ...
      ]
    >
  },
  "normalization_19" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224013>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224014>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "causal_attention_6" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223925>
      [
        [0.03975580632686615, 0.008749619126319885, 0.047931671142578125, -0.013279423117637634, -0.013245850801467896, -0.05585356056690216, -0.013620615005493164, 0.03197602927684784, -0.04547400772571564, 0.03761903941631317, 0.03663606941699982, -0.03762349486351013, 0.01000109314918518, -0.01308596134185791, 0.015029758214950562, -0.03293958306312561, 0.032212093472480774, -0.052900537848472595, 0.05307626724243164, -0.008713826537132263, 0.03084041178226471, -0.035007670521736145, -0.005815252661705017, -0.03332681953907013, -0.0033163130283355713, 0.019390255212783813, 0.04983982443809509, 0.05754546821117401, 0.003676384687423706, -0.03705018758773804, -0.037810564041137695, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223926>
      [
        [0.0057325661182403564, -0.00996197760105133, -0.009572997689247131, 0.05794805288314819, 0.04828427731990814, 0.007762208580970764, -0.01754733920097351, -0.014023497700691223, -0.016367211937904358, 0.017433971166610718, -0.017818138003349304, 0.023550093173980713, -0.05745719373226166, 0.048951566219329834, -3.2301247119903564e-4, -0.04373502731323242, 0.01328006386756897, -0.022501438856124878, -0.05002075433731079, 0.04846307635307312, -0.06120976805686951, 0.061530277132987976, -0.061119407415390015, 0.06149454414844513, 0.029913008213043213, -0.03880259394645691, 0.019962072372436523, -0.05924692749977112, 0.053466543555259705, -0.016915082931518555, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223927>
      [
        [-0.007530540227890015, -0.017455384135246277, -0.013414904475212097, -0.03001043200492859, 0.058805450797080994, 0.027058839797973633, -0.008854344487190247, -0.04394069314002991, 0.025706544518470764, -0.04956457018852234, -0.04134167730808258, -0.0345882773399353, 0.012006551027297974, -0.054361745715141296, -0.048273056745529175, 0.032691508531570435, -0.004404619336128235, -0.01956745982170105, 0.059636190533638, 0.05025474727153778, -0.027073130011558533, -0.004165083169937134, 0.052017271518707275, -0.008713409304618835, 0.010324418544769287, -0.04247170686721802, 0.03176307678222656, -0.0037843137979507446, -0.04677100479602814, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223928>
      [
        [-0.020589128136634827, 0.016773968935012817, -0.0572335422039032, -0.054901570081710815, -0.051986634731292725, -0.03901219367980957, 0.05229029059410095, -0.028979584574699402, -0.06248505413532257, 0.05948479473590851, 0.012541666626930237, 0.04265777766704559, -0.03169243037700653, -0.05623914301395416, 0.01684536039829254, -0.0017365217208862305, -0.03204962611198425, -0.02545703947544098, -0.010227441787719727, -0.05516447126865387, -0.04219040274620056, 0.038970738649368286, 0.015619367361068726, -0.05474875867366791, 0.047200724482536316, -0.03519459068775177, -0.013846814632415771, -1.2145936489105225e-4, ...],
        ...
      ]
    >
  },
  "causal_attention_7" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223929>
      [
        [0.038279592990875244, -0.05338229238986969, -0.045250579714775085, 0.044668734073638916, 0.04357665777206421, -0.020535185933113098, -0.008746117353439331, 0.025088980793952942, -0.058881714940071106, 0.03846117854118347, -0.051234468817710876, -0.008802950382232666, 0.027630552649497986, 0.053497567772865295, -0.011778086423873901, -0.050474926829338074, -0.01214165985584259, -0.020523160696029663, 0.02822507917881012, 0.00940336287021637, 0.024940967559814453, 0.006627798080444336, 0.06164179742336273, -0.020976006984710693, -0.012726694345474243, 0.039830803871154785, 0.018833771347999573, 0.012904226779937744, 0.03251352906227112, -0.03988637030124664, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223930>
      [
        [0.037747740745544434, -0.008878812193870544, -0.05591043829917908, -0.050280213356018066, -0.05409230291843414, 0.026905104517936707, 0.009063780307769775, -0.01561996340751648, 0.012101158499717712, -0.003477424383163452, 0.05192972719669342, -0.0383894145488739, 0.007722139358520508, -0.03379252552986145, 0.044305309653282166, 0.023362725973129272, -9.354948997497559e-5, 0.006550654768943787, -0.04183124005794525, 0.05311192572116852, 0.0021597743034362793, 0.0406208336353302, -0.030163124203681946, -0.03306084871292114, 0.04747910797595978, -0.051800474524497986, -0.0527167022228241, -0.013013288378715515, -0.035736218094825745, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223931>
      [
        [-0.0030989497900009155, -0.05143946409225464, 0.02378225326538086, 0.01391465961933136, 0.05959680676460266, 0.010784655809402466, 0.047866031527519226, -0.01957497000694275, -0.04925936460494995, -0.010460183024406433, 0.04659806191921234, -0.021901801228523254, 0.05499613285064697, 0.019651859998703003, 0.030718371272087097, 0.012227177619934082, 0.04696270823478699, -0.05391719937324524, 0.0096711665391922, -0.013616830110549927, -0.0022573918104171753, 0.002575919032096863, 0.059956878423690796, -0.016638249158859253, -0.028692767024040222, 0.007272154092788696, -0.03552773594856262, -0.05820663273334503, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223932>
      [
        [-0.011709004640579224, -0.04861938953399658, -0.015325650572776794, -0.003318578004837036, -0.04971122741699219, 0.0015152394771575928, 0.019107088446617126, 0.01146794855594635, -0.059434548020362854, -0.01183629035949707, 0.05704055726528168, 0.056822776794433594, -0.007194027304649353, 0.03453183174133301, -0.04852879047393799, 0.05712957680225372, -5.758106708526611e-4, 0.058826565742492676, -0.03319251537322998, 0.013112545013427734, -0.006872102618217468, -0.009747505187988281, 0.05400916934013367, -0.0011600703001022339, 0.006049394607543945, -0.03337526321411133, 0.030808910727500916, ...],
        ...
      ]
    >
  },
  "causal_attention_1" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223897>
      [
        [0.01506870985031128, -0.05148409307003021, -0.023009970784187317, -0.036842674016952515, 0.04571639001369476, -0.015336647629737854, -0.052823543548583984, -0.05420304834842682, -0.05807679891586304, 0.03370898962020874, 0.04816645383834839, -0.024105682969093323, 0.03814081847667694, -0.03880034387111664, -0.04321637749671936, 0.010633736848831177, 0.010015547275543213, 0.0612257719039917, 0.032576799392700195, -0.04307471215724945, -0.05430462956428528, -0.03653436899185181, 0.010184943675994873, 0.039335861802101135, -0.02881374955177307, -0.01885862648487091, 0.01976732909679413, 0.03198444843292236, 0.016550928354263306, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223898>
      [
        [0.00977182388305664, 0.04366813600063324, -9.154230356216431e-4, 0.015302017331123352, 0.025258511304855347, -0.026702001690864563, 0.019073963165283203, -0.05046834051609039, -0.03637833893299103, -0.04103530943393707, 0.02183246612548828, -0.05848129093647003, -0.007408514618873596, 0.02244172990322113, 0.0486253947019577, 0.05803729593753815, -0.055785492062568665, -0.003268897533416748, -0.011576533317565918, -0.02838572859764099, 0.02192777395248413, -0.04926668107509613, -0.014949902892112732, -0.03317803144454956, 0.01732151210308075, -0.014444008469581604, 0.05452212691307068, -0.05097481608390808, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223899>
      [
        [0.005973607301712036, -0.054648324847221375, -0.01970745623111725, -0.03169390559196472, 0.016631603240966797, -0.016067132353782654, -0.03650784492492676, 0.0596468448638916, 5.203187465667725e-4, 0.030002295970916748, 0.016653388738632202, -0.060789600014686584, -0.035765692591667175, -0.03314366936683655, -0.018639400601387024, -0.047814637422561646, 0.04358126223087311, 0.006635025143623352, 0.019207030534744263, -0.024693429470062256, -0.05198492109775543, 0.05463632941246033, -0.05869154632091522, 0.03899344801902771, -0.04231579601764679, 0.0037299692630767822, 0.04150767624378204, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223900>
      [
        [0.020770743489265442, 0.029785573482513428, -0.039374470710754395, -0.002952754497528076, -0.02961452305316925, 0.019166111946105957, -0.01701951026916504, -0.05620776116847992, -0.054720014333724976, -0.004926636815071106, -0.025516033172607422, -0.015365630388259888, 0.04736007750034332, 0.018546447157859802, -0.053304508328437805, 0.04099404811859131, 0.0020524561405181885, -0.02782374620437622, 0.018728122115135193, 0.02030833065509796, -0.0178043395280838, -0.03597392141819, -0.04463514685630798, -0.05451175570487976, 0.02041785418987274, -0.049391403794288635, ...],
        ...
      ]
    >
  },
  "dense_15" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223955>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223956>
      [
        [-0.006591597571969032, 0.015917304903268814, 0.023710304871201515, 0.010628440417349339, -0.03666031360626221, -0.017011316493153572, 0.0029258374124765396, 0.021925639361143112, -0.013816047459840775, -0.034644704312086105, 0.03885995224118233, 0.03816450014710426, -0.030881356447935104, 0.036499347537755966, 0.0339113250374794, -0.028529318049550056, -0.02391500025987625, 0.030093662440776825, 0.03054637834429741, 0.03250007703900337, -0.01579686999320984, -0.012109735980629921, -0.008585944771766663, -0.019892916083335876, 0.0216514952480793, 0.011779271997511387, 0.03227498009800911, ...],
        ...
      ]
    >
  },
  "dense_9" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223988>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223989>
      [
        [-0.034544140100479126, -0.03761721029877663, -0.008348771370947361, 0.008192318491637707, -0.00652977405115962, 0.0032340881880372763, 0.035090845078229904, 0.016089919954538345, 0.014906780794262886, -0.00219861906953156, 0.03645406290888786, -0.008760369382798672, 0.014154446311295033, -0.017532754689455032, -0.01134578138589859, 0.027284780517220497, 0.027129637077450752, -0.036774612963199615, -0.014615070074796677, 0.012311821803450584, -0.021076027303934097, -0.024875037372112274, 0.021069712936878204, -0.008484180085361004, 0.030842095613479614, -0.0073321606032550335, ...],
        ...
      ]
    >
  },
  "normalization_8" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224037>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224038>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "causal_attention_10" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223901>
      [
        [0.03123481571674347, 0.031119689345359802, -0.04656463861465454, 0.05552935600280762, -0.028794750571250916, -0.0030631273984909058, 0.03431239724159241, -0.0181577205657959, -0.03395538032054901, 0.03638745844364166, 0.021356016397476196, 0.05290704965591431, 0.04040166735649109, 0.0075228214263916016, 0.010965496301651001, -0.016339421272277832, 0.024726614356040955, 0.00760139524936676, -0.019101157784461975, 0.015333801507949829, -0.03493218123912811, -0.05701945722103119, 0.021580055356025696, 0.03652912378311157, 0.006339892745018005, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223902>
      [
        [-0.04824836552143097, -0.005397796630859375, -0.039476409554481506, -0.05730779469013214, 0.043096110224723816, 0.0133657306432724, -0.006188496947288513, 0.03617966175079346, -0.049161434173583984, 0.004547193646430969, -0.011875137686729431, -0.058523133397102356, -0.007445916533470154, 0.017110690474510193, -0.0052989572286605835, 0.04919283092021942, -0.002910122275352478, -0.03225669264793396, -0.034507036209106445, -0.06006839871406555, -0.01608601212501526, -0.0441855788230896, 0.017715245485305786, 0.005503728985786438, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223903>
      [
        [-0.039320990443229675, -0.004033192992210388, -0.047112837433815, -0.016364261507987976, -0.004273831844329834, -0.05017390847206116, 0.04418446123600006, -0.01084187626838684, -0.044605568051338196, -0.00321747362613678, 0.05048932135105133, 0.005256131291389465, -0.002186104655265808, -0.04918105900287628, 0.029604509472846985, -0.010393887758255005, -0.009018391370773315, -0.04071901738643646, 0.0608600378036499, 0.02122838795185089, -0.0543815940618515, -0.0014889836311340332, 0.01693873107433319, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223904>
      [
        [-0.035531967878341675, 0.0019565969705581665, 0.03795626759529114, -0.024342775344848633, 0.010045021772384644, -0.025237977504730225, 0.039928048849105835, -0.015555262565612793, 0.04864177107810974, 0.053446024656295776, 0.019868940114974976, 0.001344040036201477, 0.024630457162857056, 0.01062031090259552, 0.037274256348609924, -0.05181208252906799, -0.022365957498550415, -0.0034584254026412964, 0.025101035833358765, 0.01115061342716217, -0.018867626786231995, 0.03901831805706024, ...],
        ...
      ]
    >
  },
  "causal_attention_8" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223933>
      [
        [-0.013108879327774048, 0.026917383074760437, -0.0382012277841568, 0.04033845663070679, 0.047410815954208374, -0.058005183935165405, 0.022279053926467896, 0.05500739812850952, 0.04738195240497589, -0.018765941262245178, -0.009866058826446533, 0.018895819783210754, -0.034815818071365356, 0.03470557928085327, -0.008729875087738037, -0.05881752073764801, -0.013540968298912048, 0.05592973530292511, -0.04580886662006378, 0.011736452579498291, 0.05284160375595093, -0.022878989577293396, 0.0491088330745697, -0.01515023410320282, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223934>
      [
        [0.04275710880756378, -0.009106487035751343, -0.044138312339782715, 0.06118205189704895, -0.05124884843826294, -0.01255488395690918, -0.01715824007987976, -0.022343650460243225, 0.04343914985656738, 0.04381735622882843, -0.017749354243278503, -0.028462305665016174, -0.04988411068916321, 0.045357197523117065, 0.0034534186124801636, -0.022033438086509705, 0.05264616012573242, 0.062230855226516724, -0.03980480134487152, 0.06241023540496826, -0.024756357073783875, 0.03825180232524872, -0.060704007744789124, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223935>
      [
        [0.04367637634277344, 0.04769708216190338, 0.05252787470817566, 0.047605082392692566, -0.006850242614746094, -0.05337205529212952, 0.016617566347122192, 0.005792990326881409, -0.02684144675731659, 0.03914366662502289, 0.0010374784469604492, 0.00694650411605835, 0.02583332359790802, 0.03326672315597534, -0.0596742182970047, 0.03435900807380676, -0.05310501158237457, -0.01957280933856964, 0.027727097272872925, 0.0375472754240036, -0.005624547600746155, 0.05025559663772583, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223936>
      [
        [0.051830753684043884, -0.005421251058578491, 0.032308727502822876, -0.005199000239372253, -0.01961115002632141, 0.02105705440044403, 0.009635627269744873, 0.0598188191652298, -0.005381688475608826, -0.018103986978530884, 0.030051827430725098, 0.008235171437263489, 0.058550626039505005, -0.042152658104896545, 0.02947819232940674, 0.007268399000167847, -0.05887223780155182, -0.017300501465797424, -0.022023066878318787, 0.014484122395515442, -0.007470935583114624, ...],
        ...
      ]
    >
  },
  "normalization_12" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223999>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224000>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "dense_3" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223976>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223977>
      [
        [-0.012955248355865479, 0.029782038182020187, 0.03283224627375603, 0.021280422806739807, -0.009595430456101894, 0.0013459064066410065, 0.01878093183040619, 0.024679681286215782, 0.009832160547375679, 0.0030047285836189985, -0.01608043909072876, -0.004221625160425901, -0.021087488159537315, 0.005540870130062103, -0.020248176530003548, 0.035337865352630615, -0.0158245787024498, 0.03562016040086746, 0.034884262830019, -0.03614576533436775, 0.029169466346502304, ...],
        ...
      ]
    >
  },
  "dense_18" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223961>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223962>
      [
        [0.0202152281999588, 0.01284861285239458, -0.02074536494910717, -0.02268022671341896, -0.020536484196782112, -0.029445325955748558, 0.006689450237900019, -0.002192201092839241, 0.03470541536808014, 0.025332240387797356, -0.036912236362695694, 2.2499628539662808e-4, 0.014998111873865128, 0.012085299007594585, 0.00685903150588274, -0.03504256159067154, -0.02157972939312458, -0.01494570355862379, 0.034015923738479614, 0.027803052216768265, ...],
        ...
      ]
    >
  },
  "dense_5" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223980>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223981>
      [
        [0.028328562155365944, -0.0291450098156929, -0.016360953450202942, -0.03942672163248062, -0.01731015369296074, 0.0013208093587309122, 0.018527012318372726, -0.03066990338265896, -0.022937538102269173, 0.01168097648769617, -0.02107958123087883, -0.022728346288204193, 0.010330169461667538, -0.016313577070832253, 0.025916557759046555, -0.016573471948504448, 0.006425625644624233, 0.029292698949575424, -3.8787684752605855e-4, ...],
        ...
      ]
    >
  },
  "dense_11" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223947>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223948>
      [
        [-0.013371803797781467, -0.035878103226423264, -0.011822275817394257, -0.023128116503357887, -0.03616476431488991, 0.01225187350064516, 0.0022337152622640133, 0.014103083871304989, -0.02706325240433216, -0.03851797804236412, 0.03405740112066269, 0.01336765754967928, -0.007865134626626968, -0.020188257098197937, -0.004562672693282366, 0.014646651223301888, -0.03481720760464668, 0.020495159551501274, ...],
        ...
      ]
    >
  },
  "normalization_18" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224011>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224012>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "dense_6" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223982>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223983>
      [
        [0.007337909657508135, 0.017637724056839943, -0.006839918904006481, -0.005361638497561216, 0.026848217472434044, -0.015141051262617111, -0.016560938209295273, 0.030522722750902176, 0.012203441932797432, 0.024752531200647354, 0.004080590326339006, 0.004118843469768763, -0.009860442951321602, 0.03789469227194786, 0.009158208034932613, 0.00814934354275465, ...],
        ...
      ]
    >
  },
  "normalization_15" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224005>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224006>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "normalization_23" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224023>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224024>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "dense_10" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223945>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223946>
      [
        [0.020225152373313904, 0.008844218216836452, -0.016715440899133682, -0.015652460977435112, 0.0024590790271759033, 0.004017334431409836, 0.014676026068627834, 0.01339647639542818, -0.021712621673941612, 0.029198775067925453, -0.011233415454626083, 0.02140211872756481, 0.021059855818748474, ...],
        ...
      ]
    >
  },
  "dense_13" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223951>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223952>
      [
        [0.0052589792758226395, 0.025502970442175865, -0.016573425382375717, -0.00456579215824604, -0.015477169305086136, 0.0062408712692558765, 0.03465637192130089, -0.01246713474392891, 0.015205277130007744, -0.0085604228079319, -0.005167525727301836, -0.007248604670166969, ...],
        ...
      ]
    >
  },
  "dense_21" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223969>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223970>
      [
        [0.01808825321495533, -0.02495420165359974, 0.0352095328271389, 0.023488182574510574, -0.02437545359134674, 0.03133299946784973, 0.02110445126891136, -0.010289108380675316, 0.03858323395252228, -0.021222509443759918, 0.014392636716365814, ...],
        ...
      ]
    >
  },
  "dense_8" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223986>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223987>
      [
        [-0.021432437002658844, 0.008544194512069225, -0.015631426125764847, -0.024874839931726456, -0.031187543645501137, 0.017715945839881897, -0.01988551951944828, -0.032893817871809006, 0.023676075041294098, 0.010052707977592945, ...],
        ...
      ]
    >
  },
  "dense_0" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223941>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223942>
      [
        [-0.011716327629983425, -0.01739662140607834, -0.009319826029241085, -0.0020544833969324827, 0.006512150634080172, -0.02526707947254181, 0.036585964262485504, -0.010177608579397202, 0.038096390664577484, ...],
        ...
      ]
    >
  },
  "dense_2" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223965>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223966>
      [
        [-0.01472034864127636, -0.0023772665299475193, 0.03652699664235115, 0.037270329892635345, -0.00786773581057787, -0.013120513409376144, -0.033166397362947464, 0.028871148824691772, ...],
        ...
      ]
    >
  },
  "causal_attention_3" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223913>
      [
        [-0.018020644783973694, -0.016889408230781555, 0.06197507679462433, 0.03155261278152466, -0.05690892040729523, -0.029969483613967896, 6.003230810165405e-4, -0.023847609758377075, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223914>
      [
        [0.0044265687465667725, 0.017575889825820923, 0.052302345633506775, -0.013531804084777832, -0.05668751895427704, -0.03788416087627411, -0.01857064664363861, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223915>
      [
        [0.05523097515106201, -0.0012923777103424072, -0.011561855673789978, 0.05764460563659668, 0.03261609375476837, 0.026311054825782776, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223916>
      [
        [-0.002257600426673889, 0.03910444676876068, -0.044787049293518066, -0.0100020170211792, -0.02021804451942444, ...],
        ...
      ]
    >
  },
  "normalization_21" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224019>
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224020>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "causal_attention_5" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223921>
      [
        [-0.026542946696281433, 0.05677714943885803, 0.05226212739944458, -0.006876260042190552, 0.05446557700634003, 0.048288166522979736, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223922>
      [
        [-0.025067538022994995, -0.022490888833999634, -0.015766218304634094, -0.013734295964241028, 0.03940895199775696, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223923>
      [
        [0.05397754907608032, 0.021047234535217285, 0.05870327353477478, 0.04279196262359619, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223924>
      [
        [0.05174705386161804, 0.013104304671287537, 0.02179265022277832, ...],
        ...
      ]
    >
  },
  "dense_24" => %{
    "kernel" => #Nx.Tensor<
      f32[768][50257]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223975>
      [
        [-9.29482514038682e-4, -0.006657630205154419, 0.006712297908961773, 0.009805828332901001, -0.008236919529736042, ...],
        ...
      ]
    >
  },
  "dense_14" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223953>
      [0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223954>
      [
        [-0.004327290691435337, -0.007079409901052713, 0.033612608909606934, ...],
        ...
      ]
    >
  },
  "causal_attention_4" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223917>
      [
        [0.03733636438846588, 0.024119794368743896, 0.010078296065330505, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223918>
      [
        [0.038420453667640686, 0.05353584885597229, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223919>
      [
        [-0.008414402604103088, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223920>
      [
        ...
      ]
    >
  },
  "normalization_10" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223995>
      [1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223996>
      [0.0, ...]
    >
  },
  "dense_7" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223984>
      [0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.223985>
      [
        ...
      ]
    >
  },
  "normalization_22" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4077518872.224021>
      [...]
    >,
    ...
  },
  "causal_attention_2" => %{...},
  ...
}
```

**5.1.1 Using GPT to generate text**

Generating text involves encoding text into token IDs that the LLM processes into logit vectors. The
logit vectors are then converted back into token IDs, detokenized into a text representation.

```elixir
{:ok, input} = MyGPT.text_to_token_ids(tokenizer, "I know everything")
```

<!-- livebook:{"output":true} -->

```
{:ok,
 #Nx.Tensor<
   s64[1][3]
   [
     [40, 1440, 4395]
   ]
 >}
```

```elixir
# Nx data accessing example..
example = Nx.iota({2,3,4}) |> IO.inspect()
example[0][0][..]
example[[0,..,0]]
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[2][3][4]
  [
    [
      [0, 1, 2, 3],
      [4, 5, 6, 7],
      [8, 9, 10, 11]
    ],
    [
      [12, 13, 14, 15],
      [16, 17, 18, 19],
      [20, 21, 22, 23]
    ]
  ]
>
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[3]
  [0, 4, 8]
>
```

```elixir
token_ids = MyGPT.generate_tokens(predict_fn, params, input, 6)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[1][9]
  EXLA.Backend<host:0, 0.4241110313.4077518872.224113>
  [
    [40, 1440, 4395, 38511, 31280, 31280, 31280, 31280, 31280]
  ]
>
```

```elixir
{:ok, text} = MyGPT.token_ids_to_text(tokenizer, token_ids)
IO.puts(text)
```

<!-- livebook:{"output":true} -->

```
I know everythingn.di.di.di.di.di
```

<!-- livebook:{"output":true} -->

```
:ok
```

**5.1.2 Calculating the text generation loss**

```elixir
texts = ["every effort moves", "I really like"]
inputs = MyGPT.text_to_token_ids(tokenizer, texts)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[2][3]
  [
    [30115, 5149, 11031],
    [40, 2216, 1093]
  ]
>
```

```elixir
texts = [" effort moves you", " really like chocolate"]
targets = MyGPT.text_to_token_ids(tokenizer, texts)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[2][3]
  [
    [5149, 11031, 499],
    [2216, 1093, 18414]
  ]
>
```

```elixir
# Predict next token
logits = predict_fn.(params, inputs)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][3][50257]
  EXLA.Backend<host:0, 0.4241110313.4077518872.224114>
  [
    [
      [-0.04291093349456787, -0.03042297065258026, 0.11366280913352966, -0.1469840705394745, -0.26912248134613037, -0.09862235188484192, 0.2700127363204956, 0.10541314631700516, 0.050503894686698914, -0.01206400990486145, 0.06891915947198868, -0.19689981639385223, 0.1414811909198761, 0.017571210861206055, -0.053340084850788116, 0.17562194168567657, 0.15418653190135956, 0.10786376893520355, -0.2379651665687561, -0.04442468285560608, 0.2117079645395279, 0.2389223873615265, 0.006557870656251907, -0.04115881770849228, -0.03256578743457794, -0.1623980700969696, -0.2862240672111511, -0.054230473935604095, -0.06850172579288483, -0.250613272190094, -0.11727800965309143, -0.15643112361431122, 0.0026123374700546265, 0.08873840421438217, 0.050674550235271454, -0.056649092584848404, 0.01181425154209137, 0.021480411291122437, 0.24748975038528442, -0.1292274445295334, 0.16776004433631897, -0.12058493494987488, -0.18053990602493286, 0.050429850816726685, 0.2968904376029968, 0.2754151225090027, -0.08943308889865875, 0.04023972153663635, -0.1884220689535141, 0.09173914045095444, ...],
      ...
    ],
    ...
  ]
>
```

```elixir
predicted_new_token =
  logits
  |> Axon.Layers.softmax(axis: -1)
  |> Nx.argmax(axis: -1)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[2][3]
  EXLA.Backend<host:0, 0.4241110313.4077518872.224121>
  [
    [31867, 38999, 38999],
    [31299, 38511, 38511]
  ]
>
```

```elixir
{:ok, targets_text} = MyGPT.token_ids_to_text(tokenizer, targets[0])
{:ok, outputs_text} = MyGPT.token_ids_to_text(tokenizer, predicted_new_token[0])

IO.inspect(targets_text, label: "Targets batch 1")
IO.inspect(outputs_text, label: "Outputs batch 1")
```

<!-- livebook:{"output":true} -->

```
Targets batch 1: " effort moves you"
Outputs batch 1: ".public.public"
```

<!-- livebook:{"output":true} -->

```
".public.public"
```

Before training, the model produces random next-token probability vectors. The goal of model training is to ensure that the probability values corresponding to the highlighted target token IDs are maximized.

```elixir
probas = Axon.Layers.softmax(logits, axis: -1)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][3][50257]
  EXLA.Backend<host:0, 0.4241110313.4077518872.224133>
  [
    [
      [1.8791246475302614e-5, 1.9027380403713323e-5, 2.1976306015858427e-5, 1.6933905499172397e-5, 1.498694564361358e-5, 1.777298530214466e-5, 2.56954699580092e-5, 2.179575494665187e-5, 2.0631230654544197e-5, 1.937993147294037e-5, 2.1014679077779874e-5, 1.6109386706375517e-5, 2.259623033751268e-5, 1.9962853912147693e-5, 1.859628719103057e-5, 2.3381004211842082e-5, 2.2885158614371903e-5, 2.1849231416126713e-5, 1.5461249859072268e-5, 1.8762822946882807e-5, 2.4240142010967247e-5, 2.4908880732255057e-5, 1.9744202290894464e-5, 1.882419928733725e-5, 1.898665323096793e-5, 1.667488868406508e-5, 1.4732822819496505e-5, 1.8579736206447706e-5, 1.8316462956136093e-5, 1.526692540210206e-5, 1.7444492186768912e-5, 1.677468571870122e-5, 1.9666453226818703e-5, 2.1435327653307468e-5, 2.0634750399040058e-5, 1.8534854461904615e-5, 1.9848257579724304e-5, 2.0041044990648516e-5, 2.512319952074904e-5, 1.7237280189874582e-5, 2.319790655747056e-5, 1.7386899344273843e-5, 1.6375102859456092e-5, 2.0629700884455815e-5, 2.6395469831186347e-5, 2.5834664484136738e-5, 1.7937058146344498e-5, 2.0420548025867902e-5, 1.6246540326392278e-5, 2.1499747163034044e-5, ...],
      ...
    ],
    ...
  ]
>
```

```elixir
t = Nx.iota({3, 1000}, type: :s64)
Nx.take_along_axis(t, Nx.tensor([[0], [0], [0]]), axis: 1)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[3][1]
  [
    [0],
    [1000],
    [2000]
  ]
>
```

```elixir
text_index = 0
target_1 = Nx.reshape(targets[text_index], {3,1}) |> IO.inspect()
target_probas_1 = Nx.take_along_axis(probas[text_index], target_1, axis: 1) |> Nx.reshape({3})

text_index = 1
target_2 = Nx.reshape(targets[text_index], {3,1}) |> IO.inspect()
target_probas_2 = Nx.take_along_axis(probas[text_index], target_2, axis: 1) |> Nx.reshape({3})

IO.puts("Text 1: #{inspect(Nx.to_flat_list(target_probas_1))}")
IO.puts("Text 2: #{inspect(Nx.to_flat_list(target_probas_2))}")
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[3][1]
  [
    [5149],
    [11031],
    [499]
  ]
>
#Nx.Tensor<
  s64[3][1]
  [
    [2216],
    [1093],
    [18414]
  ]
>
Text 1: [2.1511788872885518e-5, 1.9959803466917947e-5, 1.9416229406488128e-5]
Text 2: [1.8022363292402588e-5, 1.8523836843087338e-5, 1.9572255041566677e-5]
```

<!-- livebook:{"output":true} -->

```
:ok
```

The goal of training an LLM is to maximize the likelihood of the correct token, which
involves increasing its probability relative to other tokens. This way, we ensure the
LLM consistently picks the target token essentially the next word in the sentence as the next token it generates.

Backpropagation requires a loss function, which calculates the difference between
the models predicted output (here, the probabilities corresponding to the target
token IDs) and the actual desired output. This loss function measures how far off the
models predictions are from the target values.

```elixir
# Working with logarithms of probability scores is more manageable in mathematical
# optimization than handling the scores directly.

log_probas =
  [target_probas_1, target_probas_2]
  |> Nx.concatenate()
  |> Nx.log()
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[6]
  EXLA.Backend<host:0, 0.4241110313.4077518872.224143>
  [-10.746909141540527, -10.821789741516113, -10.849401473999023, -10.923896789550781, -10.896451950073242, -10.841397285461426]
>
```

```elixir
avg_log_probas = Nx.mean(log_probas)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32
  EXLA.Backend<host:0, 0.4241110313.4077518872.224145>
  -10.846640586853027
>
```

```elixir
neg_avg_log_probas = Nx.multiply(avg_log_probas, -1)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32
  EXLA.Backend<host:0, 0.4241110313.4077518872.224146>
  10.846640586853027
>
```

The goal is to get the average log probability as close to 0 as possible by updating the
models weights as part of the training process.

However, the common practice isnt to push the average log probability up to 0 but rather to bring the
negative average log probability down to 0. The negative average log probability is
simply the average log probability multiplied by 1.

In deep learning, the term for turning this negative value, is known as cross entropy loss.

The cross entropy loss is popular measure in machine learning and deep learning that measures the difference between two probability distribution.

```elixir
IO.inspect(logits.shape, label: "Logits shape:")
IO.inspect(targets.shape, label: "Targets shape:")
```

<!-- livebook:{"output":true} -->

```
Logits shape:: {2, 3, 50257}
Targets shape:: {2, 3}
```

<!-- livebook:{"output":true} -->

```
{2, 3}
```

```elixir
logits_flat = Nx.flatten(logits, axes: [0,1])
targets_flat = Nx.flatten(targets)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[6]
  [5149, 11031, 499, 2216, 1093, 18414]
>
```

```elixir
IO.inspect(logits_flat.shape, label: "Flatten Logits shape:")
IO.inspect(targets_flat.shape, label: "Flatten Targets shape:")
```

<!-- livebook:{"output":true} -->

```
Flatten Logits shape:: {6, 50257}
Flatten Targets shape:: {6}
```

<!-- livebook:{"output":true} -->

```
{6}
```

```elixir
y_true = Nx.tensor([0, 2, 1])
y_pred = Nx.tensor([[0.2, 0.8, 0.0], [0.1, 0.2, 0.7], [0.1, 0.2, 0.7]])
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[3][3]
  [
    [0.20000000298023224, 0.800000011920929, 0.0],
    [0.10000000149011612, 0.20000000298023224, 0.699999988079071],
    [0.10000000149011612, 0.20000000298023224, 0.699999988079071]
  ]
>
```

```elixir
loss = Axon.Losses.categorical_cross_entropy(
  targets_flat, logits_flat, reduction: :mean, from_logits: true, sparse: true)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32
  EXLA.Backend<host:0, 0.4241110313.4077518872.224159>
  10.846641540527344
>
```

**Perplexity**

Perplexity is a measure often used alongside cross entropy loss to evaluate the performance of models in tasks like language modeling. It can provide a more interpretable way to understand the uncertainty of a model in predicting the next token in a
sequence.

Perplexity measures how well the probability distribution predicted by the model
matches the actual distribution of the words in the dataset. Similar to the loss, a lower
perplexity indicates that the model predictions are closer to the actual distribution.

Perplexity is often considered more interpretable than the raw loss value because it signifies the effective vocabulary size about which the model is uncertain at each step. In
the given example, this would translate to the model being unsure about which among
48,725 tokens in the vocabulary to generate as the next token.

```elixir
perplexity = Nx.exp(loss)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32
  EXLA.Backend<host:0, 0.4241110313.4077518872.224160>
  51361.3671875
>
```

**5.1.3 Calculating the training and validation set losses**

```elixir
path = 
  "/home/alde/Documents/MyDevelopment/Build_A_Large_Language_Model/the-verdict.txt"
{:ok, raw_text} = File.read(path)
```

<!-- livebook:{"output":true} -->

```
{:ok,
 "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n\n\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n\nWell!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome \"obituary\" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of \"Gisburns\" went up.\n\nIt was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.\n\nOf course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to \"lift him up\"--she had not led him back to the easel. To put the brush into his hand again--what a vocation for a wife! But Mrs. Gisburn appeared to have disdained it--and I felt it might be interesting to find out why.\n\nThe desultory life of the Riviera lends itself to such purely academic speculations; and having, on my way to Monte Carlo, caught a glimpse of Jack's balustraded terraces between the pines, I had myself borne thither the next day.\n\nI found the couple at tea beneath their palm-trees; and Mrs. Gisburn's welcome was so genial that, in the ensuing weeks, I claimed it frequently. It was not that my hostess was \"interesting\": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I may be pardoned the bull--that I found her so. For Jack, all his life, had been surrounded by interesting women: they had fostered his art, it had been reared in the hot-house of their adulation. And it was therefore instructive to note what effect the \"deadening atmosphere of mediocrity\" (I quote Miss Croft) was having on him.\n\nI have mentioned that Mrs. Gisburn was rich; and it was immediately perceptible that her husband was extracting from this circumstance a delicate but substantial satisfaction. It is, as a rule, the people who scorn money who get most out of it; and Jack's elegant disdain of his wife's big balance enabled him, with an appearance of perfect good-breeding, to transmute it into objects of art and luxury. To the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eight" <> ...}
```

```elixir
{:ok, ids} = Tiktoken.encode(tokenizer, raw_text)
```

<!-- livebook:{"output":true} -->

```
{:ok,
 [40, 473, 1846, 2744, 3463, 7762, 480, 285, 22464, 4856, 264, 12136, 35201, 313, 4636, 264, 1695,
  12637, 3403, 313, 708, 433, 574, 912, 2294, 13051, 311, 757, 311, 6865, 430, 11, 304, 279, 2673,
  315, 813, 27025, 11, 568, 1047, 12504, 813, 19354, 11, 12502, 264, 9257, ...]}
```

```elixir
String.length(raw_text) |> IO.inspect(label: "Characters")
Enum.count(ids) |> IO.inspect(label: "Tokens")
```

<!-- livebook:{"output":true} -->

```
Characters: 20479
Tokens: 4943
```

<!-- livebook:{"output":true} -->

```
4943
```

When preparing the data loaders, we split the input text into training and validation set portions. Then we tokenize the text and divide the tokenized text into
chunks of a user-specified length. Finally, we shuffle the rows and organize the chunked text into batches, which we can use for model training

```elixir
train_ratio = 0.90

split_index = floor(train_ratio * String.length(raw_text))

{train_data, validation_data} = String.split_at(raw_text, split_index)

String.length(train_data) |> IO.inspect(label: "Train Characters")
String.length(validation_data) |> IO.inspect(label: "Validation Characters")
```

<!-- livebook:{"output":true} -->

```
Train Characters: 18431
Validation Characters: 2048
```

<!-- livebook:{"output":true} -->

```
2048
```

We split the input text into training and validation set portions. Then
we tokenize the text and divide the tokenized text into
chunks of a user-specified length. Finally, we shuffle the rows and organize the chunked text into batches, which we can use for model training.

However, in practice, it can also be
beneficial to train an LLM with variable-length inputs to help the LLM to better generalize across different types of inputs when it is being used.

```elixir
defmodule MyGPT.DatasetV1 do
  def build(txt, tokenizer_model, max_length, stride) do
    {:ok, token_ids} = Tiktoken.encode(tokenizer_model, txt)

    token_ids_tensor = Nx.tensor(token_ids)
    text_length = length(token_ids)

    linespace =
      Enum.to_list(0..(length(token_ids) - max_length - 1)) |> Enum.take_every(stride)

    for i <- linespace, reduce: %{input_ids: [], target_ids: []} do
      %{input_ids: input_ids, target_ids: target_ids} = acc ->

        
        {input_ids, target_ids} =
          cond do
            i + max_length > text_length - 1 ->
              {input_ids, target_ids}

            i + max_length + 1 > text_length - 1 ->
              {input_ids, target_ids}

            true ->
              input_chunk = token_ids_tensor[i..(i + max_length - 1)]
              target_chunk = token_ids_tensor[(i + 1)..(i + max_length)]
              {input_ids ++ [input_chunk], target_ids ++ [target_chunk]}
          end

        %{acc | input_ids: input_ids, target_ids: target_ids}
    end
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, MyGPT.DatasetV1, <<70, 79, 82, 49, 0, 0, 12, ...>>, {:build, 4}}
```

```elixir
context_length = gpt_config_124m[:context_length]
train_dataset = MyGPT.DatasetV1.build(train_data, tokenizer, context_length, context_length)
validation_dataset = MyGPT.DatasetV1.build(validation_data, tokenizer, context_length, context_length)
```

<!-- livebook:{"output":true} -->

```
%{
  input_ids: [
    #Nx.Tensor<
      s64[256]
      [361, 6, 29368, 1093, 264, 3838, 315, 7563, 13, 1283, 3287, 956, 21423, 261, 11, 499, 3619, 11, 8009, 4610, 3023, 313, 383, 1120, 11203, 1070, 30666, 10307, 11, 323, 389, 813, 23726, 11, 1555, 279, 18004, 48788, 11, 358, 9508, 311, 6865, 279, 3488, 25, 364, 11787, ...]
    >,
    #Nx.Tensor<
      s64[256]
      [364, 10655, 6, 555, 1063, 832, 1501, 88, 0, 2468, 1176, 358, 574, 16984, 1364, 8434, 956, 1095, 757, 1022, 313, 438, 520, 856, 289, 1220, 6, 842, 358, 12090, 2895, 58863, 13, 7566, 11, 433, 574, 358, 889, 3940, 2895, 58863, 25, 358, 3309, 18083, 13, ...]
    >
  ],
  target_ids: [
    #Nx.Tensor<
      s64[256]
      [6, 29368, 1093, 264, 3838, 315, 7563, 13, 1283, 3287, 956, 21423, 261, 11, 499, 3619, 11, 8009, 4610, 3023, 313, 383, 1120, 11203, 1070, 30666, 10307, 11, 323, 389, 813, 23726, 11, 1555, 279, 18004, 48788, 11, 358, 9508, 311, 6865, 279, 3488, 25, 364, 11787, ...]
    >,
    #Nx.Tensor<
      s64[256]
      [10655, 6, 555, 1063, 832, 1501, 88, 0, 2468, 1176, 358, 574, 16984, 1364, 8434, 956, 1095, 757, 1022, 313, 438, 520, 856, 289, 1220, 6, 842, 358, 12090, 2895, 58863, 13, 7566, 11, 433, 574, 358, 889, 3940, 2895, 58863, 25, 358, 3309, 18083, 13, ...]
    >
  ]
}
```

```elixir
input_ids = Nx.stack(train_dataset[:input_ids])
target_ids = Nx.stack(train_dataset[:target_ids])

{input_ids[0], target_ids[0]}
```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   s64[256]
   [40, 473, 1846, 2744, 3463, 7762, 480, 285, 22464, 4856, 264, 12136, 35201, 313, 4636, 264, 1695, 12637, 3403, 313, 708, 433, 574, 912, 2294, 13051, 311, 757, 311, 6865, 430, 11, 304, 279, 2673, 315, 813, 27025, 11, 568, 1047, 12504, 813, 19354, 11, 12502, 264, 9257, 57896, ...]
 >,
 #Nx.Tensor<
   s64[256]
   [473, 1846, 2744, 3463, 7762, 480, 285, 22464, 4856, 264, 12136, 35201, 313, 4636, 264, 1695, 12637, 3403, 313, 708, 433, 574, 912, 2294, 13051, 311, 757, 311, 6865, 430, 11, 304, 279, 2673, 315, 813, 27025, 11, 568, 1047, 12504, 813, 19354, 11, 12502, 264, 9257, 57896, ...]
 >}
```

```elixir
v_input_ids = Nx.stack(validation_dataset[:input_ids])
v_target_ids = Nx.stack(validation_dataset[:target_ids])

{v_input_ids[0], v_target_ids[0]}
```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   s64[256]
   [361, 6, 29368, 1093, 264, 3838, 315, 7563, 13, 1283, 3287, 956, 21423, 261, 11, 499, 3619, 11, 8009, 4610, 3023, 313, 383, 1120, 11203, 1070, 30666, 10307, 11, 323, 389, 813, 23726, 11, 1555, 279, 18004, 48788, 11, 358, 9508, 311, 6865, 279, 3488, 25, 364, 11787, 499, ...]
 >,
 #Nx.Tensor<
   s64[256]
   [6, 29368, 1093, 264, 3838, 315, 7563, 13, 1283, 3287, 956, 21423, 261, 11, 499, 3619, 11, 8009, 4610, 3023, 313, 383, 1120, 11203, 1070, 30666, 10307, 11, 323, 389, 813, 23726, 11, 1555, 279, 18004, 48788, 11, 358, 9508, 311, 6865, 279, 3488, 25, 364, 11787, 499, ...]
 >}
```

```elixir
stream = Nx.to_batched(input_ids, 1)
x = Enum.at(stream, 0)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[1][256]
  [
    [40, 473, 1846, 2744, 3463, 7762, 480, 285, 22464, 4856, 264, 12136, 35201, 313, 4636, 264, 1695, 12637, 3403, 313, 708, 433, 574, 912, 2294, 13051, 311, 757, 311, 6865, 430, 11, 304, 279, 2673, 315, 813, 27025, 11, 568, 1047, 12504, 813, 19354, 11, 12502, 264, 9257, 57896, 11, ...]
  ]
>
```

```elixir
defmodule MyGPT.Dataset do
  def build(txt, tokenizer_model, max_length, stride, batch_size \\ 2, is_shuffle? \\ true) do
    {:ok, token_ids} = Tiktoken.encode(tokenizer_model, txt)

    token_ids_tensor = Nx.tensor(token_ids)
    text_length = length(token_ids)

    linespace =
      Enum.to_list(0..(length(token_ids) - max_length - 1)) |> Enum.take_every(stride)

    %{input_ids: input_ids, target_ids: target_ids} =
      for i <- linespace, reduce: %{input_ids: [], target_ids: []} do
        %{input_ids: input_ids, target_ids: target_ids} = acc ->
          {input_ids, target_ids} =
            cond do
              i + max_length > text_length - 1 ->
                {input_ids, target_ids}

              i + max_length + 1 > text_length - 1 ->
                {input_ids, target_ids}

              true ->
                input_chunk = token_ids_tensor[i..(i + max_length - 1)]
                target_chunk = token_ids_tensor[(i + 1)..(i + max_length)]
                {input_ids ++ [input_chunk], target_ids ++ [target_chunk]}
            end

          %{acc | input_ids: input_ids, target_ids: target_ids}
      end

    {shuffled_input_ids, shuffled_target_ids} =
      try_shuffle_datasets(input_ids, target_ids, is_shuffle?)

    input_ids_stream =
      shuffled_input_ids
      |> Nx.stack()
      |> Nx.to_batched(batch_size)

    target_ids_stream =
      shuffled_target_ids
      |> Nx.stack()
      |> Nx.to_batched(batch_size)

    Stream.zip(input_ids_stream, target_ids_stream)
  end

  def try_shuffle_datasets(input_ids, target_ids, true) do
    Enum.zip(input_ids, target_ids)
    |> Enum.shuffle()
    |> Enum.unzip()
  end

  def try_shuffle_datasets(input_ids, target_ids, _is_shuffle?) do
    {input_ids, target_ids}
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, MyGPT.Dataset, <<70, 79, 82, 49, 0, 0, 17, ...>>, {:try_shuffle_datasets, 3}}
```

```elixir
context_length = gpt_config_124m[:context_length]

training_dataset =
  MyGPT.Dataset.build(train_data, tokenizer, context_length, context_length)

validation_dataset =
  MyGPT.Dataset.build(validation_data, tokenizer, context_length, context_length)
```

<!-- livebook:{"output":true} -->

```
#Function<73.53678557/2 in Stream.zip_with/2>
```

```elixir
Enum.at(training_dataset, 0)
```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   s64[2][256]
   [
     [721, 16514, 62, 2163, 4920, 313, 28753, 568, 1047, 2586, 311, 4822, 0, 578, 2800, 315, 603, 1047, 311, 1095, 13520, 387, 41323, 3235, 477, 733, 1234, 11, 719, 568, 574, 1579, 3485, 279, 1510, 313, 263, 96049, 41582, 11, 439, 499, 2019, 382, 56084, 11, 358, 4024, 1022, ...],
     ...
   ]
 >,
 #Nx.Tensor<
   s64[2][256]
   [
     [16514, 62, 2163, 4920, 313, 28753, 568, 1047, 2586, 311, 4822, 0, 578, 2800, 315, 603, 1047, 311, 1095, 13520, 387, 41323, 3235, 477, 733, 1234, 11, 719, 568, 574, 1579, 3485, 279, 1510, 313, 263, 96049, 41582, 11, 439, 499, 2019, 382, 56084, 11, 358, 4024, 1022, ...],
     ...
   ]
 >}
```

We used a relatively small batch size to reduce the computational resource demand
because we were working with a very small dataset. In practice, training LLMs with
batch sizes of 1,024 or larger is not uncommon.

## 5.2 Training an LLM

A typical training loop for training deep neural networks consists of numerous steps, iterating over the batches in the training
set for several epochs. In each loop, we calculate the loss for each training
set batch to determine loss gradients, which we use to update the model
weights so that the training set loss is minimized.

It outlines eight steps, starting with iterating
over each epoch, processing batches, resetting gradients, calculating the loss and new gradients, and updating weights and concluding with monitoring steps like printing
losses and generating text samples.

More advanced techniques, including learning rate warmup, cosine annealing, and gradient clipping.

Adam optimizers are a popular choice for training deep neural networks. However, in
our training loop, we opt for the AdamW optimizer. AdamW is a variant of Adam that
improves the weight decay approach, which aims to minimize model complexity and
prevent overfitting by penalizing larger weights. This adjustment allows AdamW to
achieve more effective regularization and better generalization; thus, AdamW is frequently used in the training of LLMs

```elixir
model = MyGPT.model({nil, nil, 768}, gpt_config_124m)
{init_fn, predict_fn} = Axon.build(model, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```
{#Function<134.37423472/2 in Nx.Defn.Compiler.fun/2>,
 #Function<134.37423472/2 in Nx.Defn.Compiler.fun/2>}
```

```elixir
loss_fn = fn y_true, y_pred ->
  logits_flat = Nx.flatten(y_pred, axes: [0, 1])
  targets_flat = Nx.flatten(y_true)

  Axon.Losses.categorical_cross_entropy(targets_flat, logits_flat,
    reduction: :mean,
    from_logits: true,
    sparse: true
  )
end

optimizer = Polaris.Optimizers.adamw(learning_rate: 0.0004, decay: 0.1)

trained_model_state =
  model
  |> Axon.Loop.trainer(loss_fn, optimizer)
  |> Axon.Loop.validate(model, validation_dataset)
  |> Axon.Loop.run(training_dataset, %{}, epochs: 20, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```

16:55:07.659 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Epoch: 0, Batch: 0, loss: 0.0000000
Batch: 0, loss: 7.9225187
Batch: 0, loss: 6.7663846
Batch: 0, loss: 6.8652787
Batch: 0, loss: 6.8920698
Epoch: 5, Batch: 5, loss: 7.0758638
Batch: 0, loss: 6.9999743
Batch: 0, loss: 7.0172462
Batch: 0, loss: 7.0102053
Batch: 0, loss: 7.0061049
Batch: 0, loss: 7.0052652
Epoch: 11, Batch: 1, loss: 6.7770414
Batch: 0, loss: 7.0175781
Batch: 0, loss: 7.0251579
Batch: 0, loss: 7.0322456
Batch: 0, loss: 7.0395260
Epoch: 16, Batch: 6, loss: 6.6486812
Batch: 0, loss: 7.0521998
Batch: 0, loss: 7.0579853
Batch: 0, loss: 7.0637093
Batch: 0, loss: 7.0678997
```

<!-- livebook:{"output":true} -->

```
%{
  "pos_embedding_0" => %{
    "kernel" => #Nx.Tensor<
      f32[256][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114217>
      [
        [0.0030399442184716463, -0.015000810846686363, 0.008799823932349682, 0.0024707489646971226, -0.004363138694316149, -0.0027438735123723745, -0.004649634473025799, 0.011180869303643703, -0.01126465480774641, -0.015655651688575745, 0.0035278170835226774, -0.0020431303419172764, 0.0015372964553534985, -0.00501622911542654, 0.002052068244665861, -0.01119367964565754, 0.0036937538534402847, -0.004639735911041498, 0.008391641080379486, 0.01000027172267437, -0.016940027475357056, 0.005305171478539705, -0.015996214002370834, -0.0034700511023402214, 0.0030427563469856977, 0.010560252703726292, 0.004489585291594267, -9.93648063740693e-5, 0.014040867798030376, -0.011703267693519592, 0.001431313925422728, -0.00245914189144969, -0.002831702819094062, -0.006486359052360058, 0.006101747043430805, -0.009561961516737938, 9.90671687759459e-4, -0.0024925717152655125, -0.002727844985201955, -0.016208838671445847, 0.006995122414082289, 0.013075376860797405, -0.007616542745381594, 0.003244374878704548, -0.01292124018073082, -0.008164004422724247, -0.013832850381731987, -0.005320919211953878, ...],
        ...
      ]
    >
  },
  "normalization_6" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114209>
      [0.9881671667098999, 0.9936465620994568, 0.9932978749275208, 0.9929004907608032, 0.9957296848297119, 0.9902194738388062, 0.9947227239608765, 0.9935739636421204, 0.9874643683433533, 0.9874800443649292, 0.9888321161270142, 0.9909698963165283, 0.9910414218902588, 0.9932007789611816, 0.992409348487854, 0.9977383017539978, 0.9899377822875977, 0.9844974279403687, 0.9867213368415833, 0.9893515706062317, 0.9882872700691223, 0.9924044013023376, 0.9925130009651184, 0.9854100346565247, 0.9872530102729797, 0.9906017780303955, 0.9886538982391357, 0.9866654276847839, 0.9951047897338867, 0.9978554248809814, 0.9879499077796936, 0.9936858415603638, 0.9855512976646423, 0.9996060729026794, 0.9964430332183838, 0.9910537600517273, 0.9966010451316833, 0.9909834265708923, 0.9841140508651733, 0.9867188930511475, 0.9903198480606079, 0.9870376586914062, 0.9909690618515015, 0.9941594004631042, 0.9930077791213989, 0.9933387637138367, 0.9953321218490601, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114210>
      [0.004418936092406511, 0.007508032489567995, -0.005748335737735033, -0.007209074217826128, 0.007042011246085167, -0.002640129765495658, 0.0013427408412098885, -0.005164695903658867, -0.005757559090852737, 0.005995718762278557, -0.0027783650439232588, 0.002478603972122073, 0.00763765349984169, -0.007522911764681339, -0.0069214897230267525, -0.007442291360348463, -0.0066259996965527534, -6.13777490798384e-4, 0.0032776310108602047, -0.007582308258861303, -0.0022544963285326958, 0.006809221114963293, 0.003630455583333969, 0.0025939869228750467, -0.0015721970703452826, -0.0015196906169876456, -0.008048253133893013, 0.0016210178146138787, -0.006136337760835886, -0.008166286163032055, -0.007432081736624241, -0.005888410843908787, -0.002023588865995407, -0.0070847151800990105, -0.004683980252593756, 0.004460476338863373, -0.00786486268043518, 0.0013758030254393816, -0.004949749913066626, -0.004467439372092485, -1.3045653759036213e-4, 0.0018069026991724968, -0.004849700722843409, -0.0044114915654063225, 0.002597477752715349, 0.004446682054549456, ...]
    >
  },
  "dense_4" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114129>
      [0.004130190704017878, -0.0026303112972527742, 0.004634550306946039, -0.007281397935003042, 9.150980040431023e-4, 0.004659218247979879, -0.003003534162417054, 0.006177010014653206, 6.556932930834591e-4, -0.005460980348289013, 6.361347041092813e-4, 0.0021716521587222815, -0.004280936438590288, -0.006064781919121742, -0.006669355556368828, -0.005678904242813587, 0.007667467929422855, 0.00580223323777318, 0.004892511293292046, -0.00662256358191371, -0.002682086545974016, -0.0037210264708846807, 0.005786593537777662, -0.0063101546838879585, -2.7676692116074264e-4, -0.0030621143523603678, -0.0058396547101438046, -0.0048673138953745365, 0.00185766676440835, 0.0018679298227652907, 0.0020015661139041185, 0.008332083001732826, 7.120530935935676e-4, 0.006180423777550459, 0.003107501892372966, -0.004848566837608814, 0.006494876462966204, 0.001875000772997737, 0.004197766538709402, 9.788728784769773e-4, -0.0036299447529017925, -0.0020839208737015724, 0.0015199586050584912, 0.004761234391480684, -0.007217577192932367, 0.004982600454241037, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114130>
      [
        [0.018120167776942253, -0.03314176946878433, 0.015474965795874596, -0.002141403267160058, 0.0015130251413211226, 0.014732364565134048, 0.03494704142212868, 0.027320237830281258, 0.014111627824604511, 0.005388427060097456, -0.004013235215097666, -0.040662892162799835, -0.0015232254518195987, 0.020755097270011902, 0.034432943910360336, -0.01151410024613142, -0.0027022790163755417, -0.0032055117189884186, 0.021071389317512512, 0.00876476988196373, 0.0228728074580431, 0.0010587595170363784, 0.008413932286202908, -0.03860553354024887, -0.007403239142149687, 0.011007956229150295, 0.02114753983914852, -0.01934555359184742, 0.005362847354263067, 0.01898827590048313, -0.008539404720067978, 0.00297253648750484, -0.04184574633836746, 0.034659966826438904, 0.03257404640316963, 0.041142236441373825, -2.7561612660065293e-4, 0.010497859679162502, 0.028703268617391586, -0.017276428639888763, 0.028949923813343048, 0.030291173607110977, 0.03176956996321678, -0.006378664635121822, 0.02934279665350914, ...],
        ...
      ]
    >
  },
  "normalization_5" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114207>
      [0.992104172706604, 0.9953243732452393, 0.9919561743736267, 0.9884811043739319, 0.9955224990844727, 0.9875226020812988, 0.9966453909873962, 0.9900718331336975, 0.9952645301818848, 0.9943210482597351, 0.9979885816574097, 0.9990644454956055, 0.9884477853775024, 0.9896005392074585, 0.9910345077514648, 0.9935102462768555, 0.9896595478057861, 0.995361864566803, 0.9895854592323303, 0.9918802976608276, 0.9956544041633606, 0.994002640247345, 0.9860469102859497, 0.9912488460540771, 0.9861905574798584, 0.9959331750869751, 0.9925298690795898, 0.9876217246055603, 0.9973623752593994, 0.993625819683075, 0.9946304559707642, 0.9877323508262634, 0.9861040115356445, 0.9877849221229553, 1.0000427961349487, 0.9883211255073547, 0.9995887875556946, 0.988390326499939, 0.9880185127258301, 0.9921659827232361, 0.9950442910194397, 0.9930357336997986, 0.9918837547302246, 1.0007890462875366, 0.9961903691291809, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114208>
      [0.0010120560182258487, 0.0051265899091959, -0.0021477006375789642, -9.170882985927165e-4, 0.005985994357615709, -0.005046761129051447, 0.007831452414393425, -0.002402098150923848, 0.003001733450219035, -0.005542136263102293, -0.0014762147329747677, 0.006572904996573925, 0.0022997932974249125, -0.007743016351014376, -0.005099151749163866, -0.007364107761532068, 0.004213055595755577, -0.008479777723550797, 0.00829504244029522, -0.00668569840490818, 0.0032377620227634907, 0.00810111965984106, -0.0051840851083397865, -0.005619338247925043, 0.0064458418637514114, 0.007467782124876976, -0.001679958077147603, 0.003988742828369141, -0.007700372487306595, -0.002345713786780834, -0.00330195389688015, -0.004618449602276087, 0.005369225982576609, -0.0010392750846222043, 0.006674506235867739, 0.005222995765507221, -0.006280179601162672, 0.0066515374928712845, -0.0021259256172925234, 0.006554515566676855, -0.004720482509583235, -0.007189747877418995, 0.008163010701537132, -0.007438535336405039, ...]
    >
  },
  "dense_20" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114118>
      [3.2467405617353506e-6, -0.0069378213956952095, 0.0060866340063512325, -0.006169722881168127, 0.004127630032598972, 0.001191477058455348, -0.0053716618567705154, -0.0052217585034668446, -0.002047355519607663, 0.003800378879532218, 0.0038457168266177177, -0.006137889344245195, -0.0015620712656527758, -0.0041993227787315845, 4.386368382256478e-4, -0.002327426103875041, 0.004197706002742052, -0.008325462229549885, -0.006502589210867882, -0.005882250610738993, -0.00652487063780427, -0.0041940356604754925, 0.006113006733357906, -0.004120873287320137, -0.0020273802801966667, -0.007001400925219059, -0.00610163202509284, -0.005337153561413288, 0.005784623324871063, -0.003777584293857217, 0.001971307210624218, 0.005334820132702589, 0.0032906599808484316, 0.004551365040242672, 0.005629242397844791, -0.006470643449574709, 0.002506356919184327, -0.006742286961525679, -8.050311589613557e-4, -0.003878294490277767, 0.008114843629300594, 0.002546182833611965, 0.00566401332616806, -0.0022262558341026306, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114119>
      [
        [0.02476060763001442, 0.01905149407684803, -0.03329908475279808, -0.036734744906425476, -0.022596532478928566, -0.0226023830473423, 0.038417596369981766, 0.0013446967350319028, 0.017317995429039, -0.028620615601539612, 0.03660671412944794, -0.0041865636594593525, -0.019605161622166634, 0.02636108547449112, 0.009344477206468582, -0.011284420266747475, 0.010165654122829437, -0.03645019233226776, -0.028098708018660545, -0.002790372585877776, -0.020210418850183487, -0.02877851389348507, -0.025127768516540527, 0.016502132639288902, -0.024024389684200287, 0.02107885666191578, -0.00723908981308341, -0.03581039607524872, -0.0292682945728302, -0.03910881280899048, -0.00429993961006403, 0.030655262991786003, -0.008778020739555359, 0.027835281565785408, 0.017165644094347954, -0.009597741067409515, 0.030317282304167747, -0.03586767986416817, 0.027979090809822083, -0.026674596592783928, 0.019736342132091522, 0.01599903218448162, -0.021078001707792282, ...],
        ...
      ]
    >
  },
  "normalization_1" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114169>
      [0.9971684813499451, 0.9893696904182434, 0.9993425607681274, 0.9977664947509766, 0.9938521981239319, 0.9942303895950317, 0.9944689273834229, 0.9882295727729797, 0.9860831499099731, 0.9978235960006714, 0.9898713827133179, 0.9878084659576416, 0.9894587993621826, 0.988105058670044, 0.9914026260375977, 0.9962813258171082, 0.9977692365646362, 1.0000258684158325, 0.9939664006233215, 0.9873087406158447, 0.9998933672904968, 0.9974073171615601, 0.988460123538971, 0.9897100329399109, 0.9981680512428284, 0.9936938285827637, 0.9920530319213867, 0.9915438294410706, 0.9991135597229004, 0.9879347681999207, 0.9978821277618408, 0.9863351583480835, 0.9983649849891663, 0.9979074001312256, 0.9942027926445007, 0.9961107969284058, 0.9968167543411255, 0.9871593117713928, 0.988778293132782, 1.0031331777572632, 0.9979093074798584, 0.9881286025047302, 0.9926728010177612, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114170>
      [-0.0025942930951714516, -3.685219562612474e-4, -0.002641119994223118, -0.0059630353935062885, 0.007222763262689114, -5.021836841478944e-4, 0.006619688123464584, -9.416748071089387e-4, -0.005866349674761295, -0.004211109597235918, 0.005702258087694645, 0.0018735132180154324, -0.005684790667146444, 0.004388484638184309, 0.0012303298572078347, -0.006314154248684645, -0.0054289232939481735, 0.007661104667931795, 0.002560243010520935, -1.6354241233784705e-4, -0.006755437236279249, 0.005753557663410902, -0.0021775555796921253, 0.00618473906069994, -0.006056117359548807, 0.001452515716664493, -0.002131944289430976, -2.83218250842765e-4, -0.001731817377731204, 8.027399308048189e-4, -0.004276017192751169, -3.388166951481253e-4, 0.0065053049474954605, 0.0057502188719809055, 0.006246866192668676, 0.0019459528848528862, -0.007256271783262491, -0.0017089000903069973, 0.004711898509413004, 0.004997627809643745, -0.005615650676190853, -0.002964241662994027, ...]
    >
  },
  "normalization_9" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114215>
      [0.99433833360672, 0.9985561966896057, 0.9886056184768677, 0.9969040751457214, 0.9912809729576111, 0.9902849793434143, 0.9861438870429993, 0.9941280484199524, 0.999897837638855, 0.9897254109382629, 0.9963468313217163, 0.9974981546401978, 0.9949955344200134, 0.9880355596542358, 0.9925223588943481, 0.992489218711853, 0.9898313283920288, 0.9927253723144531, 0.9922938942909241, 0.9932757616043091, 1.0006639957427979, 0.9928953051567078, 0.9879754781723022, 0.9946908950805664, 0.9920088052749634, 0.9940914511680603, 0.9986709356307983, 0.9985308647155762, 0.9922637343406677, 1.0011239051818848, 0.992411196231842, 0.9907612204551697, 0.9968470335006714, 0.9905659556388855, 0.9982918500900269, 0.9884299635887146, 0.994756281375885, 0.9925292730331421, 0.9937831163406372, 0.9918406009674072, 0.9886872172355652, 0.9921319484710693, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114216>
      [0.007347702980041504, 0.005349504761397839, -0.005709194578230381, 0.0010550884762778878, 0.005714147351682186, -0.004842476453632116, 0.0011991539504379034, 0.0076274508610367775, 0.007510954048484564, 0.0050627184100449085, 0.00587485870346427, 0.006648254115134478, 0.007326774764806032, -0.006325601600110531, -0.006786382757127285, -0.005170431453734636, -0.0063715227879583836, 0.0019999879878014326, 0.007076435722410679, -0.0016706050373613834, 0.0030278440099209547, 0.00734576815739274, -0.0032944174017757177, -0.006153439171612263, -0.0032726251520216465, 0.0055737304501235485, -0.004211990162730217, 0.0010531243169680238, -0.0042388527654111385, -0.0063254558481276035, -0.003062865696847439, -0.0051261186599731445, 0.004945783410221338, -0.003914372529834509, -0.0017044995911419392, 9.536842117086053e-4, -0.0040360260754823685, -0.0034556302707642317, -0.0061850170604884624, 0.0024947228375822306, 0.0010378488805145025, ...]
    >
  },
  "dense_22" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114122>
      [-0.007063629571348429, 0.003839203855022788, 0.005165083799511194, 0.0052675846964120865, 0.0063400110229849815, -8.766390383243561e-4, 4.3988952529616654e-4, 0.0028738267719745636, -0.0011932302732020617, 0.004422477912157774, 0.006119906436651945, 0.005149764008820057, 0.0034087514504790306, 7.777076098136604e-4, -0.0033587345387786627, -0.0059297652915120125, 0.005491173360496759, 0.006787109188735485, -0.00590151222422719, -0.00338618247769773, 0.0010769659420475364, 0.0036471853964030743, 0.005543907172977924, -0.004430479370057583, 0.003920561168342829, 8.171760709956288e-4, 0.00391831761226058, 0.00617824075743556, 0.005042041651904583, 0.0027659605257213116, -0.00723277498036623, 0.00235378323122859, -0.003784571308642626, 0.004360120743513107, -0.0021053131204098463, -0.006750102620571852, -8.667183574289083e-4, 0.005803206469863653, -0.004573475103825331, 0.005862499587237835, -3.91518697142601e-4, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114123>
      [
        [0.025972910225391388, -0.02394138090312481, 0.026994766667485237, 0.02607397921383381, -0.017276693135499954, 0.009398252703249454, 0.02357492595911026, 0.017919771373271942, 0.011056163348257542, -0.019334346055984497, 0.00862677302211523, 0.0027103160973638296, -0.021803243085741997, -0.04309016093611717, 0.032548386603593826, -0.03928568586707115, -0.02372429333627224, 0.025197643786668777, 0.007029031869024038, 0.015247220173478127, 0.017062518745660782, 0.015476973727345467, 0.020870523527264595, 0.021600069478154182, 0.019561586901545525, -6.12822244875133e-4, 0.029458148404955864, -0.009709463454782963, 0.02322700433433056, -0.03147292509675026, 0.028880057856440544, -0.020214177668094635, -0.014486081898212433, 0.022213585674762726, -0.025133727118372917, 0.036012522876262665, 0.0016241793055087328, 0.021070638671517372, -0.012895442545413971, 0.012310782447457314, ...],
        ...
      ]
    >
  },
  "causal_attention_0" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114044>
      [
        [-0.03190459683537483, -0.04833981394767761, -0.01714795082807541, 0.05691982060670853, -0.03131192922592163, 0.02842801809310913, 0.033316630870103836, -0.050918858498334885, 0.021348973736166954, 0.030232029035687447, 0.027273843064904213, 0.009381406009197235, -0.058054886758327484, 0.01224635448306799, -0.031864482909440994, 0.058605216443538666, -0.04096430540084839, 0.0450139157474041, 0.0550924576818943, 0.00885833241045475, -0.020169438794255257, 0.028940491378307343, 0.005889038555324078, 0.06521978974342346, -0.017850054427981377, -0.0067937797866761684, 0.006620303727686405, 0.016171960160136223, -0.050304338335990906, -0.001194419339299202, 0.05464266240596771, 0.04246978834271431, 0.052126623690128326, 0.007566822227090597, -0.05323030799627304, 0.056584153324365616, -0.02039552479982376, -0.03059125877916813, 0.01807730831205845, -0.059929534792900085, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114045>
      [
        [-0.03612754866480827, 0.014264589175581932, 0.0015374718932434916, -0.06411539018154144, -0.008726324886083603, 0.04362591728568077, -0.021514730527997017, -0.01870274730026722, -0.052058931440114975, 0.037432555109262466, -0.03652185574173927, 0.049552273005247116, -0.06725195795297623, -0.03751663491129875, 0.023754911497235298, -0.02139483578503132, -0.04629501700401306, -0.03845696151256561, 0.04747553542256355, 0.020071741193532944, -0.020295213907957077, 0.040968265384435654, 0.029054824262857437, 0.04899752885103226, -0.04975687339901924, 0.012949435971677303, 0.03960154578089714, -0.04424417391419411, -0.021048998460173607, 0.0054325321689248085, -0.016695594415068626, 0.016579052433371544, 0.03392345458269119, -0.004145894665271044, 0.010928850620985031, 0.02012365311384201, 0.05454104021191597, 0.027817731723189354, -0.011976329609751701, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114046>
      [
        [-0.04213320091366768, -0.015898803249001503, -0.019393667578697205, 0.040266092866659164, -0.037901900708675385, -0.0016197430668398738, 0.054125431925058365, -0.058515049517154694, 0.04163810610771179, 0.055240124464035034, 0.01184991467744112, 0.05403432622551918, -0.03105376847088337, 0.004928861744701862, -0.05210597813129425, 0.03226574882864952, 0.0512205995619297, 0.0023611325304955244, 0.032273996621370316, -0.034624431282281876, -0.0012767023872584105, 0.010047739371657372, -0.03860597312450409, -0.05967193841934204, -0.06058037653565407, 0.030071185901761055, -0.025169312953948975, -0.024205569177865982, -0.0526730902493, 0.05600697547197342, -0.019849032163619995, 0.04860305413603783, -0.019166575744748116, 0.05082762986421585, 0.034711699932813644, -0.06392467767000198, 0.0102485166862607, -0.06329268217086792, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114047>
      [
        [-0.04188407585024834, -0.03747906535863876, 0.0354362353682518, -0.06176931411027908, -0.01754056103527546, -0.030482489615678787, -0.03863369673490524, 0.03865433111786842, 0.043976038694381714, 0.05244244262576103, -0.011927432380616665, -0.027401864528656006, 0.02994217351078987, 0.005431101657450199, -0.043688222765922546, -6.958766025491059e-4, -0.049249980598688126, 0.05739963427186012, 0.020485443994402885, 0.03345131129026413, -0.047021545469760895, 0.019939027726650238, 0.04665417596697807, -0.053747937083244324, 0.042391370981931686, 0.051850829273462296, -0.02936829812824726, -0.007440395653247833, -0.02737310528755188, 0.021176019683480263, -0.010537252761423588, -0.007833718322217464, -0.049034483730793, -0.04614996165037155, -0.051951851695775986, -0.04387284070253372, -0.016697382554411888, ...],
        ...
      ]
    >
  },
  "normalization_14" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114179>
      [0.9895105957984924, 0.9951261878013611, 0.987804114818573, 0.991937518119812, 0.9897922873497009, 0.9860245585441589, 0.9890797734260559, 0.9889466166496277, 0.9912100434303284, 0.9859556555747986, 0.9986648559570312, 0.989870011806488, 0.9910022020339966, 0.9893552660942078, 0.9985496401786804, 0.9912651777267456, 0.9870154857635498, 0.9927847385406494, 0.9943287372589111, 0.992034912109375, 0.9874383211135864, 0.9868220686912537, 0.9859617948532104, 0.9875226616859436, 0.9895035028457642, 0.9918275475502014, 0.9892218708992004, 0.9910162091255188, 0.9903724789619446, 0.9919095635414124, 0.994697093963623, 0.995358943939209, 0.9941636323928833, 0.9934165477752686, 0.9914501905441284, 0.9917154908180237, 0.9913410544395447, 0.9961050748825073, 0.9937815070152283, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114180>
      [-0.006411913316696882, 0.008353912271559238, 0.006301285699009895, -0.005382295697927475, 0.008082562126219273, 0.0012216161703690886, -0.007291087415069342, -0.008289477787911892, 0.0046152654103934765, -2.059573889710009e-4, 0.007970133796334267, 0.006443582009524107, -0.0020460467785596848, -0.004645291715860367, -0.008172528818249702, -0.00659757386893034, 0.0024016506504267454, 0.007502575404942036, 0.005879932548850775, -0.007550918962806463, -0.0016177321085706353, 0.0021594881545752287, -0.004914923571050167, -5.318476469255984e-4, 0.0078306645154953, 0.007242012303322554, -0.006209390703588724, -4.295319376979023e-5, -0.0064061302691698074, -0.0041244919411838055, 0.007596480194479227, 0.0018276687478646636, 0.006885921582579613, 0.007994535379111767, -0.007275475654751062, 0.007344710640609264, -0.0024149210657924414, -0.006475423462688923, ...]
    >
  },
  "normalization_7" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114211>
      [0.9978510141372681, 0.9963744878768921, 0.9951121211051941, 0.98782879114151, 0.990736186504364, 0.9879863262176514, 0.9876083731651306, 0.9945626258850098, 0.9989997148513794, 0.9971787333488464, 0.9949689507484436, 0.9958639740943909, 0.9907263517379761, 0.9914954900741577, 0.9870316386222839, 0.9888283610343933, 0.9923381209373474, 0.9920769333839417, 0.9882910251617432, 0.985933244228363, 0.9977609515190125, 0.9879745244979858, 0.996837854385376, 0.991038978099823, 0.9931434392929077, 0.9936416149139404, 0.9977903366088867, 0.9928842782974243, 0.9988641738891602, 0.9996541738510132, 0.990410327911377, 0.9949147701263428, 0.9865098595619202, 0.9874670505523682, 0.9967883825302124, 0.9897012114524841, 0.9941404461860657, 0.9908013939857483, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114212>
      [0.0058579714968800545, 0.007161486428231001, 8.239615708589554e-4, -0.0034421018790453672, 0.005443705711513758, -0.004992550238966942, 0.001424287213012576, -0.005566567648202181, 0.007436720654368401, -0.0076685259118676186, 0.003472399665042758, 0.006506856996566057, 0.005758384708315134, -0.006644062697887421, -0.0017102351412177086, -0.0020233537070453167, -0.0028567647095769644, 5.012410692870617e-4, 0.002791208680719137, -0.002351687289774418, 1.9528870325302705e-5, 0.00152348552364856, -0.006391925271600485, -0.004303051624447107, 0.0017925144638866186, 0.005540045443922281, -0.002985506784170866, 0.007395158521831036, -0.007489422336220741, -0.0041105966083705425, -0.0020563877187669277, -0.00799135398119688, 0.004256862215697765, -1.0403744090581313e-4, -0.0077404300682246685, 0.0060260784812271595, -0.0019225494470447302, ...]
    >
  },
  "dropout_2" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114153>
      [4138204160, 1086378919]
    >
  },
  "dropout_14" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114147>
      [3275511741, 238124510]
    >
  },
  "normalization_11" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114173>
      [0.9872720837593079, 0.9857141375541687, 0.993701696395874, 0.9855291843414307, 0.9932955503463745, 0.9939925074577332, 0.9923722147941589, 0.988982081413269, 0.9963670969009399, 0.9943006634712219, 0.9972527027130127, 0.9874500632286072, 0.9934197664260864, 0.9908498525619507, 0.9923058152198792, 0.991939127445221, 0.9914976954460144, 0.9902931451797485, 0.9906554818153381, 0.9926722049713135, 0.9975202679634094, 0.9873144030570984, 0.9949529767036438, 0.9970031976699829, 0.9931469559669495, 0.995453953742981, 0.9896104335784912, 0.994956374168396, 0.9948275089263916, 0.9959645867347717, 1.00006103515625, 0.9863390922546387, 0.9972429275512695, 0.9964913129806519, 0.996272623538971, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114174>
      [-4.254179075360298e-4, 0.001495723961852491, -0.002467913320288062, -0.005047093611210585, 0.008167393505573273, -5.006302380934358e-4, -0.005051654763519764, 0.004568178206682205, 0.00467539532110095, -0.007079848553985357, 0.0069547719322144985, 0.0015600939514115453, 0.0072947596199810505, -0.004551583901047707, -0.006163274869322777, -0.004150217864662409, -0.007728674449026585, 0.0030283264350146055, 0.0059028263203799725, -0.0018163599306717515, -0.006588646210730076, -3.726080758497119e-4, -0.0071462467312812805, -0.00517443148419261, -2.019925886997953e-4, 0.007134954445064068, -0.0066541931591928005, -0.004022268112748861, -0.006059885025024414, -0.004446938168257475, -0.00699001457542181, -8.994621748570353e-5, 0.00822034478187561, -0.0014137126272544265, ...]
    >
  },
  "normalization_2" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114191>
      [0.9876854419708252, 0.9858695268630981, 0.9922207593917847, 0.9914820790290833, 0.9959192276000977, 0.9929298758506775, 0.998940110206604, 0.9922024607658386, 0.9880942106246948, 0.9972125887870789, 0.9933265447616577, 0.9946858286857605, 0.9906306266784668, 0.9919256567955017, 0.9915517568588257, 0.9963056445121765, 0.9954889416694641, 0.9908716082572937, 0.9914803504943848, 0.9913568496704102, 0.9927741885185242, 0.9874855875968933, 0.9912663698196411, 0.999798059463501, 0.996334969997406, 0.9943463802337646, 0.9885823130607605, 0.9857458472251892, 0.9901326298713684, 0.9934541583061218, 0.9955664277076721, 0.9899671077728271, 0.9883355498313904, 0.9857375621795654, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114192>
      [0.0017485880525782704, -0.0024626401718705893, 0.007019580341875553, -0.004664181731641293, 0.0066779498010873795, -0.005490278825163841, 0.0070443712174892426, -0.006863442715257406, 0.004968598019331694, -0.00566173205152154, -5.472335615195334e-4, 0.004286395851522684, 0.0017449789447709918, 0.006434889975935221, -0.0028530643321573734, -0.00842367671430111, 0.00298305950127542, -0.0021345270797610283, 0.0068370201624929905, 0.005618380382657051, -0.0030899138655513525, 0.001908793579787016, -0.003366330172866583, -0.0077864681370556355, -0.001912526204250753, -0.006896140053868294, -0.007533015217632055, -0.0010089011630043387, 0.007252465933561325, -0.004892957396805286, -0.007376272231340408, -0.0031754416413605213, -0.007406146731227636, ...]
    >
  },
  "dropout_9" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114165>
      [215728877, 2390835091]
    >
  },
  "dropout_11" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114144>
      [3755504796, 1243094734]
    >
  },
  "dense_12" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114100>
      [-5.481108673848212e-4, -0.003205173183232546, 0.003220196347683668, 0.0056262267753481865, -0.006476862821727991, 0.0033405059948563576, 0.0053078592754900455, -2.1063721214886755e-4, -0.003539247438311577, -0.005779229570180178, 0.003651294158771634, -0.001923301606439054, -6.317779188975692e-4, 0.0026882777456194162, 0.003860902041196823, 6.348963361233473e-4, 0.004808847792446613, -0.00664793374016881, 0.006323772482573986, -0.005554342642426491, -0.003698448883369565, 0.006763027049601078, 4.849576725973748e-5, -0.003334944834932685, 8.996466640383005e-4, 0.005321613047271967, -0.005529677029699087, 0.003851845860481262, 0.003756203455850482, -0.003953113220632076, 0.005360746290534735, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114101>
      [
        [-0.022834179922938347, -0.03128599375486374, -0.014835945330560207, -0.007844466716051102, -0.017443954944610596, 0.0017430016305297613, 0.00565112940967083, 0.00826134905219078, 0.014521138742566109, -4.4335298298392445e-5, -0.007696821354329586, -0.0020438097417354584, -0.009640421718358994, 0.029052672907710075, -0.03298598527908325, -0.016912635415792465, -0.01619226299226284, 0.02589060366153717, 0.0031061903573572636, -0.012431906536221504, 0.01062967348843813, -0.02298565022647381, 0.03329673409461975, 0.04153246060013771, 0.015870971605181694, 0.009887496009469032, -0.02836114540696144, -0.018772991374135017, 0.012899872846901417, -0.032143913209438324, ...],
        ...
      ]
    >
  },
  "dense_23" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114124>
      [0.006676121614873409, 0.006873594131320715, -0.0011293590068817139, 0.0053713382221758366, -0.00770655507221818, -0.005369705613702536, 0.005140611436218023, -0.0050865537486970425, -0.006028077099472284, 0.003985205665230751, 0.006108761299401522, 0.006024118047207594, 0.004892782308161259, -0.005051109939813614, -0.00498197553679347, 0.0064035155810415745, 0.007829885929822922, 0.004984886851161718, 0.005114784464240074, -0.006359969265758991, -0.005746056325733662, -0.0066193630918860435, 0.008710108697414398, -0.0072562615387141705, -0.003817161777988076, 0.005847335793077946, -0.007673379499465227, -0.005287677515298128, 0.004533280618488789, -0.006281247362494469, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114125>
      [
        [-0.007748258300125599, 0.0014108530012890697, -0.0325222983956337, -0.006283629219979048, 0.023626074194908142, -0.03479958325624466, 0.02807343378663063, 0.03228265047073364, -0.03710079938173294, -0.025166356936097145, -0.018635988235473633, -0.019984448328614235, -0.016515396535396576, 0.03646844998002052, 0.01914146915078163, -0.015107868239283562, 0.0019050260307267308, -0.011288939043879509, -0.00569135881960392, 0.004569701384752989, -0.029122840613126755, 0.020475097000598907, 0.04020223021507263, 0.016351105645298958, 0.0247932318598032, 0.03469675034284592, -0.005535067990422249, -0.026686951518058777, 0.038394421339035034, ...],
        ...
      ]
    >
  },
  "embedding_0" => %{
    "kernel" => #Nx.Tensor<
      f32[50257][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114166>
      [
        [0.009494164027273655, 0.010073593817651272, 0.013317156583070755, -7.532805320806801e-4, -0.003645149990916252, -1.0347737770644017e-5, 2.7159173623658717e-4, -0.002601267769932747, 0.0019702701829373837, -0.002498557325452566, 1.2023415365547407e-5, 0.002113970695063472, 0.0013685371959581971, -0.006833044812083244, -0.009508678689599037, 0.0016026169760152698, 0.003115367842838168, 3.8442196091637015e-4, 0.010007340461015701, 0.0012504226760938764, -0.007268968503922224, -0.016176307573914528, -9.084145131055266e-5, 0.001978579442948103, 0.014792912639677525, 0.0034395630937069654, -0.006829241290688515, -0.004887895192950964, 0.010563059709966183, ...],
        ...
      ]
    >
  },
  "dropout_8" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114164>
      [4044366447, 1783482288]
    >
  },
  "normalization_19" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114189>
      [0.9892094135284424, 0.9981752038002014, 0.9919894933700562, 0.991588294506073, 0.9935871362686157, 0.9921343326568604, 0.9880547523498535, 0.9902307391166687, 0.9931799173355103, 0.993546187877655, 0.994713544845581, 0.9893707633018494, 0.9887422323226929, 0.995705783367157, 0.9859334230422974, 0.9935662150382996, 0.9941276907920837, 0.9860926866531372, 0.9930781126022339, 0.9905633926391602, 0.984050452709198, 0.9903981685638428, 0.9845932126045227, 0.9948567152023315, 0.9902247786521912, 0.9918249845504761, 0.9930837154388428, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114190>
      [0.0022743844892829657, 0.008888047188520432, -0.0024855148512870073, -0.0014650929952040315, -0.006635929923504591, -0.00854974053800106, -0.007055416237562895, -0.005035370588302612, 9.480009903199971e-4, -0.0010770317167043686, 0.0019172937609255314, 0.0069079468958079815, 0.0030590330716222525, -0.007245266810059547, 0.008727413602173328, 0.005931230261921883, 0.0027264482341706753, -0.003218946047127247, 0.006993592716753483, -0.0020236356649547815, -0.003796375822275877, 0.006078892387449741, -0.0016019247705116868, -0.006135317496955395, -0.0021469087805598974, 0.006050610914826393, ...]
    >
  },
  "dropout_6" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114162>
      [3535305574, 3707068374]
    >
  },
  "dropout_23" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114157>
      [3547647931, 3332761855]
    >
  },
  "causal_attention_6" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114076>
      [
        [0.05201854929327965, 0.007639641407877207, 0.001918863970786333, 0.02610529214143753, 0.05974140763282776, -0.0224589966237545, -0.05618627741932869, 0.016936050727963448, -0.037548407912254333, -0.004779201000928879, 0.04837620258331299, 0.05230828374624252, -0.04102528840303421, -0.03500676900148392, 0.022493813186883926, -0.025771135464310646, -0.01790691167116165, -0.052261579781770706, 0.026323514059185982, -0.0340738482773304, -0.03404178470373154, 0.04696810990571976, -0.03247823938727379, -0.002565284725278616, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114077>
      [
        [0.045810431241989136, 0.04856148362159729, 0.025317469611763954, -0.00934944674372673, 0.03516619652509689, -0.008801733143627644, 0.023740693926811218, 0.04317152872681618, 0.023644020780920982, -0.056216172873973846, 0.0476059764623642, 0.00977197103202343, -0.024712994694709778, -0.018577245995402336, 0.007447012700140476, 0.03732999786734581, 0.0032627880573272705, 0.01566113717854023, 0.018493318930268288, 0.006423033773899078, 0.01629333384335041, 0.06435684859752655, -0.04164769500494003, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114078>
      [
        [0.003171802731230855, -0.004716179799288511, 0.0020887397695332766, 0.04738689213991165, 0.0346389003098011, 0.012424178421497345, 0.024105466902256012, -0.01069110818207264, 0.03723420575261116, -0.050352711230516434, -0.03727545216679573, -0.05198333412408829, -0.057729464024305344, 0.020631035789847374, 0.045394595712423325, -0.02836604230105877, -0.008011541329324245, -0.005882304161787033, 0.012971683405339718, -0.04991566762328148, -0.04015810415148735, 0.060002367943525314, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114079>
      [
        [-0.053156621754169464, -0.009843900799751282, 0.044821448624134064, -0.03593356907367706, 0.016871057450771332, 0.03654185310006142, -4.2135940748266876e-4, -0.005440086126327515, -0.031370244920253754, -0.012674388475716114, 0.037255384027957916, 0.005761174019426107, 0.024357810616493225, 0.012096485123038292, -0.007000296376645565, 0.042299967259168625, 0.030046777799725533, 0.016699492931365967, 0.032196059823036194, -0.025760574266314507, -0.03803791105747223, ...],
        ...
      ]
    >
  },
  "causal_attention_7" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114080>
      [
        [0.05754372105002403, -0.06281263381242752, -0.003946900833398104, -0.013844024389982224, -0.0026899732183665037, 0.0576406829059124, 0.009927162900567055, 0.038426078855991364, -0.050576623529195786, -0.027333801612257957, 0.005613395012915134, -0.05476292967796326, -0.04448458179831505, 0.03658847510814667, -0.046287842094898224, -0.04604428634047508, -0.04961133748292923, -0.035712338984012604, 0.053502388298511505, 0.029070934280753136, 0.040696922689676285, 0.003968953620642424, -0.0374416708946228, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114081>
      [
        [-0.054763197898864746, 0.011212320998311043, 0.00605158181861043, -0.03324338421225548, 0.03038695827126503, -0.04484441503882408, 0.05071544647216797, 0.06005608290433884, 0.05306660011410713, 0.020685916766524315, -7.621864788234234e-4, -0.01726130023598671, -0.021711107343435287, 0.034027643501758575, -0.0173463337123394, 0.02337416633963585, 0.060069575905799866, -0.0028890499379485846, 0.006563984788954258, -0.0027979149017482996, -0.03260650485754013, 0.027355730533599854, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114082>
      [
        [0.01149813923984766, 0.012846645899116993, -0.025731872767210007, 0.03799111768603325, 0.016934506595134735, -0.046428412199020386, 0.020029976963996887, 0.03539085015654564, -0.04177458956837654, 0.0033302262891083956, 0.03366623818874359, -0.04388105869293213, 0.009871307760477066, 0.03852831944823265, -0.027000706642866135, 0.015419909730553627, 0.02642970345914364, -0.02016179822385311, 0.014564922079443932, 0.026546748355031013, -0.0049494607374072075, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114083>
      [
        [0.05568006634712219, 0.007825992070138454, 0.054333992302417755, 0.02147119678556919, 0.052198637276887894, -0.012153330259025097, -0.049419280141592026, -0.03281094878911972, 0.031149841845035553, -0.005914677400141954, -0.048122137784957886, -0.03175870329141617, -0.03505455702543259, 0.03269446641206741, 0.02650935761630535, 0.022211866453289986, -0.03216797485947609, -0.048365525901317596, 0.06040085107088089, 0.046995993703603745, ...],
        ...
      ]
    >
  },
  "causal_attention_1" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114048>
      [
        [-0.04177520424127579, 0.00677125109359622, 0.04131779074668884, -0.00652224849909544, -0.06668800860643387, -0.04636218026280403, 0.0392020046710968, 0.0350070521235466, -0.04321125149726868, -0.013992300257086754, -1.875662273960188e-4, -0.022520489990711212, -0.0011116508394479752, -0.01965291053056717, 0.057671092450618744, 0.044868629425764084, 0.04674891382455826, 0.03421959653496742, 0.02355903759598732, -0.05913742631673813, 0.024764180183410645, -0.02729620225727558, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114049>
      [
        [0.062153659760951996, -0.017099151387810707, -0.006810035556554794, 0.027456985786557198, 0.024162279441952705, 0.006832705345004797, -0.002787965815514326, -0.034815434366464615, 0.0022859612945467234, -0.006061233580112457, -0.0033820958342403173, 0.004459593445062637, 0.010868451558053493, -0.056092582643032074, 0.025769924744963646, -0.04571285843849182, 0.05520210415124893, -0.0550413578748703, 0.02903885394334793, 0.05619287118315697, -0.06035219132900238, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114050>
      [
        [-0.049414560198783875, -0.031649693846702576, 0.06044063717126846, -0.013123956508934498, 0.02660640887916088, -0.04341628775000572, -0.030609343200922012, 0.0482654795050621, 0.05379427969455719, -0.019982624799013138, -0.028457388281822205, -0.04271883890032768, -0.03275767341256142, -0.031624261289834976, -0.017235852777957916, -0.05982067063450813, 0.051490895450115204, -0.05034083127975464, 0.02963254600763321, -0.004219111520797014, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114051>
      [
        [-0.004282130394130945, 0.02828473411500454, 0.0323159284889698, 0.015089399181306362, -0.032932426780462265, 0.017894329503178596, -0.04147160053253174, -0.029433978721499443, 0.025875521823763847, -0.06179290637373924, 0.004882007371634245, -0.05114882066845894, 0.016784412786364555, -0.0261076707392931, 0.019023887813091278, 0.00475635239854455, -0.05871642380952835, -0.01632285676896572, -0.04092831164598465, ...],
        ...
      ]
    >
  },
  "dropout_1" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114142>
      [1312895730, 1070237425]
    >
  },
  "dense_15" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114106>
      [0.006768638268113136, 0.007525276858359575, -0.002546816598623991, 0.00118057441432029, -0.006852889899164438, -0.0019413352711126208, 0.003905491204932332, -0.0050737555138766766, -0.003418432781472802, 4.0114051080308855e-4, 0.003862723708152771, 0.006273355334997177, 0.005983258131891489, -0.006675555370748043, -0.004817377310246229, 0.007016433402895927, 0.007647292222827673, 0.0013119190698489547, 0.0070887841284275055, -9.938195580616593e-4, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114107>
      [
        [0.03739933669567108, 0.013625654391944408, 0.003256549360230565, 0.026651859283447266, 0.011669391766190529, 0.022969264537096024, 0.02057899720966816, -4.5079123083269224e-5, 0.03619123995304108, -0.030206559225916862, -0.012528561055660248, -0.003801837796345353, -0.004792361985892057, -0.02546864002943039, 0.017911715433001518, -0.0020239627920091152, -0.003272494301199913, 0.03039216436445713, -0.017529059201478958, ...],
        ...
      ]
    >
  },
  "dense_9" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114139>
      [0.006323033012449741, 0.0051317838951945305, -0.006572882179170847, -2.8759133419953287e-4, -0.0037870467640459538, -0.0038813301362097263, 0.004109823144972324, 0.0027336133643984795, 0.002234049141407013, 0.0036143967881798744, 0.006036001723259687, 0.006102969869971275, 0.006385290063917637, -0.005775886587798595, -0.0070905364118516445, -3.098807064816356e-5, -4.207091697026044e-4, -4.6570744598284364e-4, 0.006824368145316839, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114140>
      [
        [-0.01577042229473591, 0.025083012878894806, -0.03557153418660164, 0.030282536521553993, -0.034589461982250214, -0.03411942347884178, 0.009886984713375568, 0.04106277599930763, -0.025066815316677094, 0.006701804231852293, 9.672158630564809e-4, 0.009180895052850246, -0.02505435235798359, -0.036326900124549866, -0.026174912229180336, -0.021007291972637177, 0.020035939291119576, -0.04118809103965759, ...],
        ...
      ]
    >
  },
  "normalization_8" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114213>
      [0.9874618053436279, 0.9941616058349609, 0.9907931089401245, 0.9914515614509583, 0.9961497187614441, 0.9904305934906006, 0.9871031641960144, 0.9954640865325928, 0.992019534111023, 0.9878630042076111, 0.9901222586631775, 0.9953787326812744, 0.9963057637214661, 0.9879868030548096, 0.99543696641922, 0.992197573184967, 0.9873921871185303, 0.9876422882080078, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114214>
      [0.005145433358848095, 0.001737198093906045, 0.006454120855778456, -0.006345255766063929, -0.007054379675537348, -0.005534826312214136, 0.0026368096005171537, 0.003512331983074546, 0.008773617446422577, 5.861393292434514e-4, 0.0016314465319737792, 0.005814856383949518, -0.00724633177742362, -0.007220170460641384, -0.006308204960078001, -0.005008082836866379, 1.6000538016669452e-4, ...]
    >
  },
  "dropout_10" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114143>
      [2319554396, 2360065558]
    >
  },
  "dropout_3" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114159>
      [2199627258, 1461722796]
    >
  },
  "dropout_15" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114148>
      [4246356717, 3984337250]
    >
  },
  "causal_attention_10" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114052>
      [
        [-0.057459838688373566, -0.0625145435333252, -0.03161993250250816, 0.06423499435186386, -0.00623325863853097, 0.050513219088315964, -0.004609670955687761, -0.011763893067836761, -0.06044310703873634, -0.03192911669611931, 0.04209449142217636, -0.01958739012479782, -0.028782201930880547, -0.04377777874469757, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114053>
      [
        [-0.009740732610225677, 0.046771638095378876, 0.025989290326833725, -0.0025486648082733154, 0.01610955409705639, -0.011282467283308506, 0.015063329599797726, -0.051637616008520126, -0.04217517375946045, 0.044933903962373734, -0.05181923508644104, -0.017532970756292343, -0.05574548989534378, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114054>
      [
        [0.0071498011238873005, 0.011618571355938911, -0.03702467679977417, -0.027438174933195114, -0.055234719067811966, -0.016760725528001785, -0.051218219101428986, -0.04425530880689621, 0.053360290825366974, -0.04485664144158363, -0.02630726620554924, -0.047063279896974564, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114055>
      [
        [-0.019842788577079773, 0.0042077829129993916, -0.030674321576952934, -0.00896004494279623, 0.019024956971406937, 0.033586036413908005, -4.6494798152707517e-4, 0.058231912553310394, -0.05094011873006821, -0.045179471373558044, -0.045771513134241104, ...],
        ...
      ]
    >
  },
  "causal_attention_8" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114084>
      [
        [-0.047874052077531815, 0.018838981166481972, -0.020496081560850143, 0.022521601989865303, -0.06293800473213196, -0.005627582315355539, -0.028388088569045067, -0.0072103156708180904, -0.03300333023071289, 0.050960179418325424, 0.04587005078792572, -0.051892369985580444, 1.7121239216066897e-4, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114085>
      [
        [0.0605132170021534, -0.006930990610271692, 0.05265515297651291, -0.027492966502904892, -0.04685811325907707, 0.029100678861141205, 0.010877231135964394, 0.06066995486617088, -0.060770172625780106, 0.045265115797519684, -0.055889032781124115, 0.04533989727497101, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114086>
      [
        [0.0516621358692646, -0.020688403397798538, 0.060346320271492004, 0.016154656186699867, -0.033243242651224136, 0.0216155257076025, 0.05263114720582962, 0.039817843586206436, -0.04033585637807846, 0.01846150867640972, 0.028957437723875046, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114087>
      [
        [0.016887452453374863, 0.027797263115644455, 0.026207882910966873, -0.06338710337877274, -0.037728626281023026, -0.052232563495635986, -0.011221223510801792, -0.011119253002107143, 0.014267347753047943, -0.02141905203461647, ...],
        ...
      ]
    >
  },
  "dropout_7" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114163>
      [2261703073, 2027257910]
    >
  },
  "normalization_12" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114175>
      [0.9941497445106506, 0.9898622632026672, 0.9888628125190735, 0.9851372241973877, 0.9935579895973206, 0.993285596370697, 0.9894921183586121, 0.9859603047370911, 0.9912490844726562, 0.9875874519348145, 0.9959116578102112, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114176>
      [0.008379081264138222, 0.00351280951872468, -9.001491707749665e-4, -0.0014489145250990987, -0.007633660454303026, -0.004478113260120153, -0.0020479883532971144, -0.008293896913528442, -0.005013898480683565, 0.004447298124432564, ...]
    >
  },
  "dense_3" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114127>
      [0.0069266012869775295, 0.007255371194332838, -0.0030089898500591516, -0.006431032903492451, -0.002542272675782442, -0.0073097217828035355, 0.005969948600977659, 0.0018675338942557573, 0.006843041628599167, 0.003237202763557434, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114128>
      [
        [-0.017431093379855156, -0.03332851827144623, 0.017503924667835236, 0.030544131994247437, -0.006316334940493107, 0.011410287581384182, -0.0010987174464389682, -0.011579304002225399, 0.01230853796005249, ...],
        ...
      ]
    >
  },
  "dense_18" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114112>
      [0.005854689050465822, 0.008157150819897652, 0.004147968254983425, -0.006927981041371822, -3.044994664378464e-4, 0.0037127863615751266, 0.007657335605472326, -0.005113402847200632, -0.005100630223751068, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114113>
      [
        [0.02079125866293907, 0.012818829156458378, 0.034023579210042953, 0.020001886412501335, 0.03035629168152809, 0.011670386418700218, -0.017286932095885277, -0.03317178413271904, ...],
        ...
      ]
    >
  },
  "dense_5" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114131>
      [0.0071365744806826115, 0.007971416227519512, -0.005636551417410374, -0.0013674997026100755, 0.002500341273844242, -0.0033208602108061314, 0.0016006078803911805, 8.46053590066731e-4, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114132>
      [
        [-0.0323602594435215, 0.03139256685972214, -0.03159160166978836, -0.004217840265482664, 0.005689178127795458, 0.001631042337976396, -0.03211473301053047, ...],
        ...
      ]
    >
  },
  "dense_11" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114098>
      [0.007672191597521305, 0.007925293408334255, -0.0032689375802874565, -9.455705876462162e-4, -0.007227649446576834, -0.0017621939769014716, -5.291064735502005e-4, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114099>
      [
        [0.018696075305342674, -0.022696781903505325, -0.0036400938406586647, 0.0030045663006603718, -0.016498781740665436, 0.020350517705082893, ...],
        ...
      ]
    >
  },
  "dropout_21" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114155>
      [2819711488, 3001896492]
    >
  },
  "normalization_18" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114187>
      [0.9953553080558777, 0.9956200122833252, 0.9880553483963013, 0.9915353059768677, 0.990915060043335, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114188>
      [0.006294887978583574, 0.007707604672759771, -1.1373086454113945e-4, 0.007003240752965212, ...]
    >
  },
  "dense_6" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114133>
      [9.501499007456005e-4, 0.005432691425085068, 0.0011090110056102276, -0.00574079155921936, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114134>
      [
        [-0.007568587549030781, 0.027182798832654953, 0.03266013041138649, ...],
        ...
      ]
    >
  },
  "normalization_15" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114181>
      [0.9922001361846924, 0.9973219037055969, 0.9938342571258545, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114182>
      [0.005354762077331543, 0.005567891988903284, ...]
    >
  },
  "normalization_23" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114199>
      [0.9887099266052246, 0.9918047189712524, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114200>
      [0.0013926825486123562, ...]
    >
  },
  "dropout_24" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114158>
      [770271330, ...]
    >
  },
  "dense_10" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.4241110313.4078043160.114096>
      [...]
    >,
    ...
  },
  "dense_13" => %{...},
  ...
}
```

```elixir
{:ok, input} = MyGPT.text_to_token_ids(tokenizer, "I")
token_ids = MyGPT.generate_tokens(predict_fn, trained_model_state, input, 6)
{:ok, text} = MyGPT.token_ids_to_text(tokenizer, token_ids)
```

<!-- livebook:{"output":true} -->

```
{:ok, "I,,,,,,"}
```

At the beginning of the training, both the training and validation
set losses sharply decrease, which is a sign that the model is learning. However,
the training set loss continues to decrease past the second epoch, whereas the
validation loss stagnates. This is a sign that the model is still learning, but its
overfitting to the training set past epoch 2.

## 5.3 Decoding strategies to control randomness
