# Chapter 3: Coding Attention Mechanisms

```elixir
Mix.install([
  {:nx, "~> 0.5"},
  {:exla, "~> 0.5"},
  {:axon, "~> 0.5"},
  {:tiktoken, "~> 0.3.2"},
  {:table_rex, "~> 3.1.1"},
])
```

## Introduction

There are four different variants of attention mechanisms:

* **Simplified self-attention** is just to introduce the broader idea.

* **Self-attention**, the basis of the mechanism used in LLMs, with trainable weights.

* **Causal attention**, a type of self attention used in LLMs that allows a model to consider only previous and current inputs in a sequence, ensuring temporal order during the text generation.

* **Multi-head attention**, is an extension of self-attention and causal attention that enables the model to simultaneously attend to information from different representation subspaces.

## 3.1 The problem with modeling long sequences

Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder-decoder architecture for language translation.

An RNN is a type of neural network where outputs from previous steps are fed as inputs
to the current step, making them well-suited for sequential data like text.

In an encoder-decoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden state, at each step, trying to capture the entire meaning of the input sentence in the final hidden state.

The decoder then takes this final hidden state to start
generating the translated sentence, one word at a time. It also updates its hidden state at
each step, which is supposed to carry the context necessary for the next-word prediction.

The big issue and limitation of encoder-decoder RNNs is that the RNN can't directly
access earlier hidden states from the encoder during the decoding phase. Consequently, it
relies solely on the current hidden state, which encapsulates all relevant information. This
can lead to a loss of context, especially in complex sentences where dependencies might
span long distances.

## 3.2 Capturing data dependencies with attention mechanisms

RNNs work fine for translating short
sentences but don't work well for longer texts as they don't have direct access to previous
words in the input.

The Bahdanau attention mechanism for RNNs modifies the encoder-
decoder RNN such that the decoder can selectively access different parts of the input
sequence at each decoding step. The importance is determined by the so-called attention weights

researchers found that RNN architectures are not
required for building deep neural networks for natural language processing and proposed
the original transformer architecture with a self-attention
mechanism inspired by the Bahdanau attention mechanism.

Self-attention is a mechanism that allows each position in the input sequence to attend
to all positions in the same sequence when computing the representation of a sequence.

Self-attention is a mechanism in transformers that is used to compute more efficient input
representations by allowing each position in a sequence to interact with and weigh the importance of all other
positions within the same sequence.

## 3.3 Attending to different parts of the input with self-attention

Self-attention serves as the cornerstone of every LLM based on the transformer architecture.

In self-attention, the "self" refers to the mechanism's ability to compute attention
weights by relating different positions within a single input sequence. It assesses and
learns the relationships and dependencies between various parts of the input itself,
such as words in a sentence or pixels in an image. This is in contrast to traditional
attention mechanisms, where the focus is on the relationships between elements of
two different sequences, such as in sequence-to-sequence models where the
attention might be between an input sequence and an output sequence.

**3.3.1 A simple self-attention mechanism without trainable weights**

The goal of self-attention is to compute a context vector, for each input element, that combines
information from all other input elements.

In self-attention, our goal is to calculate context vectors z^(i) for each element x^(i) in the
input sequence. A context vector can be interpreted as an enriched embedding vector.

In self-attention, context vectors play a crucial role. Their purpose is to create enriched
representations of each element in an input sequence (like a sentence). This is
essential in LLMs, which need to understand the relationship and relevance of words in a
sentence to each other.

```elixir
inputs =
  Nx.tensor(
    [
      # Your (x^1)
      [0.43, 0.15, 0.89],
      # journey (x^2)
      [0.55, 0.87, 0.66],
      # starts (x^3)
      [0.57, 0.85, 0.64],
      # with (x^4)
      [0.22, 0.58, 0.33],
      # one (x^5)
      [0.77, 0.25, 0.10],
      # step (x^6)
      [0.05, 0.80, 0.55]
    ]
  )
```

The first step of implementing self-attention is to compute the intermediate values Ï‰, referred to as attention.

A dot product is essentially just a concise way of multiplying two vectors element-wise and then summing the products.

The dot product is a measure of similarity because it
quantifies how much two vectors are aligned: a higher dot product indicates a
greater degree of alignment or similarity between the vectors. In the context of self-
attention mechanisms, the dot product determines the extent to which elements in a
sequence attend to each other: the higher the dot product, the higher the similarity
and attention score between two elements.

```elixir
Nx.dot(inputs, inputs[1])
```

```elixir
{sequence_size, _dim} = inputs.shape

attn_scores =
  for i <- 0..(sequence_size - 1), reduce: [] do
    acc ->
      score = Nx.dot(inputs, inputs[i])
      acc ++ [score]
  end
```

```elixir
attn_scores_tensor = Nx.stack(attn_scores)
```

```elixir
# for-loops are generally slow, and we can achieve the 
# same results using matrix multiplication
Nx.dot(inputs, Nx.transpose(inputs))
```

The next step is normalization.

This normalization is a convention that is useful for interpretation and for
maintaining training stability in an LLM.

```elixir
attn_sum = Nx.sum(attn_scores_tensor, axes: [1])
attn_score_norm_tensor = Nx.divide(attn_scores_tensor, attn_sum) |> Nx.transpose()
```

```elixir
attn_sum = Nx.sum(attn_score_norm_tensor, axes: [1])
```

```elixir
# In practice, it's more common and advisable to use the softmax 
# function for normalization.
softmax_naive = fn x ->
  exp_x = Nx.exp(x)
  exp_x
  |> Nx.divide(Nx.sum(exp_x, axes: [1]))
  |> Nx.transpose()
end

# the softmax function ensures that the attention weights are always positive.
# This makes the output interpretable as probabilities or relative importance, 
# where higher weights indicate greater importance.
```

```elixir
attn_score_softmax_tensor = softmax_naive.(attn_scores_tensor)
```

```elixir
# To avoid numerical instability problems, such as overflow and underflow, 
# when dealing with large or small input values.
Axon.Activations.softmax(attn_scores_tensor)
```

The final state is calculating the context vector z^(2) by multiplying the embedded
input tokens, x^(i) , with the corresponding attention weights and then summing the resulting
vectors

**3.3.2 Computing attention weights for all input tokens**

```elixir
context_vector = Nx.dot(attn_score_softmax_tensor, inputs)
```

Self-Attention Steps:

* Compute the attention scores as dot products between the inputs.
* The attention weights are a normalized version of the attention scores.
* The context vectors are computed as a weighted sum over the inputs.

## 3.4 Implementing self-attention with trainable weights

Self-attention mechanism is also called scaled dot-product attention.

These trainable weight matrices are crucial so that the model
(specifically, the attention module inside the model) can learn to produce "good" context vectors.

<!-- livebook:{"break_markdown":true} -->

**3.4.1 Computing the attention weights step by step**

We are going to use three weight matrices W_q, W_k, and W_v. These three matrices are used to project the embedded input tokens, x^(i), into query, key, and value vectors.

```elixir
x = Axon.param("random", {1,2})
```

```elixir
x_2 = inputs[1]
w_q = Nx.tensor([[0.2961, 0.5166],[0.2517, 0.6886],[0.0740, 0.8665]])
w_k = Nx.tensor([[0.1366, 0.1025],[0.1841, 0.7264],[0.3153, 0.6871]])
w_v = Nx.tensor([[0.0756, 0.1966],[0.3164, 0.4017],[0.1186, 0.8274]])

{w_q, w_k, w_v, x_2}
```

```elixir
query_2 = Nx.dot(x_2, w_q)
key_2 = Nx.dot(x_2, w_k)
value_2 = Nx.dot(x_2, w_v)
```

In summary, weight parameters are the fundamental, learned coefficients that define the network's connections, while attention weights are dynamic, context-specific values.

```elixir
queries = Nx.dot(inputs, w_q)
keys = Nx.dot(inputs, w_k)
values = Nx.dot(inputs, w_v)
```

```elixir
attn_score_22 = Nx.dot(keys[1], queries[1])
```

```elixir
# All attention scores for given query
attn_score_2 =
  keys
  |> Nx.transpose()
  |> then(&Nx.dot(queries[1], &1))
```

That we now scale the attention scores by dividing them by the square root of the embedding dimension of the keys.

```elixir
d_k = Nx.axis_size(keys, -1)
attn_weights_2 =
  attn_score_2
  |> Nx.divide(Nx.pow(d_k, 0.5))
  |> Axon.Activations.softmax(axis: -1)
```

The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. The final step is to compute the context vectors.

```elixir
context_vec_2 = Nx.dot(attn_weights_2, values)
```

**Why query, key, and value?**

The terms "key," "query," and "value" in the context of attention mechanisms are borrowed from the domain of information retrieval and databases, where similar concepts are used to store, search, and retrieve information.

A "query" is analogous to a search query in a database. The query is used to probe the other parts of the input sequence to determine how much attention to pay to them.

The "key" is like a database key used for indexing and searching. In the attention mechanism, each item in the input sequence has an associated key. These keys are used to match with the query.

The "value" in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values.
