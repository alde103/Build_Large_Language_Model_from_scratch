# Chapter 3: Coding Attention Mechanisms

```elixir
Mix.install([
  {:nx, "~> 0.5"},
  {:exla, "~> 0.5"},
  {:axon, "~> 0.5"},
  {:tiktoken, "~> 0.3.2"},
  {:table_rex, "~> 3.1.1"},
])
```

## Introduction

There are four different variants of attention mechanisms:

* **Simplified self-attention** is just to introduce the broader idea.

* **Self-attention**, the basis of the mechanism used in LLMs, with trainable weights.

* **Causal attention**, a type of self attention used in LLMs that allows a model to consider only previous and current inputs in a sequence, ensuring temporal order during the text generation.

* **Multi-head attention**, is an extension of self-attention and causal attention that enables the model to simultaneously attend to information from different representation subspaces.

## 3.1 The problem with modeling long sequences

Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder-decoder architecture for language translation.

An RNN is a type of neural network where outputs from previous steps are fed as inputs
to the current step, making them well-suited for sequential data like text.

In an encoder-decoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden state, at each step, trying to capture the entire meaning of the input sentence in the final hidden state.

The decoder then takes this final hidden state to start
generating the translated sentence, one word at a time. It also updates its hidden state at
each step, which is supposed to carry the context necessary for the next-word prediction.

The big issue and limitation of encoder-decoder RNNs is that the RNN can't directly
access earlier hidden states from the encoder during the decoding phase. Consequently, it
relies solely on the current hidden state, which encapsulates all relevant information. This
can lead to a loss of context, especially in complex sentences where dependencies might
span long distances.

## 3.2 Capturing data dependencies with attention mechanisms

RNNs work fine for translating short
sentences but don't work well for longer texts as they don't have direct access to previous
words in the input.

The Bahdanau attention mechanism for RNNs modifies the encoder-
decoder RNN such that the decoder can selectively access different parts of the input
sequence at each decoding step. The importance is determined by the so-called attention weights

researchers found that RNN architectures are not
required for building deep neural networks for natural language processing and proposed
the original transformer architecture with a self-attention
mechanism inspired by the Bahdanau attention mechanism.

Self-attention is a mechanism that allows each position in the input sequence to attend
to all positions in the same sequence when computing the representation of a sequence.

Self-attention is a mechanism in transformers that is used to compute more efficient input
representations by allowing each position in a sequence to interact with and weigh the importance of all other
positions within the same sequence.

```elixir

```
