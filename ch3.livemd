# Chapter 3: Coding Attention Mechanisms

```elixir
Mix.install([
  {:nx, "~> 0.5"},
  {:exla, "~> 0.5"},
  {:axon, "~> 0.5"},
  {:tiktoken, "~> 0.3.2"},
  {:table_rex, "~> 3.1.1"},
])
```

## Introduction

There are four different variants of attention mechanisms:

* **Simplified self-attention** is just to introduce the broader idea.

* **Self-attention**, the basis of the mechanism used in LLMs, with trainable weights.

* **Causal attention**, a type of self attention used in LLMs that allows a model to consider only previous and current inputs in a sequence, ensuring temporal order during the text generation.

* **Multi-head attention**, is an extension of self-attention and causal attention that enables the model to simultaneously attend to information from different representation subspaces.

## 3.1 The problem with modeling long sequences

Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder-decoder architecture for language translation.

An RNN is a type of neural network where outputs from previous steps are fed as inputs
to the current step, making them well-suited for sequential data like text.

In an encoder-decoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden state, at each step, trying to capture the entire meaning of the input sentence in the final hidden state.

The decoder then takes this final hidden state to start
generating the translated sentence, one word at a time. It also updates its hidden state at
each step, which is supposed to carry the context necessary for the next-word prediction.

The big issue and limitation of encoder-decoder RNNs is that the RNN can't directly
access earlier hidden states from the encoder during the decoding phase. Consequently, it
relies solely on the current hidden state, which encapsulates all relevant information. This
can lead to a loss of context, especially in complex sentences where dependencies might
span long distances.

```elixir

```
