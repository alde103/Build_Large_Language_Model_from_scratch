<!-- livebook:{"persist_outputs":true} -->

# Chapter 4: Implementing a GPT model from scratch to generate text

```elixir
Mix.install([
  {:nx, "~> 0.5"},
  {:exla, "~> 0.5"},
  {:axon, "~> 0.5"},
  {:tiktoken, "~> 0.3.2"},
  {:table_rex, "~> 3.1.1"},
  {:kino_vega_lite, "~> 0.1.11"}
])
```

## 4.1 Coding an LLM architecture

LLMs, such as GPT (which stands for generative pretrained transformer), are large deep neural network architectures designed to generate new text one word (or token) at a time.

In the context of deep learning and LLMs like GPT, the term “parameters” refers
to the trainable weights of the model. These weights are essentially the internal variables of the model that are adjusted and optimized during the training process to
minimize a specific loss function. This optimization allows the model to learn from
the training data.

```elixir
gpt_config_124m = [
  attn_name: "attention_0",
  vocab_size: 50257,
  context_length: 1024,
  emb_dim: 768,
  n_heads: 12,
  n_layers: 12,
  drop_rate: 0.1,
  qkv_bias: false
]
```

<!-- livebook:{"output":true} -->

```
[
  attn_name: "attention_0",
  vocab_size: 50257,
  context_length: 1024,
  emb_dim: 768,
  n_heads: 12,
  n_layers: 12,
  drop_rate: 0.1,
  qkv_bias: false
]
```

```elixir
defmodule DummyGPTModel do
  def model(config \\ []) do
    Axon.input("sequence")
    |> Axon.embedding(config[:vocab_size], config[:emb_dim])
    |> Axon.embedding(config[:context_length], config[:emb_dim])
    |> Axon.dropout(rate: config[:drop_rate])
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, DummyGPTModel, <<70, 79, 82, 49, 0, 0, 8, ...>>, {:model, 1}}
```

```elixir
txt1 = "Every effort moves you"
txt2 = "Every day holds a"

# gpt2 not supported
{:ok, ids1} = Tiktoken.encode("gpt-3.5-turbo", txt1, [])
{:ok, ids2} = Tiktoken.encode("gpt-3.5-turbo", txt2, [])

tensors = Enum.map([ids1, ids2], &Nx.tensor/1)
Nx.stack(tensors)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[2][4]
  [
    [11769, 5149, 11031, 499],
    [11769, 1938, 10187, 264]
  ]
>
```

```elixir
batch = Nx.tensor([[6109, 3629, 6100, 345], [6109, 1110, 6622, 257]])
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[2][4]
  [
    [6109, 3629, 6100, 345],
    [6109, 1110, 6622, 257]
  ]
>
```

The model outputs, which are commonly referred to as logits.

## 4.2 Normalizing activations with layer normalization

Training deep neural networks with many layers can sometimes prove challenging
due to problems like vanishing or exploding gradients. These problems lead to unstable training dynamics and make it difficult for the network to effectively adjust its
weights, which means the learning process struggles to find a set of parameters
(weights) for the neural network that minimizes the loss function.

```elixir
key = Nx.Random.key(123)
Nx.Random.normal(key, shape: {2, 5}) |> IO.inspect()

batch_example =
  Nx.tensor([
    [-0.1115, 0.1204, -0.3696, -0.2404, -1.1969],
    [0.2093, -0.9724, -0.7550, 0.3239, -0.1085]
  ])
```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   f32[2][5]
   [
     [-0.5154414772987366, -0.8975640535354614, 1.9826834201812744, -1.9789758920669556, -2.8818085193634033],
     [-0.6626349687576294, -0.03326578065752983, -0.1879543960094452, -0.6107876896858215, 0.16164331138134003]
   ]
 >,
 #Nx.Tensor<
   u32[2]
   [1896456402, 17229315]
 >}
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][5]
  [
    [-0.11150000244379044, 0.12039999663829803, -0.36959999799728394, -0.24040000140666962, -1.1969000101089478],
    [0.2092999964952469, -0.9724000096321106, -0.7549999952316284, 0.3239000141620636, -0.10849999636411667]
  ]
>
```

```elixir
model =
  Axon.input("input", shape: {nil, 5})
  |> Axon.dense(6)
  |> Axon.activation(:relu)

{init_fn, predict_fn} = Axon.build(model)
template = Nx.template({1, 5}, :f32)
params = init_fn.(template, %{})
result = predict_fn.(params, batch_example) 
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][6]
  [
    [0.0, 0.6834545731544495, 0.0, 0.428937703371048, 0.5622736215591431, 0.5738084316253662],
    [0.05062224343419075, 0.6987380385398865, 0.7647035121917725, 0.5249872803688049, 0.6530346274375916, 0.4974510371685028]
  ]
>
```

```elixir
Nx.mean(result, axes: [-1], keep_axes: true) |> IO.inspect(label: "Variance")
Nx.variance(result, axes: [-1], keep_axes: true) |> IO.inspect(label: "Variance")
```

<!-- livebook:{"output":true} -->

```
Variance: #Nx.Tensor<
  f32[2][1]
  [
    [0.3747457265853882],
    [0.5315894484519958]
  ]
>
Variance: #Nx.Tensor<
  f32[2][1]
  [
    [0.07564987987279892],
    [0.05492803454399109]
  ]
>
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][1]
  [
    [0.07564987987279892],
    [0.05492803454399109]
  ]
>
```

```elixir
Nx.add(1, Nx.tensor([1,2]))
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[2]
  [2, 3]
>
```

```elixir
defmodule TransformerLayers.Norm do
  import Nx.Defn

  def normalization(%Axon{} = input, opts \\ []) do
    opts = Keyword.validate!(opts, [:name, :eps, :emb_dim])
    eps = Keyword.get(opts, :eps, 1.00e-5)
    scale = Axon.param("scale", {opts[:emb_dim]}, initializer: &ones(&1, type: &2))
    shift = Axon.param("shift", {opts[:emb_dim]}, initializer: &zeros(&1, type: &2))

    Axon.layer(
      &normalization_impl/4,
      [input, scale, shift],
      name: opts[:name],
      op_name: :normalization,
      eps: eps
    )
  end

  defp ones(shape, opts) do
    opts = Keyword.validate!(opts, [:type])
    Nx.iota(shape, type: opts[:type]) |> Nx.fill(1)
  end

  defp zeros(shape, opts) do
    opts = Keyword.validate!(opts, [:type])
    Nx.iota(shape, type: opts[:type]) |> Nx.fill(0)
  end

  defnp normalization_impl(input, scale, shift, opts \\ []) do
    mean = Nx.mean(input, axes: [-1], keep_axes: true)
    variance = Nx.variance(input, axes: [-1], keep_axes: true)
    denominator = variance |> Nx.add(opts[:eps]) |> Nx.sqrt()

    input
    |> Nx.subtract(mean)
    |> Nx.divide(denominator)
    |> Nx.multiply(scale)
    |> Nx.add(shift)
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, TransformerLayers.Norm, <<70, 79, 82, 49, 0, 0, 18, ...>>, true}
```

The variable `eps` is a small constant (epsilon) added to the variance to prevent division by zero
during normalization. The `scale` and `shift` are two trainable parameters (of the same dimension as the input) that the LLM automatically adjusts during training if it is determined that doing so would improve the model’s performance on its training task. This allows the model to learn appropriate scaling and shifting that best suit the data it is processing.

```elixir
model =
  Axon.input("input", shape: {nil, 5})
  |> Axon.dense(6)
  |> Axon.activation(:relu)
  |> TransformerLayers.Norm.normalization(emb_dim: 6, name: "norm")

{init_fn, predict_fn} = Axon.build(model)
template = Nx.template({1, 5}, :f32)
params = init_fn.(template, %{})
result = predict_fn.(params, batch_example) 
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][6]
  [
    [0.11638965457677841, -1.2090612649917603, -0.3836674094200134, -0.490633100271225, -0.06549333035945892, 2.0324654579162598],
    [-0.44719570875167847, 2.235978603363037, -0.44719570875167847, -0.44719570875167847, -0.44719570875167847, -0.44719570875167847]
  ]
>
```

```elixir
Nx.mean(result, axes: [-1], keep_axes: true) |> IO.inspect(label: "Variance")
Nx.variance(result, axes: [-1], keep_axes: true) |> IO.inspect(label: "Variance")
```

<!-- livebook:{"output":true} -->

```
Variance: #Nx.Tensor<
  f32[2][1]
  [
    [1.2417634698280722e-9],
    [9.934107758624577e-9]
  ]
>
Variance: #Nx.Tensor<
  f32[2][1]
  [
    [0.9997503757476807],
    [0.9999200701713562]
  ]
>
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][1]
  [
    [0.9997503757476807],
    [0.9999200701713562]
  ]
>
```

```elixir
model =
  Axon.input("input", shape: {nil, 5})
  |> TransformerLayers.Norm.normalization(emb_dim: 5, name: "norm")

{init_fn, predict_fn} = Axon.build(model)
template = Nx.template({1, 5}, :f32)
params = init_fn.(template, %{})
result = predict_fn.(params, batch_example) 
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][5]
  [
    [0.5527316331863403, 1.0693719387054443, -0.02227856032550335, 0.26556071639060974, -1.86538565158844],
    [0.9086875319480896, -1.3767629861831665, -0.9563034772872925, 1.1303280591964722, 0.2940508723258972]
  ]
>
```

```elixir
Nx.mean(result, axes: [-1], keep_axes: true) |> IO.inspect(label: "Variance")
Nx.variance(result, axes: [-1], keep_axes: true) |> IO.inspect(label: "Variance")
```

<!-- livebook:{"output":true} -->

```
Variance: #Nx.Tensor<
  f32[2][1]
  [
    [1.527368986842248e-8],
    [0.0]
  ]
>
Variance: #Nx.Tensor<
  f32[2][1]
  [
    [0.9999502301216125],
    [0.9999626278877258]
  ]
>
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][1]
  [
    [0.9999502301216125],
    [0.9999626278877258]
  ]
>
```

## 4.3 Implementing a feed forward networkwith GELU activations

Historically, the ReLU activation function has been commonly used in deep learning
due to its simplicity and effectiveness across various neural network architectures.
However, in LLMs, several other activation functions are employed beyond the traditional ReLU. Two notable examples are GELU (Gaussian error linear unit) and SwiGLU
(Swish-gated linear unit).

GELU and SwiGLU are more complex and smooth activation functions incorporating Gaussian and sigmoid-gated linear units, respectively. They offer improved performance for deep learning models, unlike the simpler ReLU.

The GELU activation function can be implemented in several ways; the exact version is defined as GELU(x) = x⋅Φ(x), where Φ(x) is the cumulative distribution function of the standard Gaussian distribution.

```elixir
x = Nx.linspace(-3, 3, n: 100) 
y_gelu = Axon.Activations.gelu(x) |> Nx.to_flat_list
y_relu = Axon.Activations.relu(x) |> Nx.to_flat_list
x = Nx.to_flat_list(x)
```

<!-- livebook:{"output":true} -->

```
[-3.0, -2.939393997192383, -2.8787879943847656, -2.8181817531585693, -2.757575750350952,
 -2.696969747543335, -2.6363635063171387, -2.5757575035095215, -2.5151515007019043,
 -2.454545497894287, -2.39393949508667, -2.3333332538604736, -2.2727272510528564,
 -2.2121212482452393, -2.151515007019043, -2.090909004211426, -2.0303030014038086,
 -1.9696969985961914, -1.9090908765792847, -1.848484754562378, -1.7878787517547607,
 -1.7272727489471436, -1.6666666269302368, -1.60606050491333, -1.545454502105713,
 -1.4848484992980957, -1.424242377281189, -1.3636362552642822, -1.303030252456665,
 -1.2424242496490479, -1.1818181276321411, -1.1212120056152344, -1.0606060028076172, -1.0,
 -0.9393939971923828, -0.8787877559661865, -0.8181817531585693, -0.7575757503509521,
 -0.6969695091247559, -0.6363635063171387, -0.5757575035095215, -0.5151515007019043,
 -0.4545454978942871, -0.3939392566680908, -0.33333325386047363, -0.27272725105285645,
 -0.21212100982666016, -0.15151500701904297, -0.09090900421142578, -0.030303001403808594, ...]
```

```elixir
alias VegaLite, as: Vl
Vl.new(title: "GeLU vs ReLU", width: 400, height: 400)
|> Vl.data_from_values(x: x, relu: y_relu, gelu: y_gelu)
#|> Vl.mark(:line)

|> Vl.layers([
  Vl.new()
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "x", type: :quantitative)
  |> Vl.encode_field(:y, "relu", type: :quantitative),
  Vl.new()
  |> Vl.mark(:line)
  |> Vl.encode(:color, value: "#db646f")
  |> Vl.encode_field(:x, "x", type: :quantitative)
  |> Vl.encode_field(:y, "gelu", type: :quantitative)
])
```

<!-- livebook:{"output":true} -->

```vega-lite
{"$schema":"https://vega.github.io/schema/vega-lite/v5.json","data":{"values":[{"gelu":-0.0040496885776519775,"relu":0.0,"x":-3.0},{"gelu":-0.004833197221159935,"relu":0.0,"x":-2.939393997192383},{"gelu":-0.005746176932007074,"relu":0.0,"x":-2.8787879943847656},{"gelu":-0.006805408746004105,"relu":0.0,"x":-2.8181817531585693},{"gelu":-0.008028950542211533,"relu":0.0,"x":-2.757575750350952},{"gelu":-0.009435816667973995,"relu":0.0,"x":-2.696969747543335},{"gelu":-0.011046357452869415,"relu":0.0,"x":-2.6363635063171387},{"gelu":-0.012881461530923843,"relu":0.0,"x":-2.5757575035095215},{"gelu":-0.014962762594223022,"relu":0.0,"x":-2.5151515007019043},{"gelu":-0.017312245443463326,"relu":0.0,"x":-2.454545497894287},{"gelu":-0.01995168812572956,"relu":0.0,"x":-2.39393949508667},{"gelu":-0.02290244773030281,"relu":0.0,"x":-2.3333332538604736},{"gelu":-0.026184793561697006,"relu":0.0,"x":-2.2727272510528564},{"gelu":-0.029817499220371246,"relu":0.0,"x":-2.2121212482452393},{"gelu":-0.033817026764154434,"relu":0.0,"x":-2.151515007019043},{"gelu":-0.038196951150894165,"relu":0.0,"x":-2.090909004211426},{"gelu":-0.04296703264117241,"relu":0.0,"x":-2.0303030014038086},{"gelu":-0.048132624477148056,"relu":0.0,"x":-1.9696969985961914},{"gelu":-0.05369355529546738,"relu":0.0,"x":-1.9090908765792847},{"gelu":-0.05964341387152672,"relu":0.0,"x":-1.848484754562378},{"gelu":-0.06596875190734863,"relu":0.0,"x":-1.7878787517547607},{"gelu":-0.07264793664216995,"relu":0.0,"x":-1.7272727489471436},{"gelu":-0.07965058088302612,"relu":0.0,"x":-1.6666666269302368},{"gelu":-0.08693656325340271,"relu":0.0,"x":-1.60606050491333},{"gelu":-0.09445537626743317,"relu":0.0,"x":-1.545454502105713},{"gelu":-0.10214567929506302,"relu":0.0,"x":-1.4848484992980957},{"gelu":-0.10993465781211853,"relu":0.0,"x":-1.424242377281189},{"gelu":-0.1177377924323082,"relu":0.0,"x":-1.3636362552642822},{"gelu":-0.12545864284038544,"relu":0.0,"x":-1.303030252456665},{"gelu":-0.13298910856246948,"relu":0.0,"x":-1.2424242496490479},{"gelu":-0.1402096450328827,"relu":0.0,"x":-1.1818181276321411},{"gelu":-0.14698955416679382,"relu":0.0,"x":-1.1212120056152344},{"gelu":-0.1531880795955658,"relu":0.0,"x":-1.0606060028076172},{"gelu":-0.15865525603294373,"relu":0.0,"x":-1.0},{"gelu":-0.16323310136795044,"relu":0.0,"x":-0.9393939971923828},{"gelu":-0.16675716638565063,"relu":0.0,"x":-0.8787877559661865},{"gelu":-0.1690582036972046,"relu":0.0,"x":-0.8181817531585693},{"gelu":-0.169964000582695,"relu":0.0,"x":-0.7575757503509521},{"gelu":-0.16930150985717773,"relu":0.0,"x":-0.6969695091247559},{"gelu":-0.1668989062309265,"relu":0.0,"x":-0.6363635063171387},{"gelu":-0.16258789598941803,"relu":0.0,"x":-0.5757575035095215},{"gelu":-0.15620608627796173,"relu":0.0,"x":-0.5151515007019043},{"gelu":-0.14759916067123413,"relu":0.0,"x":-0.4545454978942871},{"gelu":-0.13662324845790863,"relu":0.0,"x":-0.3939392566680908},{"gelu":-0.12314710021018982,"relu":0.0,"x":-0.33333325386047363},{"gelu":-0.10705402493476868,"relu":0.0,"x":-0.27272725105285645},{"gelu":-0.08824368566274643,"relu":0.0,"x":-0.21212100982666016},{"gelu":-0.06663398444652557,"relu":0.0,"x":-0.15151500701904297},{"gelu":-0.04216200113296509,"relu":0.0,"x":-0.09090900421142578},{"gelu":-0.0147852199152112,"relu":0.0,"x":-0.030303001403808594},{"gelu":0.015517781488597393,"relu":0.030303001403808594,"x":0.030303001403808594},{"gelu":0.04874713718891144,"relu":0.09090924263000488,"x":0.09090924263000488},{"gelu":0.08488116413354874,"relu":0.15151524543762207,"x":0.15151524543762207},{"gelu":0.12387749552726746,"relu":0.21212124824523926,"x":0.21212124824523926},{"gelu":0.1656734049320221,"relu":0.27272748947143555,"x":0.27272748947143555},{"gelu":0.21018634736537933,"relu":0.33333349227905273,"x":0.33333349227905273},{"gelu":0.2573162317276001,"relu":0.3939394950866699,"x":0.3939394950866699},{"gelu":0.306946337223053,"relu":0.4545454978942871,"x":0.4545454978942871},{"gelu":0.35894539952278137,"relu":0.5151515007019043,"x":0.5151515007019043},{"gelu":0.41316983103752136,"relu":0.5757577419281006,"x":0.5757577419281006},{"gelu":0.46946483850479126,"relu":0.6363637447357178,"x":0.6363637447357178},{"gelu":0.5276682376861572,"relu":0.696969747543335,"x":0.696969747543335},{"gelu":0.587611973285675,"relu":0.7575759887695312,"x":0.7575759887695312},{"gelu":0.6491237878799438,"relu":0.8181819915771484,"x":0.8181819915771484},{"gelu":0.712030827999115,"relu":0.8787879943847656,"x":0.8787879943847656},{"gelu":0.7761608958244324,"relu":0.9393939971923828,"x":0.9393939971923828},{"gelu":0.8413447141647339,"relu":1.0,"x":1.0},{"gelu":0.9074179530143738,"relu":1.0606060028076172,"x":1.0606060028076172},{"gelu":0.9742224216461182,"relu":1.1212120056152344,"x":1.1212120056152344},{"gelu":1.0416089296340942,"relu":1.1818184852600098,"x":1.1818184852600098},{"gelu":1.1094354391098022,"relu":1.242424488067627,"x":1.242424488067627},{"gelu":1.1775718927383423,"relu":1.3030304908752441,"x":1.3030304908752441},{"gelu":1.245898723602295,"relu":1.3636364936828613,"x":1.3636364936828613},{"gelu":1.3143079280853271,"relu":1.4242424964904785,"x":1.4242424964904785},{"gelu":1.3827028274536133,"relu":1.4848484992980957,"x":1.4848484992980957},{"gelu":1.450999140739441,"relu":1.545454502105713,"x":1.545454502105713},{"gelu":1.5191245079040527,"relu":1.6060609817504883,"x":1.6060609817504883},{"gelu":1.5870164632797241,"relu":1.6666669845581055,"x":1.6666669845581055},{"gelu":1.6546250581741333,"relu":1.7272729873657227,"x":1.7272729873657227},{"gelu":1.7219102382659912,"relu":1.7878789901733398,"x":1.7878789901733398},{"gelu":1.7888414859771729,"relu":1.848484992980957,"x":1.848484992980957},{"gelu":1.8553974628448486,"relu":1.9090909957885742,"x":1.9090909957885742},{"gelu":1.9215643405914307,"relu":1.9696969985961914,"x":1.9696969985961914},{"gelu":1.9873359203338623,"relu":2.0303030014038086,"x":2.0303030014038086},{"gelu":2.0527119636535645,"relu":2.090909004211426,"x":2.090909004211426},{"gelu":2.1176984310150146,"relu":2.151515483856201,"x":2.151515483856201},{"gelu":2.1823041439056396,"relu":2.2121214866638184,"x":2.2121214866638184},{"gelu":2.2465426921844482,"relu":2.2727274894714355,"x":2.2727274894714355},{"gelu":2.3104310035705566,"relu":2.3333334922790527,"x":2.3333334922790527},{"gelu":2.3739876747131348,"relu":2.39393949508667,"x":2.39393949508667},{"gelu":2.4372332096099854,"relu":2.454545497894287,"x":2.454545497894287},{"gelu":2.5001888275146484,"relu":2.5151515007019043,"x":2.5151515007019043},{"gelu":2.5628764629364014,"relu":2.5757579803466797,"x":2.5757579803466797},{"gelu":2.6253178119659424,"relu":2.636363983154297,"x":2.636363983154297},{"gelu":2.6875340938568115,"relu":2.696969985961914,"x":2.696969985961914},{"gelu":2.749547004699707,"relu":2.7575759887695312,"x":2.7575759887695312},{"gelu":2.8113765716552734,"relu":2.8181819915771484,"x":2.8181819915771484},{"gelu":2.873041868209839,"relu":2.8787879943847656,"x":2.8787879943847656},{"gelu":2.934560775756836,"relu":2.939393997192383,"x":2.939393997192383},{"gelu":2.995950222015381,"relu":3.0,"x":3.0}]},"height":400,"layer":[{"encoding":{"x":{"field":"x","type":"quantitative"},"y":{"field":"relu","type":"quantitative"}},"mark":"line"},{"encoding":{"color":{"value":"#db646f"},"x":{"field":"x","type":"quantitative"},"y":{"field":"gelu","type":"quantitative"}},"mark":"line"}],"title":"GeLU vs ReLU","width":400}
```

The smoothness of GELU can lead to better optimization properties during training,
as it allows for more nuanced adjustments to the model’s parameters. In contrast,
ReLU has a sharp corner at zero, which can sometimes make opti
mization harder, especially in networks that are very deep or have complex architec
tures. Moreover, unlike ReLU, which outputs zero for any negative input, GELU
allows for a small, non-zero output for negative values. This characteristic means that
during the training process, neurons that receive negative input can still contribute to
the learning process, albeit to a lesser extent than positive inputs.

```elixir
emb_dim = 768

model =
  Axon.input("sequence", shape: {nil, 3, emb_dim})
  |> Axon.dense(4*emb_dim)
  |> Axon.activation(:gelu)
  |> Axon.dense(emb_dim)

template = Nx.template({2, 3, emb_dim}, :f32)
Axon.Display.as_graph(model, template)
```

<!-- livebook:{"output":true} -->

```mermaid
graph TD;
10[/"sequence (:input) {2, 3, 768}"/];
11["dense_0 (:dense) {2, 3, 3072}"];
12["gelu_0 (:gelu) {2, 3, 3072}"];
13["dense_1 (:dense) {2, 3, 768}"];
12 --> 13;
11 --> 12;
10 --> 11;
```

```elixir
{init_fn, predict_fn} = Axon.build(model, compiler: EXLA)
params = init_fn.(template, %{})
#result = predict_fn.(params, batch_example)
```

<!-- livebook:{"output":true} -->

```
%{
  "dense_0" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      EXLA.Backend<host:0, 0.1768894099.981860376.232361>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      EXLA.Backend<host:0, 0.1768894099.981860376.232362>
      [
        [0.02097768522799015, 0.031294386833906174, 0.0024026273749768734, 0.005480404011905193, -0.007342961151152849, 0.0028619689401239157, -0.004735986236482859, -0.038826681673526764, -0.00559077225625515, -0.007228936068713665, -0.018880367279052734, 0.031191624701023102, -0.032205719500780106, -0.012479443103075027, -0.025089016184210777, 0.007253872696310282, -0.009341426193714142, 0.004801089409738779, -0.015404026955366135, 0.038609083741903305, 0.013533261604607105, -0.015206342563033104, -0.036094438284635544, -0.010683449916541576, 0.018807319924235344, -0.026785846799612045, 0.02419404499232769, 0.0030467987526208162, 0.02585729770362377, 0.007523804437369108, -0.03833182156085968, 0.01789243519306183, 0.0032201968133449554, -0.002261083573102951, 0.010172858834266663, 0.024634934961795807, 0.0201718769967556, 0.03874386101961136, 0.010313451290130615, 0.039134178310632706, -0.019057704135775566, 0.024506641551852226, 0.009927967563271523, -0.023874692618846893, -0.025675596669316292, 0.0035961801186203957, 0.006239184178411961, ...],
        ...
      ]
    >
  },
  "dense_1" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      EXLA.Backend<host:0, 0.1768894099.981860376.232363>
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      EXLA.Backend<host:0, 0.1768894099.981860376.232364>
      [
        [0.022473853081464767, -0.013434239663183689, 0.01966402865946293, 0.003647533245384693, -0.023335669189691544, -0.02398679591715336, 0.002741054864600301, 0.01993248052895069, -0.017838697880506516, -0.01836279407143593, -0.005626659840345383, -0.03902643918991089, -5.746009628637694e-5, 0.020980115979909897, 0.027108196169137955, 0.002506313845515251, 0.022075900807976723, 0.007029555272310972, -0.0010618007509037852, -0.02154512330889702, -0.026461642235517502, -0.03096531704068184, 0.014962450601160526, -0.025245301425457, -0.01907324604690075, -0.0079491613432765, -0.03935951367020607, -0.035175736993551254, 0.01731068082153797, -0.01155348401516676, -0.035019755363464355, 0.012266943231225014, -0.02797056920826435, 0.026368537917733192, -0.015977486968040466, 0.009221812710165977, 0.009152383543550968, 0.01810385100543499, 0.0347323901951313, -0.02875596471130848, 0.03782958909869194, 0.025884779170155525, 0.03811647370457649, -8.691581315360963e-4, 0.031269337981939316, -0.027123700827360153, ...],
        ...
      ]
    >
  }
}
```

```elixir
{x, _new_key} = Nx.Random.normal(key, shape: {2,3,768})
result = predict_fn.(params, x)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][3][768]
  EXLA.Backend<host:0, 0.1768894099.981860376.232366>
  [
    [
      [-0.06589678674936295, 0.5753874778747559, -0.04706764221191406, -0.20908518135547638, 0.5793106555938721, 0.33128705620765686, 0.2559962868690491, -0.6003052592277527, 0.38525670766830444, 0.5195212960243225, 0.8793835043907166, 0.4150995910167694, 0.03224582225084305, 0.03605985641479492, 1.3245837688446045, 0.9497147798538208, 0.3406720757484436, -0.09657637774944305, 0.37301599979400635, -0.08912376314401627, -0.4522574245929718, 0.18592101335525513, -0.4339276850223541, 0.25965243577957153, 0.4061870872974396, 0.04942312836647034, 0.4992932677268982, -0.4300670623779297, 0.8751217126846313, 0.314741313457489, -0.11185029149055481, 0.1205078512430191, 0.11451971530914307, -0.058551207184791565, -0.015350431203842163, 0.20886856317520142, -0.581383228302002, 1.4550261497497559, 0.271809458732605, -0.5003750920295715, -0.3515125811100006, 0.6320643424987793, -0.3868289291858673, -0.2776110768318176, 0.05392007529735565, -0.39922580122947693, -0.14714351296424866, 0.859970211982727, -0.5242009162902832, -0.8396066427230835, ...],
      ...
    ],
    ...
  ]
>
```

```elixir
Nx.shape(result)
```

<!-- livebook:{"output":true} -->

```
{2, 3, 768}
```

The FeedForward module plays a crucial role in enhancing the model’s ability to learn
from and generalize the data. Although the input and output dimensions of this
module are the same, it internally expands the embedding dimension into a higher
dimensional space through the first linear layer.

This expansion is followed by a nonlinear GELU activation and then a contraction back to the original dimension with the second linear transformation. Such a design allows for the
exploration of a richer representation space.

shortcut connections that we insert between different layers of a neural network, which are important for improving the training
performance in deep neural network architectures.

## 4.4 Adding shortcut connections

The shortcut connections were proposed for deep networks in
computer vision (specifically, in residual networks) to mitigate the challenge of vanishing gradients. The vanishing gradient problem refers to the issue where gradients
(which guide weight updates during training) become progressively smaller as they
propagate backward through the layers, making it difficult to effectively train earlier
layers.

```elixir
defmodule LLM.Layer do
  def shortcut(x, layer_impl, opts \\ []) when is_function(layer_impl) do
    with {:arity, arity} <- Function.info(layer_impl, :arity),
          layer_output <- execute_layer(x, layer_impl, opts, arity),
          output <- shortcut_impl(x, layer_output, opts) do
      output
    end    
  end

  defp execute_layer(x, layer_impl, _opts, 1), do: layer_impl.(x)
  defp execute_layer(x, layer_impl, opts, _arity), do: layer_impl.(x, opts)
  defp shortcut_impl(x, layer_output, opts) do
    use_shortcut? = Keyword.get(opts, :use_shortcut, false)
    if use_shortcut?, 
      do: Axon.add(x, layer_output),
      else: layer_output
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, LLM.Layer, <<70, 79, 82, 49, 0, 0, 10, ...>>, {:shortcut_impl, 3}}
```

```elixir
model =
  Axon.input("input", shape: {nil, 2})
  #|>Axon.dense(2)
  |> LLM.Layer.shortcut(&Axon.dense(&1, 2, use_bias: false))

model_with_shortcut =
  Axon.input("input", shape: {nil, 2})
  |> LLM.Layer.shortcut(
    &Axon.dense(&1, 2, use_bias: false),
    use_shortcut: true
  )
{_init_fn, predict_with_short_fn} = Axon.build(model_with_shortcut)
{init_fn, predict_fn} = Axon.build(model)
template = Nx.template({1, 2}, :f32)

params = init_fn.(template, %{})
```

<!-- livebook:{"output":true} -->

```
%{
  "dense_0" => %{
    "kernel" => #Nx.Tensor<
      f32[2][2]
      [
        [-1.0623576641082764, 0.539414644241333],
        [-0.37706416845321655, -0.6262171268463135]
      ]
    >
  }
}
```

```elixir
predict_fn.(params, Nx.tensor([[103, 103]]))
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1][2]
  [
    [-148.2604522705078, -8.940655708312988]
  ]
>
```

```elixir
predict_with_short_fn.(params, Nx.tensor([[103, 103]]))
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1][2]
  [
    [-45.26045227050781, 94.05934143066406]
  ]
>
```

In conclusion, shortcut connections are important for overcoming the limitations
posed by the vanishing gradient problem in deep neural networks. Shortcut connections are a core building block of very large models such as LLMs.

## 4.5 Connecting attention and linear layers in a transformer block

```elixir
defmodule Transformer.Layers do
  import Nx.Defn
  
  def attention(%Axon{} = input, opts \\ []) do
    #opts = Keyword.validate!(opts, [:name, :d_in, :d_out, :num_heads])
    head_dim = div(opts[:d_out], opts[:num_heads])
    w_query = Axon.param("w_query", fn _ -> {opts[:d_in], opts[:d_out]} end)
    w_key = Axon.param("w_key", fn _ -> {opts[:d_in], opts[:d_out]} end)
    w_value = Axon.param("w_value", fn _ -> {opts[:d_in], opts[:d_out]} end)
    out_proj = Axon.param("out_proj", fn _ -> {opts[:d_out], opts[:d_out]} end)

    Axon.layer(
      &attention_impl/6,
      [input, w_query, w_key, w_value, out_proj],
      name: opts[:name],
      op_name: :causal_attention,
      head_dim: head_dim,
      num_heads: opts[:num_heads]
    )
  end

  #defnp attention_impl(input, w_query, w_key, w_value, head_dim, num_heads, _opts \\ []) do
  defnp attention_impl(input, w_query, w_key, w_value, out_proj, opts \\ []) do
    {b, num_tokens, _d_in} = Nx.shape(input)
    keys = Nx.dot(input, w_key)
    queries = Nx.dot(input, w_query)
    values = Nx.dot(input, w_value)
    d_k = Nx.axis_size(keys, -1)

    keys_reshaped = 
      keys
      |> Nx.reshape({b, num_tokens, opts[:num_heads], opts[:head_dim]}) 
      |> Nx.transpose(axes: [0, 2, 1, 3])
    
    queries_reshaped = 
      queries
      |> Nx.reshape({b, num_tokens, opts[:num_heads], opts[:head_dim]}) 
      |> Nx.transpose(axes: [0, 2, 1, 3])
    
    values_reshaped = 
      values
      |> Nx.reshape({b, num_tokens, opts[:num_heads], opts[:head_dim]}) 
      |> Nx.transpose(axes: [0, 2, 1, 3])

    attn_score =
      keys_reshaped
      |> Nx.transpose(axes: [0, 1, 3, 2])
      |> then(&Nx.dot(queries_reshaped, [3], [0, 1], &1, [2], [0, 1]))

    simple_mask =
      attn_score
      |> then(&Nx.broadcast(Nx.Constants.infinity(), &1))
      |> Nx.triu(k: 1)

    masked = Nx.multiply(simple_mask, -1) |> Nx.add(attn_score)

    attn_weights =
      masked
      |> Nx.divide(Nx.pow(d_k, 0.5))
      |> Axon.Activations.softmax(axis: -1)
    
    context_vec =
      attn_weights
      |> Nx.dot([3], [0, 1], values_reshaped, [2], [0, 1])
      |> Nx.transpose(axes: [0, 2, 1, 3])

    context_vec
    |> Nx.reshape({b, num_tokens, opts[:num_heads] * opts[:head_dim]})
    |> Nx.dot(out_proj)
  end

  def shortcut(x, layer_impl, opts \\ []) when is_function(layer_impl) do
    with {:arity, arity} <- Function.info(layer_impl, :arity),
          layer_output <- execute_layer(x, layer_impl, opts, arity),
          output <- shortcut_impl(x, layer_output, opts) do
      output
    end    
  end

  defp execute_layer(x, layer_impl, _opts, 1), do: layer_impl.(x)
  defp execute_layer(x, layer_impl, opts, _arity), do: layer_impl.(x, opts)
  defp shortcut_impl(x, layer_output, opts) do
    use_shortcut? = Keyword.get(opts, :use_shortcut, false)
    if use_shortcut?, 
      do: Axon.add(x, layer_output),
      else: layer_output
  end

  def normalization(%Axon{} = input, opts \\ []) do
    #opts = Keyword.validate!(opts, [:name, :eps, :emb_dim])
    eps = Keyword.get(opts, :eps, 1.00e-5)
    scale = Axon.param("scale", {opts[:emb_dim]}, initializer: &ones(&1, type: &2))
    shift = Axon.param("shift", {opts[:emb_dim]}, initializer: &zeros(&1, type: &2))

    Axon.layer(
      &normalization_impl/4,
      [input, scale, shift],
      name: opts[:name],
      op_name: :normalization,
      eps: eps
    )
  end

  defp ones(shape, opts) do
    opts = Keyword.validate!(opts, [:type])
    Nx.iota(shape, type: opts[:type]) |> Nx.fill(1)
  end

  defp zeros(shape, opts) do
    opts = Keyword.validate!(opts, [:type])
    Nx.iota(shape, type: opts[:type]) |> Nx.fill(0)
  end

  defnp normalization_impl(input, scale, shift, opts \\ []) do
    mean = Nx.mean(input, axes: [-1], keep_axes: true)
    variance = Nx.variance(input, axes: [-1], keep_axes: true)
    denominator = variance |> Nx.add(opts[:eps]) |> Nx.sqrt()

    input
    |> Nx.subtract(mean)
    |> Nx.divide(denominator)
    |> Nx.multiply(scale)
    |> Nx.add(shift)
  end

  def feedforward(input, emb_dim) do
    input
    |> Axon.dense(4*emb_dim)
    |> Axon.activation(:gelu)
    |> Axon.dense(emb_dim)
  end

  def feedforward_block(input, opts) do
    input
    |> normalization(opts)
    |> feedforward(opts[:emb_dim])
    |> Axon.dropout(rate: opts[:drop_rate])
  end

  def attention_block(input, opts) do
    input
    |> normalization(opts)
    |> attention(d_in: opts[:emb_dim], d_out: opts[:emb_dim], num_heads: opts[:n_heads] )
    |> Axon.dropout(rate: opts[:drop_rate])
  end

  def block(input, opts \\ []) do
    input
    |> shortcut(&attention_block(&1, opts), use_shortcut: true)
    |> shortcut(&feedforward_block(&1, opts), use_shortcut: true)
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Transformer.Layers, <<70, 79, 82, 49, 0, 0, 45, ...>>, {:block, 2}}
```

```elixir
gpt_config_124m
```

<!-- livebook:{"output":true} -->

```
[
  attn_name: "attention_0",
  vocab_size: 50257,
  context_length: 1024,
  emb_dim: 768,
  n_heads: 12,
  n_layers: 12,
  drop_rate: 0.1,
  qkv_bias: false
]
```

```elixir
model = 
  Axon.input("sequence", shape: {2,4,768})
  |> Transformer.Layers.block(gpt_config_124m)

{init_fn, predict_fn} = Axon.build(model)
template = Nx.template({2,4,768}, :f32)

params = init_fn.(template, %{})
```

<!-- livebook:{"output":true} -->

```
%{
  "causal_attention_0" => %{
    "out_proj" => #Nx.Tensor<
      f32[768][768]
      [
        [0.048212990164756775, -0.02135278284549713, 4.1525065898895264e-4, 0.0010306984186172485, 0.02545630931854248, 0.0011367946863174438, -0.018539682030677795, 0.03942526876926422, -0.05111232399940491, 0.005287677049636841, 0.0402548611164093, -0.028292834758758545, -0.021534278988838196, 0.061078235507011414, -0.03922495245933533, 0.06146968901157379, 0.035551056265830994, -0.03532347083091736, 0.027499482035636902, -0.008672282099723816, 0.01170453429222107, -0.02114388346672058, -0.03899948298931122, -0.04424773156642914, -0.004638671875, 0.061769649386405945, -0.03490301966667175, -0.051083266735076904, 0.011647403240203857, -0.015038996934890747, 0.02045176923274994, 0.019451528787612915, -0.03217315673828125, -0.015817835927009583, -0.026152417063713074, -0.006401389837265015, 0.0457518994808197, 0.03007124364376068, -0.01338416337966919, 0.004566371440887451, -0.024767577648162842, -0.05465881526470184, 0.04641236364841461, 0.005639776587486267, -0.014009863138198853, 0.019025638699531555, 0.019213750958442688, 0.04369349777698517, ...],
        ...
      ]
    >,
    "w_key" => #Nx.Tensor<
      f32[768][768]
      [
        [0.04515509307384491, -8.539706468582153e-4, -0.0014599114656448364, -0.049233272671699524, -0.01645684242248535, 0.050656914710998535, -0.03950360417366028, -0.03971555829048157, -0.05294997990131378, 0.020471900701522827, -0.011017948389053345, -0.036764368414878845, -0.04229655861854553, 0.004784315824508667, 0.04555578529834747, 0.052394866943359375, -0.020329609513282776, 0.03703536093235016, 0.0038404762744903564, -0.02730606496334076, -0.0422617644071579, -0.023164570331573486, 0.012925907969474792, -0.01633840799331665, 0.04552869498729706, 0.0059317946434021, 0.06138455867767334, 0.04674258828163147, -0.04867313802242279, 0.017092615365982056, -0.04353523254394531, -0.025372594594955444, 0.002022877335548401, -0.04072807729244232, -0.057321399450302124, 0.018066823482513428, -0.05403272807598114, -0.02611604332923889, 0.027959033846855164, -3.324449062347412e-4, -0.05752812325954437, 0.022641107439994812, -0.010749921202659607, 0.041250571608543396, 0.026591137051582336, 0.03248697519302368, -0.052971333265304565, ...],
        ...
      ]
    >,
    "w_query" => #Nx.Tensor<
      f32[768][768]
      [
        [0.055858805775642395, -0.05666010081768036, 0.027975961565971375, 0.02930283546447754, -0.020014166831970215, 0.03396531939506531, 0.025497496128082275, -0.05639638006687164, 0.03342175483703613, -0.04305167496204376, -0.0319366455078125, 0.04718504846096039, -0.04320657253265381, 0.061369866132736206, -0.005380570888519287, 0.040038108825683594, -0.058081164956092834, -0.02334900200366974, -0.030190959572792053, 0.017649903893470764, 0.0020727068185806274, -0.053653210401535034, 0.04725658893585205, -0.008756250143051147, -0.005902141332626343, 0.04160770773887634, -0.0245942622423172, -0.01207759976387024, -0.007174432277679443, -0.04752166569232941, 0.03584432601928711, -0.051740869879722595, 0.06141798198223114, -0.027664348483085632, 0.031125396490097046, -0.05798402428627014, 0.02153092622756958, 0.046145349740982056, 0.05683125555515289, -0.02372080087661743, 0.05109764635562897, -0.009639918804168701, -0.0032905936241149902, 0.02997678518295288, 0.029068350791931152, -0.05478714406490326, ...],
        ...
      ]
    >,
    "w_value" => #Nx.Tensor<
      f32[768][768]
      [
        [0.05840298533439636, 0.028866425156593323, 0.011713162064552307, -0.04495878517627716, 0.028301119804382324, 0.009988978505134583, 0.0064216554164886475, -0.020921170711517334, -0.027949348092079163, -0.04108819365501404, 0.03623831272125244, -0.014106810092926025, -0.03876368701457977, 0.018321499228477478, 0.021634727716445923, -0.05844572186470032, -0.0028414279222488403, 0.0399271696805954, 0.0368209034204483, 0.05719780921936035, 0.058653995394706726, 0.03442680835723877, 0.002306535840034485, -0.04516732692718506, -0.017129972577095032, 0.04329982399940491, 0.018157079815864563, -0.013204649090766907, -0.05778177082538605, -0.059841275215148926, -0.054835185408592224, 0.061194807291030884, 0.004299908876419067, 0.03798624873161316, 0.05058261752128601, -0.030618980526924133, 0.013535797595977783, -0.023691296577453613, 0.05242954194545746, 0.027654066681861877, 0.05171090364456177, -0.029277294874191284, -0.014544710516929626, 0.003078833222389221, 0.003193885087966919, ...],
        ...
      ]
    >
  },
  "dense_0" => %{
    "bias" => #Nx.Tensor<
      f32[3072]
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[768][3072]
      [
        [0.03949105739593506, -0.015957998111844063, -0.012447861954569817, -0.032738324254751205, -0.03111119568347931, -0.018885089084506035, 0.027405261993408203, 0.0011886805295944214, -0.03255854547023773, -0.022515639662742615, -0.012260053306818008, -1.7232447862625122e-4, -0.030039330944418907, 0.025964178144931793, -0.004949115216732025, -0.016661128029227257, 0.029370345175266266, 0.02119271829724312, 0.034041136503219604, -0.033037737011909485, 0.01381520926952362, 0.004624232649803162, 0.008894987404346466, -0.021799899637699127, -0.029549039900302887, -0.010732777416706085, 0.013289030641317368, -0.03624830022454262, -0.02708686888217926, -0.03403199091553688, -0.029377715662121773, -0.009325603023171425, 0.0196542851626873, 0.027669347822666168, -0.015470892190933228, 0.012114938348531723, 0.019321229308843613, -0.021527528762817383, -0.03881030157208443, 0.008725754916667938, -0.03565309941768646, 0.019619178026914597, 0.03554204851388931, 0.03737794607877731, 0.02181464061141014, -0.009362565353512764, ...],
        ...
      ]
    >
  },
  "dense_1" => %{
    "bias" => #Nx.Tensor<
      f32[768]
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[3072][768]
      [
        [-0.010293981060385704, -0.031161747872829437, 0.029444150626659393, -0.031134512275457382, 0.022892441600561142, -0.00522550567984581, -0.004617907106876373, -0.008461317047476768, 0.022837180644273758, -0.03243391960859299, -0.021652286872267723, -0.008168220520019531, 0.01929134503006935, 0.027013927698135376, 0.038260750472545624, 0.03386042267084122, -0.02250041998922825, 0.015221826732158661, -0.03391241654753685, -0.003358498215675354, 0.030164040625095367, 0.008378937840461731, 0.0018902570009231567, -0.02223089337348938, -0.007182549685239792, 0.002218805253505707, 0.030978374183177948, -0.007535416632890701, -0.028109125792980194, -0.0021909773349761963, 0.02835843712091446, -0.019224684685468674, -1.0244175791740417e-4, 0.03894221782684326, -0.02970166690647602, -0.009980009868741035, -0.01619289070367813, 0.02163327857851982, -0.0326077975332737, -0.016736004501581192, -0.033043812960386276, -0.031646110117435455, 0.014016512781381607, 0.008738446980714798, 0.011421147733926773, ...],
        ...
      ]
    >
  },
  "normalization_0" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  },
  "normalization_1" => %{
    "scale" => #Nx.Tensor<
      f32[768]
      [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...]
    >,
    "shift" => #Nx.Tensor<
      f32[768]
      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]
    >
  }
}
```

```elixir
Axon.Display.as_graph(model, template)
```

<!-- livebook:{"output":true} -->

```mermaid
graph TD;
134[/"sequence (:input) {2, 4, 768}"/];
135["normalization_0 (:normalization) {2, 4, 768}"];
136["causal_attention_0 (:causal_attention) {2, 4, 768}"];
137["dropout_0 (:dropout) {2, 4, 768}"];
138["container_0 (:container) {{2, 4, 768}, {2, 4, 768}}"];
139["add_0 (:add) {2, 4, 768}"];
140["normalization_1 (:normalization) {2, 4, 768}"];
141["dense_0 (:dense) {2, 4, 3072}"];
142["gelu_0 (:gelu) {2, 4, 3072}"];
143["dense_1 (:dense) {2, 4, 768}"];
144["dropout_1 (:dropout) {2, 4, 768}"];
145["container_1 (:container) {{2, 4, 768}, {2, 4, 768}}"];
146["add_1 (:add) {2, 4, 768}"];
145 --> 146;
144 --> 145;
139 --> 145;
143 --> 144;
142 --> 143;
141 --> 142;
140 --> 141;
139 --> 140;
138 --> 139;
137 --> 138;
134 --> 138;
136 --> 137;
135 --> 136;
134 --> 135;
```

```elixir
key = Nx.Random.key(103)
{input, _new_key} = Nx.Random.normal(key, shape: {2,4,768})
result = predict_fn.(params, input)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[2][4][768]
  [
    [
      [-0.1704925298690796, 2.6229088306427, -0.613010048866272, 0.6813195943832397, -2.1811046600341797, 0.6973523497581482, -0.3068247139453888, 2.2546515464782715, -0.25860595703125, 1.6516574621200562, -1.8097500801086426, -0.834059476852417, 1.320986032485962, 0.46887099742889404, -1.3357069492340088, 1.0589089393615723, 0.7781949043273926, 1.1724971532821655, 0.21648138761520386, -3.5949342250823975, 1.1928690671920776, -0.12543022632598877, 0.23205339908599854, 1.5443565845489502, 0.8970698714256287, 0.7682569622993469, 0.8656567931175232, -2.480109453201294, 2.4590208530426025, 1.3989135026931763, 1.0046881437301636, 1.3691630363464355, -0.47362440824508667, 0.04292607307434082, 4.020037651062012, 1.7052584886550903, -2.3611581325531006, 1.651090383529663, -1.092333197593689, 2.8792104721069336, 1.2400672435760498, 0.8116423487663269, 2.3448500633239746, -0.226919025182724, -0.6237384080886841, 1.1861971616744995, 0.9084106087684631, -0.18755584955215454, -0.7010538578033447, -0.9347135424613953, ...],
      ...
    ],
    ...
  ]
>
```

The Transformer.Layers.block includes a multi-head
attention mechanism (MultiHeadAttention) and a feed forward network (Feed-Forward), both configured based on a provided configuration dictionary (cfg), such as GPT_CONFIG_124M.

Layer normalization (LayerNorm) is applied before each of these two components,
and dropout is applied after them to regularize the model and prevent overfitting. This is also known as Pre-LayerNorm. Older architectures, such as the original transformer model, applied layer normalization after the self-attention and feed forward networks instead, known as Post-LayerNorm, which often leads to worse training dynamics.

The module also implements the forward pass, where each component is followed by
a shortcut connection that adds the input of the block to its output. This critical feature helps gradients flow through the network during training and improves the learning of deep models.

The transformer block maintains the input dimensions in its output, indicating that the transformer architecture processes sequences of data without altering
their shape throughout the network.

<!-- livebook:{"break_markdown":true} -->

The preservation of shape throughout the transformer block architecture is not
incidental but a crucial aspect of its design. This design enables its effective applica-
tion across a wide range of sequence-to-sequence tasks, where each output vector
directly corresponds to an input vector, maintaining a one-to-one relationship. However, the output is a context vector that encapsulates information from the entire
input sequence. This means that while the physical dimensions of the
sequence (length and feature size) remain unchanged as it passes through the trans-
former block, the content of each output vector is re-encoded to integrate contextual
information from across the entire input sequence.
