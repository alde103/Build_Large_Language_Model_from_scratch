<!-- livebook:{"persist_outputs":true} -->

# Chapter 7: Fine-tuning to follow instructions

```elixir
Mix.install([
  {:nx, "~> 0.5"},
  {:exla, "~> 0.5"},
  {:axon, "~> 0.5"},
  {:table_rex, "~> 3.1.1"},
  {:bumblebee, "~> 0.6.0"},
  {:explorer, "~> 0.7.1"},
  {:req, "~> 0.4.5"},
  {:kino_vega_lite, "~> 0.1.11"}
])

Nx.global_default_backend(EXLA.Backend)
```

## Introduction

```elixir
{:ok, gpt2} = Bumblebee.load_model({:hf, "openai-community/gpt2"})
{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "openai-community/gpt2"})
{:ok, generation_config} = Bumblebee.load_generation_config({:hf, "openai-community/gpt2"})

serving = Bumblebee.Text.generation(gpt2, tokenizer, generation_config)

text_input = Kino.Input.text("Text", default: "Yesterday, I was reading a book and")
```

```elixir
text = Kino.Input.read(text_input)
Nx.Serving.run(serving, text)
```

<!-- livebook:{"output":true} -->

```
%{
  results: [
    %{
      text: " I was thinking, \"What's going on here?\" I was thinking, \"What's going on",
      token_summary: %{input: 8, output: 20, padding: 0}
    }
  ]
}
```

```elixir
%{model: model, params: params} = gpt2

tokenizer =
      Bumblebee.configure(tokenizer,
        length: nil,
        pad_direction: :left,
        return_token_type_ids: false,
        return_length: true
      )

input = Bumblebee.apply_tokenizer(tokenizer, "I want to")

gpt2_model = Axon.nx(model, & &1.logits)

{_init_fn, predict_fn} = Axon.build(gpt2_model)

result = predict_fn.(params, input)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1][3][50257]
  EXLA.Backend<host:0, 0.2805699667.4000710680.246591>
  [
    [
      [-39.308448791503906, -39.010066986083984, -41.837467193603516, -41.781246185302734, -40.84248352050781, -40.89142990112305, -38.62623596191406, -40.154056549072266, -38.097896575927734, -41.04249954223633, -40.9429931640625, -36.262168884277344, -37.39033889770508, -36.03800964355469, -38.52249526977539, -40.54604721069336, -39.718971252441406, -39.7431640625, -40.27290344238281, -40.314857482910156, -40.54868698120117, -41.00197219848633, -40.9098014831543, -40.914119720458984, -41.297733306884766, -37.69235610961914, -39.106632232666016, -41.460182189941406, -40.526241302490234, -40.43655014038086, -38.97370147705078, -41.32615661621094, -39.90999984741211, -40.565555572509766, -40.7227897644043, -40.8016471862793, -40.875083923339844, -40.86553955078125, -40.39710998535156, -40.221649169921875, -38.78817367553711, -40.58393096923828, -40.43303298950195, -40.767242431640625, -40.72999572753906, -40.78556442260742, -40.461753845214844, -41.084720611572266, -41.600372314453125, -41.25688552856445, ...],
      ...
    ]
  ]
>
```

## 7.1 Introduction to instruction fine-tuning

Pre-trained LLMs often struggle with specific instructions. Here, we focus on improving the LLM’s ability to follow such instructions and generate a desired response. Preparing the dataset is a key
aspect of instruction fine-tuning.

## 7.2 Preparing a dataset for supervised instruction fine-tuning

```elixir
File.cd!(__DIR__)
instructions_data = File.read!("instruction–response.json")
{:ok, dataset} = Jason.decode(instructions_data)
```

<!-- livebook:{"output":true} -->

```
{:ok,
 [
   %{
     "input" => "freind --> friend",
     "instruction" => "Evaluate the following phrase by transforming it into the spelling given.",
     "output" => "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\"."
   },
   %{
     "input" => "He go to the park every day.",
     "instruction" => "Edit the following sentence for grammar.",
     "output" => "He goes to the park every day."
   },
   %{
     "input" => "",
     "instruction" => "Convert 45 kilometers to meters.",
     "output" => "45 kilometers is 45000 meters."
   },
   %{
     "input" => "",
     "instruction" => "Rewrite this sentence to start with 'Although': Despite the rain, they went for a walk.",
     "output" => "Although it was raining, they went for a walk."
   },
   %{
     "input" => "",
     "instruction" => "What are the first 10 square numbers?",
     "output" => "1, 4, 9, 16, 25, 36, 49, 64, 81, 100."
   },
   %{
     "input" => "",
     "instruction" => "Suggest a more formal synonym for \"happy.\"",
     "output" => "A more formal synonym for \"happy\" is \"content.\""
   },
   %{
     "input" => "Where is the nearest restaurant?",
     "instruction" => "Translate the following sentence into French.",
     "output" => "Où est le restaurant le plus proche?"
   },
   %{
     "input" => "",
     "instruction" => "Look up the melting point of iron.",
     "output" => "The melting point of iron is approximately 1538 degrees Celsius or 2800 degrees Fahrenheit."
   },
   %{
     "input" => "The cake was baked by Sarah.",
     "instruction" => "Rewrite the following sentence so that it is in active voice.",
     "output" => "Sarah baked the cake."
   },
   %{
     "input" => "",
     "instruction" => "Convert the active sentence to passive: 'The manager approved the project.'",
     "output" => "The project was approved by the manager."
   },
   %{
     "input" => "",
     "instruction" => "What is the contraction for \"will not\"?",
     "output" => "The contraction for \"will not\" is \"won't\"."
   },
   %{
     "input" => "The actual fact is that he was late.",
     "instruction" => "Rewrite the following sentence to remove redundancy.",
     "output" => "The fact is that he was late."
   },
   %{
     "input" => "",
     "instruction" => "What is the capital of Indonesia?",
     "output" => "The capital of Indonesia is Jakarta."
   },
   %{
     "input" => "",
     "instruction" => "Provide a synonym for 'bright'.",
     "output" => "A synonym for 'bright' is 'radiant'."
   },
   %{
     "input" => "",
     "instruction" => "Suggest an alternate word for 'happy'.",
     "output" => "joyful."
   },
   %{
     "input" => "",
     "instruction" => "Name the device that measures atmospheric pressure.",
     "output" => "The device that measures atmospheric pressure is a barometer."
   },
   %{
     "input" => "",
     "instruction" => "List two antonyms for 'intelligent.'",
     "output" => "Two antonyms for 'intelligent' are 'stupid' and 'dumb.'"
   },
   %{
     "input" => "",
     "instruction" => "What is the pH of a neutral solution at 25°C?",
     "output" => "The pH of a neutral solution at 25°C is 7."
   },
   %{
     "input" => "'Her eyes are like stars.'",
     "instruction" => "Rewrite the following sentence to use a metaphor",
     "output" => "Her eyes are stars in the night sky."
   },
   %{
     "input" => "",
     "instruction" => "Name three types of biomes.",
     "output" => "Forest, desert, and grassland."
   },
   %{
     "input" => "",
     "instruction" => "Identify the climax in 'Romeo and Juliet'.",
     "output" => "The climax of 'Romeo and Juliet' occurs when Romeo kills Tybalt, leading to his banishment and the subsequent tragic events."
   },
   %{
     "input" => "The dog is very loyal.",
     "instruction" => "Rewrite the sentence using a simile.",
     "output" => "The dog is as loyal as a soldier."
   },
   %{
     "input" => "The storm started suddenly.",
     "instruction" => "Rewrite the sentence using an idiom.",
     "output" => "The storm came out of the blue."
   },
   %{
     "input" => "",
     "instruction" => "Convert 10 feet to meters.",
     "output" => "10 feet is equal to approximately 3.048 meters."
   },
   %{
     "input" => "The chef cooked a delicious meal.",
     "instruction" => "Convert this sentence to passive voice",
     "output" => "A delicious meal was cooked by the chef."
   },
   %{
     "input" => "",
     "instruction" => "What is the plural form of 'mouse'?",
     "output" => "The plural form of 'mouse' is 'mice'."
   },
   %{
     "input" => "",
     "instruction" => "What is the opposite of 'thin'?",
     "output" => "The opposite of 'thin' is 'thick'."
   },
   %{
     "input" => "",
     "instruction" => "What is the state capital of California?",
     "output" => "The state capital of California is Sacramento."
   },
   %{
     "input" => "",
     "instruction" => "Translate 'family' into Italian.",
     "output" => "The Italian word for 'family' is 'famiglia'."
   },
   %{
     "input" => "",
     "instruction" => "What is the past tense of 'draw'?",
     "output" => "The past tense of 'draw' is 'drew.'"
   },
   %{
     "input" => "The ship sails at dawn.",
     "instruction" => "Alter the content of the sentence to use the past tense.",
     "output" => "The ship sailed at dawn."
   },
   %{
     "input" => "",
     "instruction" => "Generate a sentence that follows the pattern: 'Rarely do I _____'",
     "output" => "Rarely do I eat fast food."
   },
   %{
     "input" => "",
     "instruction" => "What is the atomic number of oxygen?",
     "output" => "The atomic number of oxygen is 8."
   },
   %{
     "input" => "",
     "instruction" => "What is a synonym of 'luminous'?",
     "output" => "A synonym for 'luminous' is 'bright'."
   },
   %{
     "input" => "",
     "instruction" => "What is the capital of Brazil?",
     "output" => "The capital of Brazil is Brasília."
   },
   %{
     "input" => "",
     "instruction" => "List the ingredients required to make a basic salad.",
     "output" => "Ingredients for a basic salad typically include lettuce, tomatoes, cucumbers, and dressing."
   },
   %{
     "input" => "It's hot outside.",
     "instruction" => "Rewrite the sentence using a synonym.",
     "output" => "It's scorching outside."
   },
   %{
     "input" => "",
     "instruction" => "Provide a synonym for 'happy'.",
     "output" => "A synonym for 'happy' is 'content'."
   },
   %{
     "input" => "",
     "instruction" => "What is the derivative of x^3 + 5x^2 - 3x + 7 with respect to x?",
     "output" => "3x^2 + 10x - 3."
   },
   %{
     "input" => "She felt a sense of relief.",
     "instruction" => "Identify the point of view used in the sentence.",
     "output" => "The point of view used in the sentence is third person."
   },
   %{
     "input" => "",
     "instruction" => "Generate a sentence using the word 'gregarious'.",
     "output" => "He was gregarious, making friends wherever he went."
   },
   %{
     "input" => "",
     "instruction" => "What is the atomic number of carbon?",
     "output" => "The atomic number of carbon is 6."
   },
   %{
     "input" => "",
     "instruction" => "What is the formula for area of a triangle?",
     "output" => "The formula for the area of a triangle is 1/2 * base * height."
   },
   %{
     "input" => "Zebra, Elephant, Crocodile",
     "instruction" => "Sort the following list in alphabetical order.",
     "output" => "Crocodile, Elephant, Zebra"
   },
   %{
     "input" => "Moon and stars",
     "instruction" => "Reverse the order of the given phrase.",
     "output" => "Stars and moon"
   },
   %{"input" => "", "instruction" => "What is a synonym for 'begin'?", ...},
   %{"input" => "", ...},
   %{...},
   ...
 ]}
```

```elixir
data_length = Enum.count(dataset)
```

<!-- livebook:{"output":true} -->

```
1100
```

```elixir
Enum.at(dataset, 50)
```

<!-- livebook:{"output":true} -->

```
%{
  "input" => "Ocassion",
  "instruction" => "Identify the correct spelling of the following word.",
  "output" => "The correct spelling is 'Occasion.'"
}
```

Instruction fine-tuning involves training a model on a dataset where the input-output pairs, like those we extracted from the JSON file, are explicitly provided. There are various methods to format these entries for LLMs. There are two different approaches (prompt styles):

* Alpaca prompt style template.

The Alpaca style uses a structured format with defined sections for instruction, input, and response.

```
**Below is an instruction that describes a task. Write a response that appropriately completes the request.**

### Instruction:
Identify the correct spelling of the following word.

### Input:
Ocassion

### Response:
The correct spelling is 'Occasion'
```

* Phi-3 prompt style template.

The Phi-3 style employs a simpler format with designated <|user|> and <|assistant|> tokens.

```
<|user|>
Identify the correct spelling of the following word: 'Ocassion'

<|assistant>
The correct spelling is 'Occasion'.
```

Alpaca was one of the early LLMs to publicly detail its instruction fine-tuning process. Phi-3, developed by Microsoft, is included to demonstrate the diversity in prompt styles.

```elixir
defmodule MyGPT.Assistant.Formatter do
  def build_instruction(json) do
    instruction = 
    """
    Below is an instruction that describes a task. Write a response that appropriately completes the request.
    
    ### Instruction:
    #{json["instruction"]}\
    """

    instruction <> build_input(json)
  end

  def build_response(json), do: "\n\n### Response:\n#{json["output"]}"
  defp build_input(%{"input" => input_text}) when input_text == "", do: ""
  defp build_input(%{"input" => input_text}) do
    "\n\n### Input:\n#{input_text}"
  end
  
end

alias MyGPT.Assistant.Formatter
```

<!-- livebook:{"output":true} -->

```
MyGPT.Assistant.Formatter
```

```elixir
prompt =
  dataset
  |> Enum.at(50)
  |> Formatter.build_instruction()
IO.puts(prompt)
```

<!-- livebook:{"output":true} -->

```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Identify the correct spelling of the following word.

### Input:
Ocassion
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
response = 
  dataset
  |> Enum.at(50)
  |> Formatter.build_response()
IO.puts(response)
```

<!-- livebook:{"output":true} -->

```


### Response:
The correct spelling is 'Occasion.'
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
(prompt <> response) |> IO.puts
```

<!-- livebook:{"output":true} -->

```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Identify the correct spelling of the following word.

### Input:
Ocassion

### Response:
The correct spelling is 'Occasion.'
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
prompt = 
  dataset
  |> Enum.at(999)
  |> Formatter.build_instruction()

response = 
  dataset
  |> Enum.at(999)
  |> Formatter.build_response()

(prompt <> response) |> IO.puts
Enum.at(dataset, 999)
```

<!-- livebook:{"output":true} -->

```
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
What is an antonym of 'complicated'?

### Response:
An antonym of 'complicated' is 'simple'.
```

<!-- livebook:{"output":true} -->

```
%{
  "input" => "",
  "instruction" => "What is an antonym of 'complicated'?",
  "output" => "An antonym of 'complicated' is 'simple'."
}
```

```elixir
train_length = floor(data_length * 0.85)
test_length = floor(data_length * 0.1)

dataset = Enum.shuffle(dataset)

{train_dataset, dataset_tail} = Enum.split(dataset, train_length)
{test_dataset, validation_dataset} = Enum.split(dataset_tail, test_length)

IO.puts("Training set length: #{Enum.count(train_dataset)}")
IO.puts("Validation set length: #{Enum.count(validation_dataset)}")
IO.puts("Test set length: #{Enum.count(test_dataset)}")
```

<!-- livebook:{"output":true} -->

```
Training set length: 935
Validation set length: 55
Test set length: 110
```

<!-- livebook:{"output":true} -->

```
:ok
```

## 7.3 Organizing data into training batches

A collate function is responsible for taking a list of individual data samples and merging them into a single batch that can be processed efficiently by the model during training.

```elixir
defmodule MyGPT.Assistant.Dataset do
  import MyGPT.Assistant.Formatter

  def build(json_dataset, tokenizer, opt \\ []) do
    with max_length <- Keyword.get(opt, :max_length),
         pad_token_id <- Keyword.get(opt, :pad_token_id, 50256),
         exclude_token_id <- Keyword.get(opt, :exclude_token_id, -100),
         batch_size <- Keyword.get(opt, :batch_size, 2) do

      json_stream =  Stream.chunk_every(json_dataset, batch_size)
      
      for json_batch <- json_stream, reduce: []  do
        acc ->
          prompt_batch =  
            json_batch
            |> Stream.map(&build_prompt(&1))
            |> Stream.map(&encode_prompt(&1, tokenizer))
          
          max_length_in_batch = 
            prompt_batch
            |> Enum.map(fn encoded_prompt -> Nx.size(encoded_prompt["input_ids"]) end)
            |> Enum.max()
            |> then(&(max_length || &1))
          
          input = 
            prompt_batch
            |> Enum.map(&build_input(&1, pad_token_id, max_length_in_batch))
            |> Nx.Batch.stack()

          label = 
            prompt_batch
            |> Enum.map(&build_label(&1, pad_token_id, exclude_token_id, max_length_in_batch))
            |> Nx.Batch.stack()

          acc ++ [{input, label}]
      end
    end
  end

  def build_prompt(json), do: build_instruction(json) <> build_response(json)
  def encode_prompt(prompt, tokenizer), do: Bumblebee.apply_tokenizer(tokenizer, prompt)

  def build_input(%{"input_ids" => encoded_prompt}, pad_token_id, max_length) do
    input_length = Nx.size(encoded_prompt)
    length_diff = max_length - input_length

    encoded_prompt
    |> Nx.pad(pad_token_id, [{0, 0, 0}, {0, length_diff, 0}])
    |> Nx.reshape({max_length})
    |> then(&(%{"input_ids" => &1}))
  end
  
  def build_label(%{"input_ids" => encoded_prompt}, pad_token_id, exclude_token_id, max_length) do
    input_length = Nx.size(encoded_prompt)
    label_tensor = 
      encoded_prompt[[.., 1..(input_length-1)]]
      |> Nx.pad(pad_token_id, [{0, 0, 0}, {0, 1, 0}])
    
    length_diff = max_length - input_length

    label_tensor
    |> Nx.pad(exclude_token_id, [{0, 0, 0}, {0, length_diff, 0}])
    |> Nx.reshape({max_length})
  end
end

alias MyGPT.Assistant.Dataset
```

<!-- livebook:{"output":true} -->

```
MyGPT.Assistant.Dataset
```

```elixir
res = Dataset.build(train_dataset, tokenizer)
```

<!-- livebook:{"output":true} -->

```
[
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, pad: 0, ...>},
  {#Nx.Batch<size: 2, pad: 0, ...>, #Nx.Batch<size: 2, ...>},
  {#Nx.Batch<size: 2, ...>, #Nx.Batch<...>},
  {#Nx.Batch<...>, ...},
  {...},
  ...
]
```

```elixir
{input, label} = Enum.at(res, 1)
input = Nx.Defn.jit_apply(&Function.identity/1, [input])
label = Nx.Defn.jit_apply(&Function.identity/1, [label])

IO.inspect({input, label}, limit: :infinity)
```

<!-- livebook:{"output":true} -->

```
{%{
   "input_ids" => #Nx.Tensor<
     u32[2][59]
     EXLA.Backend<host:0, 0.2805699667.4000972833.51805>
     [
       [21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 3103, 1851, 262, 9546, 656, 257, 1844, 6827, 13, 198, 198, 21017, 23412, 25, 198, 3847, 6766, 198, 198, 21017, 18261, 25, 198, 464, 1755, 6766, 373, 38745, 351, 12925, 665, 676, 1359, 5788, 13],
       [21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 30003, 6525, 262, 1708, 6827, 284, 779, 257, 23094, 25, 705, 6653, 2456, 2005, 9211, 621, 257, 9845, 2637, 198, 198, 21017, 18261, 25, 198, 6653, 2456, 547, 257, 9845, 11, 7720, 2769, 13, 50256]
     ]
   >
 },
 #Nx.Tensor<
   s64[2][59]
   EXLA.Backend<host:0, 0.2805699667.4000972833.51806>
   [
     [318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 3103, 1851, 262, 9546, 656, 257, 1844, 6827, 13, 198, 198, 21017, 23412, 25, 198, 3847, 6766, 198, 198, 21017, 18261, 25, 198, 464, 1755, 6766, 373, 38745, 351, 12925, 665, 676, 1359, 5788, 13, 50256],
     [318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 30003, 6525, 262, 1708, 6827, 284, 779, 257, 23094, 25, 705, 6653, 2456, 2005, 9211, 621, 257, 9845, 2637, 198, 198, 21017, 18261, 25, 198, 6653, 2456, 547, 257, 9845, 11, 7720, 2769, 13, 50256, -100]
   ]
 >}
```

<!-- livebook:{"output":true} -->

```
{%{
   "input_ids" => #Nx.Tensor<
     u32[2][59]
     EXLA.Backend<host:0, 0.2805699667.4000972833.51805>
     [
       [21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 3103, 1851, 262, 9546, 656, 257, 1844, 6827, 13, 198, 198, 21017, 23412, 25, 198, 3847, 6766, 198, 198, 21017, 18261, 25, 198, 464, ...],
       ...
     ]
   >
 },
 #Nx.Tensor<
   s64[2][59]
   EXLA.Backend<host:0, 0.2805699667.4000972833.51806>
   [
     [318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 3103, 1851, 262, 9546, 656, 257, 1844, 6827, 13, 198, 198, 21017, 23412, 25, 198, 3847, 6766, 198, 198, 21017, 18261, 25, 198, 464, 1755, ...],
     ...
   ]
 >}
```

We assign a -100 placeholder value to all padding tokens. This special value allows us to exclude these padding tokens from contributing to the training loss calculation, ensuring that only meaningful data influences model learning.

However, we retain one end-of-text token, ID 50256. Retaining it allows the LLM to learn when to generate an end-of-text token in response to instructions, which we use as an indicator that the generated response is complete.

The default setting of the cross entropy function in PyTorch is cross_entropy(..., ignore_index=-100). This means that it ignores targets labeled with -100. We take advantage of this ignore_index to ignore the additional end-of-text (padding) tokens that we used to pad the training examples to have the same length in each batch.

However, we want to keep one 50256 (end-of-text) token ID in the targets because it helps the LLM to learn to generate end-of-text tokens
.
In addition to masking out padding tokens, it is also common to mask out the target token IDs that correspond to the instruction. By masking out the LLM’s target token IDs corresponding to the instruction, the cross entropy loss is only computed for the generated response target IDs. Thus, the model
is trained to focus on generating accurate responses rather than memorizing instructions, which can help reduce overfitting.

As of this writing, researchers are divided on whether masking the instructions is universally beneficial during instruction fine-tuning.
