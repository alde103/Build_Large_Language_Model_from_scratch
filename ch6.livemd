<!-- livebook:{"persist_outputs":true} -->

# Chapter 6: Fine-tuning for classification

```elixir
Mix.install([
  {:nx, "~> 0.5"},
  {:exla, "~> 0.5"},
  {:axon, "~> 0.5"},
  {:tiktoken, "~> 0.3.2"},
  {:table_rex, "~> 3.1.1"},
  {:bumblebee, "~> 0.6.0"},
  {:kino_vega_lite, "~> 0.1.11"}
])

Nx.global_default_backend(EXLA.Backend)
```

## Introduction

```elixir
{:ok, gpt2} = Bumblebee.load_model({:hf, "openai-community/gpt2"})
{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "openai-community/gpt2"})
{:ok, generation_config} = Bumblebee.load_generation_config({:hf, "openai-community/gpt2"})

serving = Bumblebee.Text.generation(gpt2, tokenizer, generation_config)

text_input = Kino.Input.text("Text", default: "Yesterday, I was reading a book and")
```

```elixir
text = Kino.Input.read(text_input)
Nx.Serving.run(serving, text)
```

<!-- livebook:{"output":true} -->

```
%{
  results: [
    %{
      text: " I was thinking, \"What's going on here?\" I was thinking, \"What's going on",
      token_summary: %{input: 8, output: 20, padding: 0}
    }
  ]
}
```

```elixir
%{model: model, params: params} = gpt2

tokenizer =
      Bumblebee.configure(tokenizer,
        length: nil,
        pad_direction: :left,
        return_token_type_ids: false,
        return_length: true
      )

input = Bumblebee.apply_tokenizer(tokenizer, "I want to")

gpt2_model = Axon.nx(model, & &1.logits)

{_init_fn, predict_fn} = Axon.build(gpt2_model)

result = predict_fn.(params, input)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1][3][50257]
  EXLA.Backend<host:0, 0.3387358558.2529296408.123594>
  [
    [
      [-39.308448791503906, -39.010066986083984, -41.837467193603516, -41.781246185302734, -40.84248352050781, -40.89142990112305, -38.62623596191406, -40.154056549072266, -38.097896575927734, -41.04249954223633, -40.9429931640625, -36.262168884277344, -37.39033889770508, -36.03800964355469, -38.52249526977539, -40.54604721069336, -39.718971252441406, -39.7431640625, -40.27290344238281, -40.314857482910156, -40.54868698120117, -41.00197219848633, -40.9098014831543, -40.914119720458984, -41.297733306884766, -37.69235610961914, -39.106632232666016, -41.460182189941406, -40.526241302490234, -40.43655014038086, -38.97370147705078, -41.32615661621094, -39.90999984741211, -40.565555572509766, -40.7227897644043, -40.8016471862793, -40.875083923339844, -40.86553955078125, -40.39710998535156, -40.221649169921875, -38.78817367553711, -40.58393096923828, -40.43303298950195, -40.767242431640625, -40.72999572753906, -40.78556442260742, -40.461753845214844, -41.084720611572266, -41.600372314453125, -41.25688552856445, ...],
      ...
    ]
  ]
>
```

```elixir
defmodule GPTModel do
  def text_to_token_ids(tokenizer, text) do
    tokenizer
    |> Bumblebee.configure(
        length: nil,
        pad_direction: :left,
        return_token_type_ids: false,
        return_length: true
      )
    |> Bumblebee.apply_tokenizer(text)
  end

  def token_ids_to_text(tokenizer, token_ids) do
    tokenizer
    |> Bumblebee.configure(
      length: nil,
      pad_direction: :left,
      return_token_type_ids: false,
      return_length: true
    )
    |> Bumblebee.Tokenizer.decode(token_ids)
    |> Enum.at(0)
  end

  def generate_text(model, params, tokenizer, text, max_new_token, k \\ 0, temperature \\ 1) do
    {_init_fn, predict_fn} = Axon.build(model)
    input = text_to_token_ids(tokenizer, text)
    %{"input_ids" => new_tokens_ids} = 
      for _new_token_index <- 1..max_new_token, reduce: input do
        %{"input_ids" => input, "attention_mask" => attention_mask, "length" => length} = full_input ->
          logit = predict_fn.(params, full_input)
  
          # Get last element of the vector.
          predicted_new_token =
            logit[[.., -1]]
            |> top_k(k)
            |> softmax_with_temperature(temperature)
            |> Nx.new_axis(0)
  
          input = Nx.concatenate([input, predicted_new_token], axis: 1)
          attention_mask = Nx.concatenate([attention_mask, Nx.tensor([[1]])], axis: 1)
          length = Nx.add(length, 1)
          %{"input_ids" => input, "attention_mask" => attention_mask, "length" => length}
      end
    token_ids_to_text(tokenizer, new_tokens_ids)
  end

  defp multinomial(probabilities, num_samples, max_random_number \\ 1000) do
    seed = :rand.uniform(max_random_number)

    key = Nx.Random.key(seed)

    {random_values, _new_key} = Nx.Random.uniform(key, shape: {num_samples})

    cumulative_probs = Nx.cumulative_sum(probabilities, axis: -1)

    Enum.map(Nx.to_flat_list(random_values), fn value ->
      Enum.find_index(
        Nx.to_flat_list(cumulative_probs),
        fn prob -> prob >= value end
      )
    end)
  end

  defp softmax_with_temperature(logits, temperature) when temperature < 0,
    do: Axon.Layers.softmax(logits, axis: -1) |> Nx.argmax(axis: -1)

  defp softmax_with_temperature(logits, temperature) when temperature > 0 do
    scaled_logits = Nx.divide(logits, temperature)

    Axon.Layers.softmax(scaled_logits, axis: -1)
    |> multinomial(1)
    |> Nx.tensor()
  end

  defp top_k(logits, k) when k == 0, do: logits

  defp top_k(logits, k) do
    {top_logits, _top_pos} = Nx.top_k(logits, k: k)
    min_index = Nx.reduce_min(top_logits)
    neg_inf_tensor = Nx.broadcast(Nx.Constants.neg_infinity(), logits.shape)
    Nx.select(Nx.less(logits, min_index), neg_inf_tensor, logits)
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, GPTModel, <<70, 79, 82, 49, 0, 0, 24, ...>>, {:top_k, 2}}
```

```elixir
GPTModel.generate_text(gpt2_model, params, tokenizer, "I want to", 15)
```

<!-- livebook:{"output":true} -->

```
"I want to talk to you about that and hang up on you… Because I'm sure"
```

## 6.1 Different categories of fine-tuning

The most common ways to fine-tune language models are instruction fine-tuning and
classification fine-tuning. Instruction fine-tuning involves training a language model on a set of tasks using specific instructions to improve its ability to understand and execute tasks described in natural language prompts.

In classification fine-tuning, the model is trained to recognize a specific set of class labels, such as “spam” and “not spam.”

The key point is that a classification fine-tuned model is restricted to predicting
classes it has encountered during its training, it is easier to develop a
specialized model than a generalist model that works well across various tasks.

**Choosing the right approach**

Instruction fine-tuning improves a model’s ability to understand and generate responses
based on specific user instructions. Instruction fine-tuning is best suited for models
that need to handle a variety of tasks based on complex user instructions, improving
flexibility and interaction quality. Classification fine-tuning is ideal for projects requiring precise categorization of data into predefined classes, such as sentiment analysis or spam detection.

While instruction fine-tuning is more versatile, it demands larger datasets and greater
computational resources to develop models proficient in various tasks. In contrast,
classification fine-tuning requires less data and compute power, but its use is confined to the specific classes on which the model has been trained.
