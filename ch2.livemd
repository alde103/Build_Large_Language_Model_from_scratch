# Chapter 2: Working with Text Data

```elixir
Mix.install([
  {:nx, "~> 0.5"},
  {:exla, "~> 0.5"},
  {:axon, "~> 0.5"},
  {:tiktoken, "~> 0.3.2"},
])
```

## 2.1 Understanding word embeddings

The concept of converting data into a vector format is often referred to as embedding.

Embedding model to transform this raw data into a dense vector representation that deep
learning architectures can easily understand and process.

Different data formats require distinct embedding models.

An embedding is a mapping from discrete objects, such as words, images, or
even entire documents, to points in a continuous vector space.

The primary purpose of embeddings is to convert non-numeric data into a format that neural networks can process.

Retrieval-augmented generation combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text, which is a technique that is beyond the scope of this book.

A higher dimensionality might capture more nuanced relationships but at the cost of computational efficiency.

LLMs commonly produce their own embeddings that are part of the input layer and are updated during training.

The embedding size is a trade-off between performance and efficiency.

## 2.2 Tokenizing text

Tokens are either individual words or special characters, including punctuation characters.

```elixir
path = 
  "/home/alde/Documents/MyDevelopment/Build_A_Large_Language_Model/the-verdict.txt"
{:ok, raw_text} = File.read(path)
```

```elixir
String.length(raw_text) |> IO.inspect()
```

```elixir
{line, rest} = String.split_at(raw_text, 99)
IO.puts(line)
```

```elixir
text = "Hello, world. This, is a test."

{:ok, regex} = Regex.compile("(\s)")
Regex.split(regex, text, include_captures: true)
```

```elixir
{:ok, regex} = Regex.compile("([,.]|\s)")
result = Regex.split(regex, text, include_captures: true)
```

```elixir
Enum.filter(result, fn x -> not (x == " " or x == "") end)
```

Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing).

```elixir
text = "Hello, world. Is this-- a test?"
{:ok, regex} = Regex.compile("([,.:;?_!\"()\']|--|\s)")
result = Regex.split(regex, text, include_captures: true)
Enum.filter(result, fn x -> not (x == " " or x == "") end)
```

```elixir
result = Regex.split(regex, raw_text, include_captures: true)
preprocessed = Enum.filter(result, fn x -> not (x == " " or x == "") end)
Enum.count(preprocessed) |> IO.inspect()
```

```elixir
{head, tail} = Enum.split(preprocessed, 30)
IO.inspect(head)
```

## 2.3 Converting tokens into token IDs

In this section, we will convert these tokens from strings to an integer
representation to produce the so-called token IDs.

This conversion is an intermediate step before converting the token IDs into embedding vectors.

To map the previously generated tokens into token IDs, we have to build a so-called vocabulary first.

We build a vocabulary by tokenizing the entire text in a training dataset into individual tokens.

These individual tokens are then sorted alphabetically, and duplicate tokens are removed.

The unique tokens are then aggregated into a vocabulary that defines a mapping from each unique token to a unique integer value.

The depicted vocabulary is purposefully small for illustration purposes and contains no punctuation or special characters for simplicity.

```elixir
vocabulary =
  preprocessed
  |> Enum.uniq()
  |> Enum.sort()
  |> Enum.with_index()
  |> Enum.reduce(%{}, fn {value, index}, acc -> Map.put(acc, value, index) end)
```

when we want to convert the outputs of an LLM from numbers back into
text, we also need a way to turn token IDs into text.

```elixir
defmodule SimpleTokenizerV1 do
  def encode(vocabulary, text) do
    {:ok, regex} = Regex.compile("([,.:;?_!\"()\']|--|\s)")
    result = Regex.split(regex, text, include_captures: true)
    preprocessed = Enum.filter(result, fn x -> not (x == " " or x == "") end)
    Enum.map(preprocessed, fn token -> Map.get(vocabulary, token) end)
  end

  def decode(reverse_vocabulary, ids) do
    tokens = Enum.map(ids, fn id -> Map.get(reverse_vocabulary, id) end)
    text = Enum.reduce(tokens, "", fn token, acc -> acc <> token <> " " end)
    {:ok, regex} = Regex.compile("\s+([,.?!\"()\'])")
    String.replace(text, regex, "\\1")
  end
end
```

```elixir
reverse_vocabulary = 
  vocabulary
  |> Enum.map(fn {key, value} -> {value, key} end)
  |> Enum.into(%{})
```

```elixir
text = """
It's the last he painted, you know," Mrs. Gisburn said with pardonable pride.\
"""
ids = SimpleTokenizerV1.encode(vocabulary, text)
```

```elixir
SimpleTokenizerV1.decode(reverse_vocabulary, ids)
```

```elixir
text = "Hello, do you like tea?"

ids = SimpleTokenizerV1.encode(vocabulary, text)
```

## 2.4 Adding special context tokens

To handle certain contexts. We add an `<|unk|>` token to represent new and unknown words that were not part of the training data and thus not part of the existing vocabulary. 
Furthermore, we add an `<|endoftext|>` token that we can use to separate two unrelated text sources.

```elixir
build_vocabulary = fn text ->
  {:ok, regex} = Regex.compile("([,.:;?_!\"()\']|--|\s)")

  Regex.split(regex, text, include_captures: true)
  |> Enum.map(fn token -> String.trim(token, "\n") end)
  |> Enum.filter(fn x -> not (x == " " or x == "") end)
  |> Enum.uniq()
  |> Enum.sort()
  |> Enum.concat(["<|endoftext|>", "<|unk|>"])
  |> Enum.with_index()
  |> Enum.reduce(%{}, fn {value, index}, acc -> Map.put(acc, value, index) end)
end

build_reverse_vocabulary = fn vocabulary ->
  vocabulary
  |> Enum.map(fn {key, value} -> {value, key} end)
  |> Enum.into(%{})
end
```

```elixir
vocabulary = build_vocabulary.(raw_text)
reverse_vocabulary = build_reverse_vocabulary.(vocabulary)
```

```elixir
vocabulary["<|unk|>"]
```

```elixir
Enum.count(vocabulary)
```

```elixir
vocabulary["yourself"]
```

```elixir
defmodule SimpleTokenizerV2 do
  def encode(vocabulary, text) do
    {:ok, regex} = Regex.compile("([,.:;?_!\"()\']|--|\s)")
    result = Regex.split(regex, text, include_captures: true)
    preprocessed = Enum.filter(result, fn x -> not (x == " " or x == "") end)

    Enum.map(
      preprocessed,
      fn token ->
        id = Map.get(vocabulary, token)
        if is_nil(id), do: Map.get(vocabulary, "<|unk|>"), else: id
      end
    )
  end

  def decode(reverse_vocabulary, ids) do
    tokens = Enum.map(ids, fn id -> Map.get(reverse_vocabulary, id) end)
    text = Enum.reduce(tokens, "", fn token, acc -> acc <> token <> " " end)
    {:ok, regex} = Regex.compile("\s+([,.?!\"()\'])")
    String.replace(text, regex, "\\1")
  end
end
```

```elixir
text1 = "Hello, do you like tea?"
text2 = "In the sunlit terraces of the palace."
text = text1 <> " <|endoftext|> " <> text2
IO.puts(text)
```

```elixir
ids = SimpleTokenizerV2.encode(vocabulary, text)
```

```elixir
SimpleTokenizerV2.decode(reverse_vocabulary, ids)
```

Depending on the LLM, some researchers also consider additional special tokens such
as the following:

* `[BOS]` (beginning of sequence): This token marks the start of a text. It
  signifies to the LLM where a piece of content begins.

* `[EOS]` (end of sequence): This token is positioned at the end of a text,
  and is especially useful when concatenating multiple unrelated texts,
  similar to `<|endoftext|>`. For instance, when combining two different
  Wikipedia articles or books, the `[EOS]` token indicates where one article
  ends and the next one begins.

* `[PAD]` (padding): When training LLMs with batch sizes larger than one,
  the batch might contain texts of varying lengths. To ensure all texts have
  the same length, the shorter texts are extended or "padded" using the
  `[PAD]` token, up to the length of the longest text in the batch.

When training on batched inputs, we typically use a mask, meaning we don't attend to padded tokens. Thus, the specific token chosen for padding becomes inconsequential.

## 2.5 Byte pair encoding

This section covers a more sophisticated tokenization scheme based on a concept
called byte pair encoding (BPE).

In Elixir there is also an tiktoken wrapper, to add it to the project use:
`{:tiktoken, "~> 0.3.2"}`

```elixir
text = "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace."

# gpt2 not supported
{:ok, ids} = Tiktoken.encode("gpt-3.5-turbo", text, ["<|endoftext|>"])
```

```elixir
Tiktoken.decode("gpt-3.5-turbo", ids)
```

```elixir
Tiktoken.decode("gpt-4", ids)
```

The BPE tokenizer can handle any unknown word.

The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary
into smaller subword units or even individual characters, enabling it to handle out-of-
vocabulary words.

The algorithm builds its vocabulary by iteratively merging frequent characters into subwords and
frequent subwords into words.The merges are determined by a frequency cutoff.

## 2.6 Data sampling with a sliding window

Given a text sample, extract input blocks as subsamples that serve as input to the LLM, and the
LLM's prediction task during training is to predict the next word that follows the input block. During training, we mask out all words that are past the target.

```elixir
{:ok, ids} = Tiktoken.encode("gpt-3.5-turbo", raw_text)
Enum.count(ids)
```

```elixir
{_unused, enc_sample} = Enum.split(ids, 50)
context_size = 4
enc_tensor = Nx.tensor(enc_sample)

x = enc_tensor[0..context_size-1]
y = enc_tensor[1..context_size]
{x,y}
```

```elixir
for index <- 0..context_size-1 do
  context = enc_tensor[0..index]
  desired = enc_tensor[index+1]

  {:ok, context_text} = context |> Nx.to_list() |> then(fn ids -> Tiktoken.decode("gpt-3.5-turbo", ids) end)
  {:ok, desired_text} = [Nx.to_number(desired)] |> then(fn ids -> Tiktoken.decode("gpt-3.5-turbo", ids) end)
  IO.puts("#{context_text} ---> #{desired_text}")
end
```

In particular, we are interested in returning two tensors: an input tensor containing the
text that the LLM sees and a target tensor that includes the targets for the LLM to predict.

```elixir
defmodule GPTDatasetV1 do
  def build_dataset(txt, tokenizer_model, max_length, stride) do
    {:ok, token_ids} = Tiktoken.encode(tokenizer_model, txt)

    token_ids_tensor = Nx.tensor(token_ids)
    text_length = length(token_ids)

    linespace =
      Enum.to_list(0..(length(token_ids) - max_length - 1)) |> Enum.take_every(stride)

    for i <- linespace, reduce: %{input_ids: [], target_ids: []} do
      %{input_ids: input_ids, target_ids: target_ids} = acc ->

        
        {input_ids, target_ids} =
          cond do
            i + max_length > text_length - 1 ->
              {input_ids, target_ids}

            i + max_length + 1 > text_length - 1 ->
              {input_ids, target_ids}

            true ->
              input_chunk = token_ids_tensor[i..(i + max_length)]
              target_chunk = token_ids_tensor[(i + 1)..(i + max_length + 1)]
              {input_ids ++ [input_chunk], target_ids ++ [target_chunk]}
          end

        %{acc | input_ids: input_ids, target_ids: target_ids}
    end
  end
end
```

```elixir
dataset = GPTDatasetV1.build_dataset(raw_text, "gpt-3.5-turbo", 4, 4)
```

```elixir
{:ok, input_text} = dataset[:input_ids] |> Enum.at(1) |> Nx.to_list() |> then(fn ids -> Tiktoken.decode("gpt-3.5-turbo", ids) end)
{:ok, target_text} = dataset[:target_ids] |> Enum.at(1) |> Nx.to_list() |> then(fn ids -> Tiktoken.decode("gpt-3.5-turbo", ids) end)
IO.puts("INPUT> #{input_text}\n")
IO.puts("TARGET> #{target_text}")
```
