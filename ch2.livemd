# Chapter 2: Working with Text Data

```elixir
Mix.install([
  {:nx, "~> 0.5"},
  {:exla, "~> 0.5"},
  {:axon, "~> 0.5"}
])
```

## 2.1 Understanding word embeddings

The concept of converting data into a vector format is often referred to as embedding.

Embedding model to transform this raw data into a dense vector representation that deep
learning architectures can easily understand and process.

Different data formats require distinct embedding models.

An embedding is a mapping from discrete objects, such as words, images, or
even entire documents, to points in a continuous vector space.

The primary purpose of embeddings is to convert non-numeric data into a format that neural networks can process.

Retrieval-augmented generation combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text, which is a technique that is beyond the scope of this book.

A higher dimensionality might capture more nuanced relationships but at the cost of computational efficiency.

LLMs commonly produce their own embeddings that are part of the input layer and are updated during training.

The embedding size is a trade-off between performance and efficiency.

## 2.2 Tokenizing text

Tokens are either individual words or special characters, including punctuation characters.

```elixir
path = 
  "/home/alde/Documents/MyDevelopment/Build_A_Large_Language_Model/the-verdict.txt"
{:ok, raw_text} = File.read(path)
```

```elixir
String.length(raw_text) |> IO.inspect()
```

```elixir
{line, rest} = String.split_at(raw_text, 99)
IO.puts(line)
```

```elixir
text = "Hello, world. This, is a test."

{:ok, regex} = Regex.compile("(\s)")
Regex.split(regex, text, include_captures: true)
```

```elixir
{:ok, regex} = Regex.compile("([,.]|\s)")
result = Regex.split(regex, text, include_captures: true)
```

```elixir
Enum.filter(result, fn x -> not (x == " " or x == "") end)
```

Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing).

```elixir
text = "Hello, world. Is this-- a test?"
{:ok, regex} = Regex.compile("([,.:;?_!\"()\']|--|\s)")
result = Regex.split(regex, text, include_captures: true)
Enum.filter(result, fn x -> not (x == " " or x == "") end)
```

```elixir
result = Regex.split(regex, raw_text, include_captures: true)
preprocessed = Enum.filter(result, fn x -> not (x == " " or x == "") end)
Enum.count(preprocessed) |> IO.inspect()
```

```elixir
{head, tail} = Enum.split(preprocessed, 30)
IO.inspect(head)
```

## 2.3 Converting tokens into token IDs

In this section, we will convert these tokens from strings to an integer
representation to produce the so-called token IDs.

This conversion is an intermediate step before converting the token IDs into embedding vectors.

To map the previously generated tokens into token IDs, we have to build a so-called vocabulary first.

We build a vocabulary by tokenizing the entire text in a training dataset into individual tokens.

These individual tokens are then sorted alphabetically, and duplicate tokens are removed.

The unique tokens are then aggregated into a vocabulary that defines a mapping from each unique token to a unique integer value.

The depicted vocabulary is purposefully small for illustration purposes and contains no punctuation or special characters for simplicity.

```elixir
vocabulary =
  preprocessed
  |> Enum.uniq()
  |> Enum.sort()
  |> Enum.with_index()
  |> Enum.reduce(%{}, fn {value, index}, acc -> Map.put(acc, value, index) end)
```

when we want to convert the outputs of an LLM from numbers back into
text, we also need a way to turn token IDs into text.

```elixir
defmodule SimpleTokenizerV1 do
  def encode(vocabulary, text) do
    {:ok, regex} = Regex.compile("([,.:;?_!\"()\']|--|\s)")
    result = Regex.split(regex, text, include_captures: true)
    preprocessed = Enum.filter(result, fn x -> not (x == " " or x == "") end)
    Enum.map(preprocessed, fn token -> Map.get(vocabulary, token) end)
  end

  def decode(reverse_vocabulary, ids) do
    tokens = Enum.map(ids, fn id -> Map.get(reverse_vocabulary, id) end)
    text = Enum.reduce(tokens, "", fn token, acc -> acc <> token <> " " end)
    {:ok, regex} = Regex.compile("\s+([,.?!\"()\'])")
    String.replace(text, regex, "\\1")
  end
end
```

```elixir
reverse_vocabulary = 
  vocabulary
  |> Enum.map(fn {key, value} -> {value, key} end)
  |> Enum.into(%{})
```

```elixir
text = """
It's the last he painted, you know," Mrs. Gisburn said with pardonable pride.\
"""
ids = SimpleTokenizerV1.encode(vocabulary, text)
```

```elixir
SimpleTokenizerV1.decode(reverse_vocabulary, ids)
```

```elixir
text = "Hello, do you like tea?"

ids = SimpleTokenizerV1.encode(vocabulary, text)
```


